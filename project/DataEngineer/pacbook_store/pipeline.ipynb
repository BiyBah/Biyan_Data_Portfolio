{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b7b5e62",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8afe3547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q numpy pandas luigi sqlalchemy dotenv psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a90fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import subprocess\n",
    "import luigi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy import types as sa_types\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from io import StringIO\n",
    "import logging\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5509038",
   "metadata": {},
   "source": [
    "# Supporting functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57d8f8",
   "metadata": {},
   "source": [
    "## Log to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a1e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_to_csv(log_msg: dict, filename: str):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    # Define the column headers\n",
    "    headers = [\"step\", \"status\", \"error_msg\", \"source\", \"table_name\", \"execution_time\", \"etl_date\"]\n",
    "\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "\n",
    "        # Write the header only if the file doesn't exist\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # Append the log message\n",
    "        writer.writerow(log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aa2e91",
   "metadata": {},
   "source": [
    "## Read SQL File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f81b3b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sql_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads an SQL file and returns its content as a string.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the SQL file to be read.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The content of the SQL file as a string if successful, \n",
    "                     or None if an error occurs.\n",
    "\n",
    "    Note:\n",
    "        Make sure to handle exceptions properly in the calling code.\n",
    "        The function assumes that the SQL file is encoded as UTF-8.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            sql_string = file.read()\n",
    "        return sql_string\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading SQL file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01a963",
   "metadata": {},
   "source": [
    "## Postgres connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "967b8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_connection():\n",
    "    try:\n",
    "        src_database = os.getenv(\"SRC_POSTGRES_DB\")\n",
    "        src_host = os.getenv(\"SRC_POSTGRES_HOST\")\n",
    "        src_user = os.getenv(\"SRC_POSTGRES_USER\")\n",
    "        src_password = os.getenv(\"SRC_POSTGRES_PASSWORD\")\n",
    "        src_port = os.getenv(\"SRC_POSTGRES_PORT\")\n",
    "\n",
    "        dwh_database = os.getenv(\"DWH_POSTGRES_DB\")\n",
    "        dwh_host = os.getenv(\"DWH_POSTGRES_HOST\")\n",
    "        dwh_user = os.getenv(\"DWH_POSTGRES_USER\")\n",
    "        dwh_password = os.getenv(\"DWH_POSTGRES_PASSWORD\")\n",
    "        dwh_port = os.getenv(\"DWH_POSTGRES_PORT\")\n",
    "        \n",
    "        src_conn = f\"postgresql://{src_user}:{src_password}@{src_host}:{src_port}/{src_database}\"\n",
    "        dwh_conn = f\"postgresql://{dwh_user}:{dwh_password}@{dwh_host}:{dwh_port}/{dwh_database}\"\n",
    "        \n",
    "        src_engine = create_engine(src_conn)\n",
    "        dwh_engine = create_engine(dwh_conn)\n",
    "        \n",
    "        return src_engine, dwh_engine\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098ff77",
   "metadata": {},
   "source": [
    "## Move table from temp log to log directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a98f3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_temp_to_log_dir(\n",
    "    logger, DIR_TEMP_LOG, DIR_LOG, etl_phase, log_time\n",
    "):\n",
    "    try:\n",
    "        # Create destination directory if it doesn't exist\n",
    "        subprocess.run([\"mkdir\", \"-p\", DIR_LOG], check=True)\n",
    "        \n",
    "        # Move all files from temp log to log directory\n",
    "        result = subprocess.run([\"mv\", f\"{DIR_TEMP_LOG}/{etl_phase}_log_{log_time}.log\", DIR_LOG], \n",
    "                            shell=False, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            logger.info(f\"{etl_phase}_log_{log_time} moved to {DIR_LOG} successfully\")\n",
    "        else:\n",
    "            logger.info(f\"Error: {result.stderr}\")\n",
    "            \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.error(f\"Command failed: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8215d6",
   "metadata": {},
   "source": [
    "# ETL functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fc9f20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIR_ROOT_PROJECT: /home/biyanbahtiar/pacmann/data_storage/dbt_x_luigi/pacbook_store\n",
      "DIR_TEMP_LOG: /home/biyanbahtiar/pacmann/data_storage/dbt_x_luigi/pacbook_store/pipeline/temp/logs\n",
      "DIR_TEMP_DATA: /home/biyanbahtiar/pacmann/data_storage/dbt_x_luigi/pacbook_store/pipeline/temp/data\n",
      "DIR_EXTRACT_QUERY: /home/biyanbahtiar/pacmann/data_storage/dbt_x_luigi/pacbook_store/pipeline/src_query/extract\n",
      "DIR_LOAD_QUERY: /home/biyanbahtiar/pacmann/data_storage/dbt_x_luigi/pacbook_store/pipeline/src_query/load\n",
      "DIR_LOAD_QUERY: /home/biyanbahtiar/pacmann/data_storage/dbt_x_luigi/pacbook_store/pipeline/src_query/dbt_transform\n",
      "DIR_LOG: /home/biyanbahtiar/pacmann/data_storage/dbt_x_luigi/pacbook_store/logs\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define DIR\n",
    "DIR_ROOT_PROJECT = os.getenv(\"DIR_ROOT_PROJECT\")\n",
    "DIR_TEMP_LOG = os.getenv(\"DIR_TEMP_LOG\")\n",
    "DIR_TEMP_DATA = os.getenv(\"DIR_TEMP_DATA\")\n",
    "DIR_EXTRACT_QUERY = os.getenv(\"DIR_EXTRACT_QUERY\")\n",
    "DIR_LOAD_QUERY = os.getenv(\"DIR_LOAD_QUERY\")\n",
    "DIR_DBT_TRANSFORM = os.getenv(\"DIR_DBT_TRANSFORM\")\n",
    "DIR_LOG = os.getenv(\"DIR_LOG\")\n",
    "\n",
    "print(f\"DIR_ROOT_PROJECT: {DIR_ROOT_PROJECT}\")\n",
    "print(f\"DIR_TEMP_LOG: {DIR_TEMP_LOG}\")\n",
    "print(f\"DIR_TEMP_DATA: {DIR_TEMP_DATA}\")\n",
    "print(f\"DIR_EXTRACT_QUERY: {DIR_EXTRACT_QUERY}\")\n",
    "print(f\"DIR_LOAD_QUERY: {DIR_LOAD_QUERY}\")\n",
    "print(f\"DIR_LOAD_QUERY: {DIR_DBT_TRANSFORM}\")\n",
    "print(f\"DIR_LOG: {DIR_LOG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25cbb366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalParams(luigi.Config):\n",
    "    tables_to_modify = [\n",
    "        \"country\",\n",
    "        \"address_status\",\n",
    "        \"author\",\n",
    "        \"book_language\",\n",
    "        \"publisher\",\n",
    "        \"shipping_method\",\n",
    "        \"order_status\",\n",
    "        \"customer\",\n",
    "        \"order_history\",\n",
    "        \"order_line\",\n",
    "        \"book_author\",\n",
    "        \"customer_address\",\n",
    "        \"address\",\n",
    "        \"book\",\n",
    "        \"cust_order\",\n",
    "    ]\n",
    "\n",
    "    result_tables = [\n",
    "        \"dim_customer\",\n",
    "        \"dim_address\",\n",
    "        \"dim_book\",\n",
    "        \"dim_author\",\n",
    "        \"bridge_book_author\",\n",
    "        \"dim_date\",\n",
    "        \"dim_time\",\n",
    "        \"fct_book_order\",\n",
    "        \"fct_customer_behavior\"\n",
    "    ]\n",
    "\n",
    "    CurrentTimestampParams = luigi.DateSecondParameter(default = datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a584069",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: unmatched '(' (3699571493.py, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 42\u001b[0;36m\u001b[0m\n\u001b[0;31m    df.to_csv(f\"{DIR_TEMP_DATA}/{table_name.replace(\"public.\", \"\")}.csv\", index=False)\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: unmatched '('\n"
     ]
    }
   ],
   "source": [
    "class Extract(luigi.Task):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.get_current_timestamp = GlobalParams().CurrentTimestampParams\n",
    "        self.table_to_extract = [\n",
    "            \"public.\"+table_name for table_name in GlobalParams().tables_to_modify\n",
    "        ]\n",
    "\n",
    "    def requires(self):\n",
    "        pass\n",
    "\n",
    "    def run(self):\n",
    "        logger = logging.getLogger(f\"extract_log_{self.get_current_timestamp}.log\")\n",
    "\n",
    "        # Check if handlers are already set to avoid duplicate logs on re-runs\n",
    "        if not logger.handlers:\n",
    "            handler = logging.FileHandler(f\"{DIR_TEMP_LOG}/extract_log_{self.get_current_timestamp}.log\")\n",
    "            formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "            logger.setLevel(logging.INFO)\n",
    "        \n",
    "        try:\n",
    "            # Define db connection engine\n",
    "            src_engine, _ = db_connection()\n",
    "\n",
    "            # Define the query using the SQL content\n",
    "            extract_query = read_sql_file(\n",
    "                file_path = f\"{DIR_EXTRACT_QUERY}/all-tables.sql\"\n",
    "            )\n",
    "\n",
    "            logger.info(\"==================================STARTING EXTRACT DATA=======================================\")\n",
    "            for table_name in self.table_to_extract:\n",
    "                start_time = time.time()\n",
    "\n",
    "                try:\n",
    "                    start_time_extract = time.time()\n",
    "                    # Read data into Dataframe\n",
    "                    df = pd.read_sql_query(extract_query.format(table_name=table_name), src_engine)\n",
    "                    \n",
    "                    # Write dataframe to .csv\n",
    "                    df.to_csv(f\"{DIR_TEMP_DATA}/{table_name.replace('public.', '')}.csv\", index=False)\n",
    "                    logger.info(f\"EXTRACT {table_name} - SUCCESS.\")\n",
    "\n",
    "                    end_time = time.time()  # Record end time\n",
    "                    execution_time = end_time - start_time_extract  # Calculate execution time\n",
    "                    \n",
    "                    # Get summary\n",
    "                    summary_data = {\n",
    "                        \"step\":\"extraction\",\n",
    "                        \"status\":\"success\",\n",
    "                        \"error_msg\":\"\",\n",
    "                        \"source\":\"pacbook_transactional_db\",\n",
    "                        \"table_name\":table_name,\n",
    "                        \"execution_time\":execution_time,\n",
    "                        \"etl_date\":self.get_current_timestamp\n",
    "                    }\n",
    "\n",
    "                    # load log to csv file\n",
    "                    log_to_csv(summary_data, f\"{DIR_LOG}/etl_log.csv\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    end_time = time.time()  # Record end time\n",
    "                    execution_time = end_time - start_time  # Calculate execution time\n",
    "                    \n",
    "                    # Get summary\n",
    "                    summary_data = {\n",
    "                        \"step\":\"extraction\",\n",
    "                        \"status\":\"failed\",\n",
    "                        \"error_msg\":str(e),\n",
    "                        \"source\":\"pacbook_transactional_db\",\n",
    "                        \"table_name\":table_name,\n",
    "                        \"execution_time\":execution_time,\n",
    "                        \"etl_date\":self.get_current_timestamp\n",
    "                    }\n",
    "\n",
    "                    # load log to csv file\n",
    "                    log_to_csv(summary_data, f\"{DIR_LOG}/etl_log.csv\")\n",
    "                    logger.error(f\"Failed to extract {table_name} tables\")\n",
    "                    raise Exception(f\"Failed to extract {table_name} tables: {e}\")\n",
    "                  \n",
    "        except Exception as e:\n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "             \n",
    "            # Get summary\n",
    "            summary_data = {\n",
    "                \"step\":\"extraction\",\n",
    "                \"status\":\"failed\",\n",
    "                \"error_msg\":str(e),\n",
    "                \"source\":\"pacbook_transactional_db\",\n",
    "                \"table_name\":self.table_to_extract,\n",
    "                \"execution_time\":execution_time,\n",
    "                \"etl_date\":self.get_current_timestamp\n",
    "            }\n",
    "            \n",
    "            # load log to csv file\n",
    "            log_to_csv(summary_data, f\"{DIR_LOG}/etl_log.csv\")\n",
    "            \n",
    "            # Write exception\n",
    "            logger.error(f\"Extract All Tables From Sources - FAILED\")\n",
    "            raise Exception(f\"FAILED to execute EXTRACT TASK: {e}\")\n",
    "        \n",
    "        logger.info(\"==================================ENDING EXTRACT DATA=======================================\")\n",
    "        \n",
    "        move_temp_to_log_dir(logger, DIR_TEMP_LOG, DIR_LOG, etl_phase=\"extract\", log_time=self.get_current_timestamp)\n",
    "\n",
    "    def output(self):\n",
    "        outputs = []\n",
    "        for table_name in self.table_to_extract:\n",
    "            outputs.append(luigi.LocalTarget(f\"{DIR_TEMP_DATA}/{table_name.replace('public.', '')}.csv\"))\n",
    "\n",
    "        outputs.append(luigi.LocalTarget(f\"{DIR_LOG}/etl_log.csv\"))\n",
    "\n",
    "        outputs.append(luigi.LocalTarget(f\"{DIR_LOG}/extract_log_{self.get_current_timestamp}.log\"))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbbb536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Load(luigi.Task):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.get_current_timestamp = GlobalParams().CurrentTimestampParams\n",
    "\n",
    "    def requires(self):\n",
    "        return Extract()\n",
    "        \n",
    "    def run(self):\n",
    "        logger = logging.getLogger(f\"load_log_{self.get_current_timestamp}.log\")\n",
    "\n",
    "        # Check if handlers are already set to avoid duplicate logs on re-runs\n",
    "        if not logger.handlers:\n",
    "            handler = logging.FileHandler(f\"{DIR_TEMP_LOG}/load_log_{self.get_current_timestamp}.log\")\n",
    "            formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "            logger.setLevel(logging.INFO)\n",
    "\n",
    "        # ------ 1. Define SQL files path ------\n",
    "        truncate_sql_file = \"truncate_tables.sql\"\n",
    "        \n",
    "        table_configs = {\n",
    "            table_name: {\"sql_file\":f\"staging-{table_name}.sql\"}\n",
    "            for table_name in GlobalParams().tables_to_modify\n",
    "        }\n",
    "        \n",
    "        # ------ 2. Read SQL file and store in variable ------\n",
    "        truncate_queries_content = \"\"\n",
    "\n",
    "        try:\n",
    "            # Read the truncate SQL file\n",
    "            truncate_queries_content = read_sql_file(\n",
    "                file_path = f\"{DIR_LOAD_QUERY}/{truncate_sql_file}\"\n",
    "            )\n",
    "    \n",
    "            logger.info(\"Read Truncate Query - SUCCESS\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            logger.error(\"Read Truncate Query - FAILED\")\n",
    "            raise Exception(f\"Failed to read Truncate SQL queries: {e}\")\n",
    "\n",
    "        # ------ 3. Read extracted CSV data ------\n",
    "        input_paths = [i.path for i in self.input()]\n",
    "\n",
    "        try:\n",
    "            for table_name, config in table_configs.items():\n",
    "                expected_file_path = f\"{DIR_TEMP_DATA}/{table_name}.csv\"\n",
    "                if expected_file_path in input_paths:\n",
    "                    config[\"df\"] = pd.read_csv(expected_file_path)\n",
    "            logger.info(\"Read extracted CSV - SUCCESS\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(\"Read extracted CSV - FAILED\")\n",
    "            raise Exception(f\"Failed to read CSV: {e}\")\n",
    "            \n",
    "        # ------ 4. Execute queries to create schema if not exists and truncate ------\n",
    "        # Establish connections to DWH\n",
    "        dwh_engine = None\n",
    "        try:\n",
    "            _, dwh_engine = db_connection()\n",
    "            logger.info(f\"Connect to DWH - SUCCESS\")\n",
    "            \n",
    "        except Exception:\n",
    "            logger.error(f\"Connect to DWH - FAILED\")\n",
    "            raise Exception(\"Failed to connect to Data Warehouse\")\n",
    "\n",
    "        # Create staging schema and truncate all tables before load\n",
    "        try:            \n",
    "            # Create session\n",
    "            Session = sessionmaker(bind = dwh_engine)\n",
    "            session = Session()\n",
    "\n",
    "            # Split the SQL queries if multiple queries are present\n",
    "            truncate_query = truncate_queries_content.split(\"--\")\n",
    "    \n",
    "            # Remove newline characters and leading/trailing whitespaces\n",
    "            truncate_query = [query.strip() for query in truncate_query if query.strip()]\n",
    "            \n",
    "            # Execute create schema query and truncate query\n",
    "            for query in truncate_query:\n",
    "                query = sqlalchemy.text(query)\n",
    "                session.execute(query)\n",
    "\n",
    "            session.commit()\n",
    "            session.close()\n",
    "\n",
    "            logger.info(f\"Create/Truncate staging Schema in DWH - SUCCESS\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Create/Truncate staging Schema in DWH - FAILED : {e}\")\n",
    "            raise Exception(\"Failed to Create/Truncate public Schema in DWH\")\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "        # 5. Load data into staging schema DWH\n",
    "        # Record start time for loading tables\n",
    "        start_time = time.time()  \n",
    "        logger.info(\"==================================STARTING LOAD DATA=======================================\")\n",
    "        # Load to tables\n",
    "        try:\n",
    "            #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "            try:\n",
    "                # Create session\n",
    "                Session = sessionmaker(bind = dwh_engine)\n",
    "                session = Session()\n",
    "   \n",
    "                for table_name, config in table_configs.items():\n",
    "                    start_time_load = time.time()\n",
    "                    config[\"df\"].to_sql(\n",
    "                        table_name,\n",
    "                        con = dwh_engine,\n",
    "                        if_exists = \"append\",\n",
    "                        index = False,\n",
    "                        schema = \"staging\"\n",
    "                    )\n",
    "\n",
    "                    # Record end time for loading tables\n",
    "                    end_time = time.time()  \n",
    "                    execution_time = end_time - start_time_load\n",
    "                    \n",
    "                    # Get summary\n",
    "                    summary_data = {\n",
    "                        \"step\":\"load\",\n",
    "                        \"status\":\"success\",\n",
    "                        \"error_msg\":\"\",\n",
    "                        \"source\":\"extracted csv from Extract()\",\n",
    "                        \"table_name\":table_name,\n",
    "                        \"execution_time\":execution_time,\n",
    "                        \"etl_date\":self.get_current_timestamp\n",
    "                    }\n",
    "\n",
    "                    logger.info(f\"LOAD Table {table_name} To staging schema DWH - SUCCESS\")\n",
    "                    \n",
    "                    # load log to csv file\n",
    "                    log_to_csv(summary_data, f\"{DIR_LOG}/etl_log.csv\")\n",
    "                \n",
    "                logger.info(f\"LOAD All Tables To staging schema DWH - SUCCESS\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"LOAD All Tables To staging schema DWH - FAILED: {e}\")\n",
    "                raise Exception(\"Failed Load Tables To staging schema DWH\")\n",
    "            \n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "        except Exception as e:\n",
    "            end_time = time.time()  \n",
    "            execution_time = end_time - start_time\n",
    "            \n",
    "            # Get summary\n",
    "            summary_data = {\n",
    "                \"step\":\"load\",\n",
    "                \"status\":\"failed\",\n",
    "                \"error_msg\":str(e),\n",
    "                \"source\":\"extracted csv in temp\",\n",
    "                \"table_name\":list(table_configs.keys()),\n",
    "                \"execution_time\":execution_time,\n",
    "                \"etl_date\":self.get_current_timestamp\n",
    "            }\n",
    "            \n",
    "            # load log to csv file\n",
    "            log_to_csv(summary_data, f\"{DIR_LOG}/etl_log.csv\")\n",
    "            \n",
    "            logger.error(\"LOAD All Tables To DWH - FAILED\")\n",
    "            raise Exception(\"Failed Load Tables To DWH\")   \n",
    "        \n",
    "        logger.info(\"==================================ENDING LOAD DATA=======================================\")\n",
    "\n",
    "    move_temp_to_log_dir(logger, DIR_TEMP_LOG, DIR_LOG, etl_phase=\"load\", log_time=self.get_current_timestamp)\n",
    "        \n",
    "    #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "    def output(self):\n",
    "        return [luigi.LocalTarget(f\"{DIR_LOG}/load_log_{self.get_current_timestamp}.log\"),\n",
    "                luigi.LocalTarget(f\"{DIR_LOG}/etl_log.csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a8985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform(luigi.Task):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.get_current_timestamp = GlobalParams().CurrentTimestampParams\n",
    "        self.result_tables = GlobalParams().result_tables\n",
    "    \n",
    "    def requires(self):\n",
    "        return Load()\n",
    "    \n",
    "    def run(self):\n",
    "        logger = logging.getLogger(f'transform_log_{self.get_current_timestamp}.log')\n",
    "\n",
    "        # Check if handlers are already set to avoid duplicate logs on re-runs\n",
    "        if not logger.handlers:\n",
    "            handler = logging.FileHandler(f'{DIR_TEMP_LOG}/transform_log_{self.get_current_timestamp}.log')\n",
    "            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "            logger.setLevel(logging.INFO)\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "        # Record start time for transform tables\n",
    "        start_time = time.time()\n",
    "        logging.info(\"==================================STARTING TRANSFROM DATA=======================================\")  \n",
    "               \n",
    "        # Transform to dimensions tables\n",
    "        try:\n",
    "            with open (file = f'{DIR_TEMP_LOG}/transform_log_{self.get_current_timestamp}.log', mode = 'a') as f :\n",
    "                subprocess.run(\n",
    "                    f\"cd {DIR_DBT_TRANSFORM} && dbt debug && dbt deps && dbt seed && dbt run && dbt snapshot && dbt test\",\n",
    "                    stdout = f,\n",
    "                    stderr = subprocess.PIPE,\n",
    "                    text = True,\n",
    "                    shell = True,\n",
    "                    check = True\n",
    "                )\n",
    "        \n",
    "            # Record end time for loading tables\n",
    "            end_time = time.time()  \n",
    "            execution_time = end_time - start_time\n",
    "            logging.info(f\"Transform to All Dimensions and Fact Tables - SUCCESS\")\n",
    "\n",
    "            # Get summary\n",
    "            summary_data = {\n",
    "                \"step\":\"transform\",\n",
    "                \"status\":\"success\",\n",
    "                \"error_msg\":\"\",\n",
    "                \"source\":\"staging schema pacbook_dwh\",\n",
    "                \"table_name\":self.result_tables,\n",
    "                \"execution_time\":execution_time,\n",
    "                \"etl_date\":self.get_current_timestamp\n",
    "            }\n",
    "            \n",
    "            # load log to csv file\n",
    "            log_to_csv(summary_data, f\"{DIR_LOG}/etl_log.csv\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Transform to All Dimensions and Fact Tables - FAILED\")\n",
    "        \n",
    "            # Get summary\n",
    "            summary_data = {\n",
    "                \"step\":\"transform\",\n",
    "                \"status\":\"failed\",\n",
    "                \"error_msg\":str(e),\n",
    "                \"source\":\"staging schema pacbook_dwh\",\n",
    "                \"table_name\":self.result_tables,\n",
    "                \"execution_time\":execution_time,\n",
    "                \"etl_date\":self.get_current_timestamp\n",
    "            }\n",
    "            \n",
    "            # load log to csv file\n",
    "            log_to_csv(summary_data, f\"{DIR_LOG}/etl_log.csv\")\n",
    "            \n",
    "            logging.error(f\"Transform Tables - FAILED: {e}\")\n",
    "            raise Exception('Failed Transforming Tables')   \n",
    "        \n",
    "        logging.info(\"==================================ENDING TRANSFROM DATA=======================================\")\n",
    "\n",
    "        move_temp_to_log_dir(logger, DIR_TEMP_LOG, DIR_LOG, etl_phase=\"transform\", log_time=self.get_current_timestamp)\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "    def output(self):\n",
    "        return [luigi.LocalTarget(f\"{DIR_LOG}/transform_log_{self.get_current_timestamp}.log\"),\n",
    "                luigi.LocalTarget(f\"{DIR_LOG}/etl_log.csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994dd5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if Extract() is complete\n",
      "INFO: Informed scheduler that task   Extract__99914b932b   has status   DONE\n",
      "DEBUG: Checking if Load() is complete\n",
      "DEBUG: Checking if Extract() is complete\n",
      "INFO: Informed scheduler that task   Load__99914b932b   has status   PENDING\n",
      "INFO: Informed scheduler that task   Extract__99914b932b   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 1\n",
      "INFO: [pid 121497] Worker Worker(salt=2845575323, workers=1, host=LAPTOP-JMN90Q2L, username=biyanbahtiar, pid=121497) running   Load()\n",
      "INFO: [pid 121497] Worker Worker(salt=2845575323, workers=1, host=LAPTOP-JMN90Q2L, username=biyanbahtiar, pid=121497) done      Load()\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   Load__99914b932b   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=2845575323, workers=1, host=LAPTOP-JMN90Q2L, username=biyanbahtiar, pid=121497) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 2 tasks of which:\n",
      "* 1 complete ones were encountered:\n",
      "    - 1 Extract()\n",
      "* 1 ran successfully:\n",
      "    - 1 Load()\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    luigi.build([Extract(), Load(), Transform()], local_scheduler = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c52a0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9739d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
