{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aeecef5-2db9-4232-b6d6-867a7389d8e9",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "317cfaf5-4a1a-4c75-8c69-06b4959f4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Any\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.sql import quoted_name\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import stat\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    sum as _sum, min as _min, max as _max, avg as _avg, sha2, col, when, coalesce, lit, concat_ws, first, countDistinct\n",
    ")\n",
    "from pyspark.sql.types import DataType, DateType, TimestampType, NumericType, DecimalType\n",
    "import traceback\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dae3be-c1e3-4a4f-9df3-f1866e037fe0",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c28bd88-b8e8-4147-be01-eddebfab94ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "\n",
    "DB_HOST_SOURCE = os.getenv(\"DB_HOST_SOURCE\")\n",
    "DB_USER_SOURCE = os.getenv(\"DB_USER_SOURCE\")\n",
    "DB_PASS_SOURCE = os.getenv(\"DB_PASS_SOURCE\")\n",
    "DB_PORT_SOURCE = os.getenv(\"DB_PORT_SOURCE\")\n",
    "\n",
    "DB_HOST_TARGET = os.getenv(\"DB_HOST_TARGET\")\n",
    "DB_USER_TARGET = os.getenv(\"DB_USER_TARGET\")\n",
    "DB_PASS_TARGET = os.getenv(\"DB_PASS_TARGET\")\n",
    "DB_PORT_TARGET = os.getenv(\"DB_PORT_TARGET\")\n",
    "\n",
    "DB_NAME_SOURCE = os.getenv(\"DB_NAME_SOURCE\")\n",
    "DB_NAME_STG = os.getenv(\"DB_NAME_STG\")\n",
    "DB_NAME_LOG = os.getenv(\"DB_NAME_LOG\")\n",
    "DB_NAME_WH = os.getenv(\"DB_NAME_WH\")\n",
    "\n",
    "API_PATH = os.getenv(\"API_PATH\")\n",
    "\n",
    "# Create URL link for each database connection\n",
    "def source_engine():\n",
    "    DB_URL = f\"jdbc:postgresql://{DB_HOST_SOURCE}:{DB_PORT_SOURCE}/{DB_NAME_SOURCE}\"\n",
    "    return DB_URL, DB_USER_SOURCE, DB_PASS_SOURCE, DB_NAME_SOURCE\n",
    "\n",
    "def stg_engine():\n",
    "    DB_URL = f\"jdbc:postgresql://{DB_HOST_TARGET}:{DB_PORT_TARGET}/{DB_NAME_STG}\"\n",
    "    return DB_URL, DB_USER_TARGET, DB_PASS_TARGET, DB_NAME_STG\n",
    "\n",
    "def log_engine():\n",
    "    DB_URL = f\"jdbc:postgresql://{DB_HOST_TARGET}:{DB_PORT_TARGET}/{DB_NAME_LOG}\"\n",
    "    return DB_URL, DB_USER_TARGET, DB_PASS_TARGET, DB_NAME_LOG\n",
    "\n",
    "def wh_engine():\n",
    "    DB_URL = f\"jdbc:postgresql://{DB_HOST_TARGET}:{DB_PORT_TARGET}/{DB_NAME_WH}\"\n",
    "    return DB_URL, DB_USER_TARGET, DB_PASS_TARGET, DB_NAME_WH\n",
    "\n",
    "def wh_engine_sqlalchemy():\n",
    "    return create_engine(f\"postgresql://{DB_USER_TARGET}:{DB_PASS_TARGET}@{DB_HOST_TARGET}:{DB_PORT_TARGET}/{DB_NAME_WH}\")\n",
    "\n",
    "def load_log(spark: SparkSession, log_msg):\n",
    "    DB_URL, DB_USER, DB_PASS, DB_NAME = log_engine()\n",
    "    table_name = \"etl_log\"\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "    }\n",
    "\n",
    "    log_msg.write.jdbc(url = DB_URL,\n",
    "                      table = table_name,\n",
    "                      mode = \"append\",\n",
    "                      properties = connection_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138f5ada-42ba-44e6-9e9f-92926090d9d4",
   "metadata": {},
   "source": [
    "# Define spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2f1ac3-3626-480d-8a85-32f4a9885715",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL_Pipeline\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfd5299-12ca-4fee-9f34-3ea25afa39a2",
   "metadata": {},
   "source": [
    "# Profiling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e754e657-b853-4079-b43f-b97b273f8f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProfileData():\n",
    "    @staticmethod\n",
    "    def table_profiler(spark: SparkSession, table: str, source_type: str = \"db\", **kwargs) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Profile a table from database, CSV, or API, returning column statistics including data type,\n",
    "        null percentage, duplicates, and max/mode.\n",
    "    \n",
    "        Args:\n",
    "            spark: SparkSession object\n",
    "            table: Name of the table or data source identifier (e.g., table name, file name, or API endpoint)\n",
    "            source_type: Source type ('db', 'csv', or 'api') (default: 'db')\n",
    "            **kwargs: Additional parameters:\n",
    "                - For 'db': db_url, connection_properties\n",
    "                - For 'csv': path (base directory for CSV files)\n",
    "                - For 'api': api_url, params (optional query parameters)\n",
    "    \n",
    "        Returns:\n",
    "            List of dictionaries containing profiling results for each column\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        try:\n",
    "            # Load data based on source type\n",
    "            if source_type == \"db\":\n",
    "                db_url = kwargs.get(\"db_url\")\n",
    "                connection_properties = kwargs.get(\"connection_properties\")\n",
    "                if not db_url or not connection_properties:\n",
    "                    raise ValueError(\"db_url and connection_properties required for db source\")\n",
    "                df = spark.read.jdbc(url=db_url, table=table, properties=connection_properties)\n",
    "            \n",
    "            elif source_type == \"csv\":\n",
    "                path = kwargs.get(\"path\", \"data/\")\n",
    "                df = spark.read.csv(f\"{path}{table}\", header=True)\n",
    "            \n",
    "            elif source_type == \"api\":\n",
    "                api_url = kwargs.get(\"api_url\")\n",
    "                params = kwargs.get(\"params\", {})\n",
    "                if not api_url:\n",
    "                    raise ValueError(\"api_url required for api source\")\n",
    "                response = requests.get(url=api_url, params=params)\n",
    "                if response.status_code != 200:\n",
    "                    raise Exception(f\"API request fail with status code: {response.status_code}\")\n",
    "                json_data = response.json()\n",
    "                if not json_data:\n",
    "                    print(f\"No data from API for {table}\")\n",
    "                    return results\n",
    "                df = spark.createDataFrame(pd.DataFrame(json_data))\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported source_type: {source_type}\")\n",
    "    \n",
    "            # Get basic stats\n",
    "            num_rows = df.count()\n",
    "            num_cols = len(df.columns)\n",
    "            \n",
    "            # Profile each column\n",
    "            for column in df.columns:\n",
    "                try:\n",
    "                    # Ensure column name is a string\n",
    "                    column = str(column)\n",
    "                    \n",
    "                    col_type = df.schema[column].dataType\n",
    "                    col_type_str = str(col_type.simpleString())\n",
    "                    \n",
    "                    # Null percentage\n",
    "                    null_count = df.filter((df[column].isNull()) | (df[column]==\"\")).count()\n",
    "                    null_pct = (null_count / num_rows * 100) if num_rows > 0 else 0.0\n",
    "                    \n",
    "                    # Duplicate count\n",
    "                    distinct_count = df.select(countDistinct(col(column))).first()[0]\n",
    "                    duplicate_count = num_rows - distinct_count if num_rows > 0 else 0\n",
    "                    \n",
    "                    # Min and max for numeric/date or mode for others\n",
    "                    if isinstance(col_type, (DateType, TimestampType, NumericType)):\n",
    "                        max_val = df.select(_max(col(column))).first()[0]\n",
    "                        max_val = str(max_val) if max_val is not None else None\n",
    "                        \n",
    "                        min_val = df.select(_min(col(column))).first()[0]\n",
    "                        min_val = str(min_val) if min_val is not None else None\n",
    "\n",
    "                        mode_val = None\n",
    "                    else:\n",
    "                        mode_val = df.groupBy(col(column)).count().orderBy(col(\"count\").desc()).first()\n",
    "                        mode_val = str(mode_val[column]) if mode_val is not None else None\n",
    "                        \n",
    "                        max_val = None\n",
    "                        min_val = None\n",
    "\n",
    "                    # Possible data value\n",
    "                    distinct_value = [row[column] for row in df.select(col(column)).distinct().collect()]\n",
    "                    \n",
    "                    # Append results\n",
    "                    results.append({\n",
    "                        \"PIC\": \"Biyan\",\n",
    "                        \"date\": datetime.now(),\n",
    "                        \"table_name\": table,\n",
    "                        \"column_name\": column,\n",
    "                        \"num_rows\": num_rows,\n",
    "                        \"num_columns\": num_cols,\n",
    "                        \"data_type\": col_type_str,\n",
    "                        \"null_percentage\": null_pct,\n",
    "                        \"duplicate_count\": duplicate_count,\n",
    "                        \"min\": min_val,\n",
    "                        \"max\": max_val,\n",
    "                        \"mode\": mode_val,\n",
    "                        \"distinct_value\": distinct_value\n",
    "                    })\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error profiling column {column} in table {table}: {str(e)}\\n{traceback.format_exc()}\")\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error profiling table {table} from {source_type}: {str(e)}\\n{traceback.format_exc()}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    @classmethod\n",
    "    def from_database(cls, spark: SparkSession) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Profile all tables in a PostgreSQL database, including column count, row count,\n",
    "        and per-column statistics (data type, null percentage, duplicates, max/mode).\n",
    "    \n",
    "        Args:\n",
    "            spark: SparkSession object\n",
    "    \n",
    "        Returns:\n",
    "            Pandas DataFrame with profiling results\n",
    "        \"\"\"\n",
    "        # Initialize results list\n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            # Get database connection details\n",
    "            db_url, db_user, db_pass, db_name = source_engine()\n",
    "            connection_properties = {\n",
    "                \"user\": db_user,\n",
    "                \"password\": db_pass,\n",
    "                \"driver\": \"org.postgresql.Driver\"\n",
    "            }\n",
    "        \n",
    "            # Get list of tables\n",
    "            tables_query = f\"(SELECT table_name FROM information_schema.tables WHERE table_schema = 'public') AS tables\"\n",
    "            tables_df = spark.read.jdbc(url=db_url, table=tables_query, properties=connection_properties)\n",
    "            tables = [row.table_name for row in tables_df.collect()]\n",
    "    \n",
    "            for table in tables:\n",
    "                # Profile each table\n",
    "                res = cls.table_profiler(\n",
    "                    spark=spark,\n",
    "                    table=table,\n",
    "                    source_type='db',\n",
    "                    db_url=db_url,\n",
    "                    connection_properties=connection_properties\n",
    "                )\n",
    "                results = results + res\n",
    "    \n",
    "            # Convert to pandas DataFrame\n",
    "            result_df = pd.DataFrame(results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error profiling database: {str(e)}\\n{traceback.format_exc()}\")\n",
    "            result_df = pd.DataFrame()\n",
    "    \n",
    "        finally:\n",
    "            return result_df\n",
    "    \n",
    "    @classmethod\n",
    "    def from_csv(cls, spark: SparkSession) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Profile all CSV files in a directory, returning column statistics as a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            spark: SparkSession object\n",
    "        \n",
    "        Returns:\n",
    "            Pandas DataFrame with profiling results\n",
    "        \"\"\"\n",
    "        path = \"data/\"\n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            for file_name in os.listdir(path):\n",
    "                fullpath = os.path.join(path, file_name)\n",
    "\n",
    "                _, file_extension = os.path.splitext(file_name)\n",
    "                    \n",
    "                if file_extension.lower() == '.csv':                  \n",
    "                    # Start profiling table\n",
    "                    res = cls.table_profiler(\n",
    "                        spark=spark,\n",
    "                        table=file_name,\n",
    "                        source_type=\"csv\",\n",
    "                        path=path\n",
    "                    )\n",
    "                    results = results + res\n",
    "    \n",
    "            result_df = pd.DataFrame(results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error profiling csv: {str(e)}\\n{traceback.format_exc()}\")\n",
    "            result_df = pd.DataFrame()\n",
    "            \n",
    "        finally:\n",
    "            return result_df\n",
    "    \n",
    "    @classmethod\n",
    "    def from_api(cls, spark: SparkSession, api_url: str, params: Dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Profile data from an API endpoint, returning column statistics as a DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            spark: SparkSession object\n",
    "            api_url: API endpoint URL\n",
    "            params: Dictionary of API query parameters\n",
    "        \n",
    "        Returns:\n",
    "            Pandas DataFrame with profiling results        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Start profiling table\n",
    "            res = cls.table_profiler(\n",
    "                spark=spark,\n",
    "                table=api_url,\n",
    "                source_type=\"api\",\n",
    "                api_url=api_url,\n",
    "                params=params\n",
    "            )\n",
    "    \n",
    "            result_df = pd.DataFrame(res)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error profiling api: {str(e)}\\n{traceback.format_exc()}\")\n",
    "            result_df = pd.DataFrame()\n",
    "            \n",
    "        finally:\n",
    "            return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b5da1-9425-47fe-9bef-5c9156a21438",
   "metadata": {},
   "source": [
    "# Extract Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e677cda-c85b-41cb-864a-6314079c8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extract():\n",
    "    @staticmethod\n",
    "    def from_database(spark: SparkSession, table_to_extract: List, source_type: str = \"source\", write_log: bool = True) -> Optional[Dict]:\n",
    "        current_timestamp = datetime.now()\n",
    "        df_dict = {}\n",
    "        \n",
    "        try:\n",
    "            # Database connection\n",
    "            if source_type == \"source\":\n",
    "                source_db_url, source_db_user, source_db_pass, source_db_name = source_engine()\n",
    "                step = \"staging\"\n",
    "            elif source_type == \"staging\":\n",
    "                source_db_url, source_db_user, source_db_pass, source_db_name = stg_engine()\n",
    "                step = \"warehouse\"\n",
    "            else:\n",
    "                raise ValueError(\"Unknown source type\")\n",
    "                return None\n",
    "            \n",
    "            # Connection properties\n",
    "            connection_properties = {\n",
    "                \"user\": source_db_user,\n",
    "                \"password\": source_db_pass,\n",
    "                \"driver\": \"org.postgresql.Driver\"\n",
    "            }\n",
    "\n",
    "            # Loop and extract table from table_to_extract\n",
    "            for table_name in table_to_extract:\n",
    "                # Read from postgres\n",
    "                df = spark.read.jdbc(\n",
    "                    url=source_db_url,\n",
    "                    table=table_name,\n",
    "                    properties=connection_properties\n",
    "                )\n",
    "\n",
    "                if df.count() == 0:\n",
    "                    raise ValueError(f\"{table_name} is empty\")\n",
    "\n",
    "                df_dict[table_name] = df\n",
    "                \n",
    "                # Log extract success\n",
    "                if write_log:\n",
    "                    log_msg = spark.createDataFrame(\n",
    "                        [(step, f\"{source_type}_extraction\", \"success\", source_db_name, table_name, current_timestamp)],\n",
    "                        ['step', 'process', 'status', 'source', 'table_name', 'etl_date']\n",
    "                    )\n",
    "                    load_log(spark, log_msg)\n",
    "            # return dictionary of spark dataframe\n",
    "            return df_dict\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log failure\n",
    "            if write_log:\n",
    "                log_msg = spark.createDataFrame(\n",
    "                    [(step, f\"{source_type}_extraction\", \"fail\", source_db_name, \"\", current_timestamp, str(e))],\n",
    "                    ['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg']\n",
    "                )\n",
    "                load_log(spark, log_msg) \n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def from_csv(spark: SparkSession, path = \"data/\", step=\"staging\", write_log: bool = True) -> Optional[DataFrame]:\n",
    "        current_timestamp = datetime.now()\n",
    "        df_dict = {}\n",
    "        try:\n",
    "            for file_name in os.listdir(path):\n",
    "                fullpath = os.path.join(path, file_name)\n",
    "            \n",
    "                table_name, file_extension = os.path.splitext(file_name)\n",
    "                    \n",
    "                if file_extension.lower() == '.csv': \n",
    "                    df = spark.read.csv(f\"{path}{file_name}\", header=True, inferSchema=True)\n",
    "                    if df.count() == 0:\n",
    "                        raise ValueError(f\"{table_name} is empty\")\n",
    "                    \n",
    "                    df_dict[table_name] = df\n",
    "                    \n",
    "                    if write_log:\n",
    "                        log_msg = spark.createDataFrame(\n",
    "                            [(step, \"extraction\", \"success\", f\"{path}{file_name}\", file_name, current_timestamp)],\n",
    "                            ['step', 'process', 'status', 'source', 'table_name', 'etl_date']\n",
    "                        )\n",
    "                        load_log(spark, log_msg)\n",
    "                    \n",
    "            return df_dict\n",
    "        \n",
    "        except Exception as e:\n",
    "            if write_log:\n",
    "                log_msg = spark.createDataFrame(\n",
    "                    [(step, \"extraction\", \"fail\", f\"{path}{file_name}\", file_name, current_timestamp, str(e))],\n",
    "                    ['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg']\n",
    "                )\n",
    "                load_log(spark, log_msg)\n",
    "            \n",
    "            print(f\"Error extracting CSV {file_name}: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_api(spark: SparkSession, start_date: str, end_date: str, write_log: bool = True) -> Optional[DataFrame]:\n",
    "        \"\"\"\n",
    "        Extract data from an API and convert to Spark DataFrame with logging.\n",
    "        \n",
    "        Args:\n",
    "            spark: SparkSession object\n",
    "            ds: Date string for API query\n",
    "            write_log: Whether to log the extraction process (default: False)\n",
    "        \n",
    "        Returns:\n",
    "            Spark DataFrame or None if extraction fails\n",
    "        \"\"\"\n",
    "        current_timestamp = datetime.now()\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                url=API_PATH,\n",
    "                params={\"start_date\": start_date, \"end_date\": end_date}\n",
    "            )\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                raise Exception(f\"API request fail with status code: {response.status_code}\")\n",
    "            \n",
    "            json_data = response.json()\n",
    "            if not json_data:\n",
    "                if write_log:\n",
    "                    log_msg = spark.createDataFrame(\n",
    "                        [(\"staging\", \"extraction\", \"skipped\", \"api\", \"milestone\", current_timestamp, \"No new data in API\")],\n",
    "                        ['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg']\n",
    "                    )\n",
    "                    load_log(spark, log_msg)\n",
    "                return None\n",
    "            \n",
    "            # Convert JSON to Spark DataFrame\n",
    "            df = spark.createDataFrame(json_data)\n",
    "            \n",
    "            if write_log:\n",
    "                log_msg = spark.createDataFrame(\n",
    "                    [(\"staging\", \"extraction\", \"success\", \"api\", \"milestone\", current_timestamp)],\n",
    "                    ['step', 'process', 'status', 'source', 'table_name', 'etl_date']\n",
    "                )\n",
    "                load_log(spark, log_msg)\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            if write_log:\n",
    "                log_msg = spark.createDataFrame(\n",
    "                    [(\"staging\", \"extraction\", \"fail\", \"api\", \"data\", current_timestamp, str(e))],\n",
    "                    ['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg']\n",
    "                )\n",
    "                load_log(spark, log_msg)\n",
    "            \n",
    "            print(f\"Error extracting API data: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c75a0-6b79-410d-8207-6058297ef237",
   "metadata": {},
   "source": [
    "# Load Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c098541a-5f8a-4bcb-b6d7-107744a2416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Load():\n",
    "    @staticmethod\n",
    "    def to_staging(spark: SparkSession, df_dict):\n",
    "        process = \"load_to_staging\"\n",
    "        try:\n",
    "            # Define current timestamp for logging\n",
    "            current_timestamp = datetime.now()\n",
    "\n",
    "            # Define connection properties\n",
    "            db_url, db_user, db_pass, db_name = stg_engine()\n",
    "            properties = {\n",
    "                \"user\": db_user,\n",
    "                \"password\": db_pass\n",
    "            }\n",
    "    \n",
    "            for table_name, spark_df in df_dict.items():\n",
    "                # Check if any dataframe row count = 0\n",
    "                if spark_df.count() == 0:\n",
    "                    raise ValueError(f\"Dataframe in table: {table_name} is empty\")\n",
    "\n",
    "                # load data\n",
    "                spark_df.write.jdbc(url = db_url,\n",
    "                            table = table_name,\n",
    "                            mode = \"overwrite\",\n",
    "                            properties = properties)\n",
    "            \n",
    "            # Structure log message\n",
    "            error_msg = \"\"\n",
    "            status = \"success\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Structure log message\n",
    "            # Capture full traceback information\n",
    "            tb_str = traceback.format_exc()\n",
    "            error_msg = f\"\"\"\n",
    "            Fail to perform {process} for table '{table_name}'.\n",
    "            \n",
    "            Full Traceback:\n",
    "            {tb_str}\n",
    "            \"\"\"\n",
    "            status = \"fail\"\n",
    "            \n",
    "        finally:\n",
    "            # log message\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"staging\", process, status, \"source transformation result\", \"\", current_timestamp, error_msg)])\\\n",
    "                .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\", \"error_msg\"])\n",
    "         \n",
    "            load_log(spark, log_msg)\n",
    "\n",
    "    @staticmethod\n",
    "    def to_warehouse(spark: SparkSession, df_dict):\n",
    "        current_timestamp = datetime.now()\n",
    "        db_url, db_user, db_pass, db_name = wh_engine()\n",
    "        properties = {\n",
    "            \"user\": db_user,\n",
    "            \"password\": db_pass\n",
    "        }\n",
    "        \n",
    "        for table_name, df in df_dict.items():\n",
    "            if table_name == \"startup_event\":\n",
    "                wh_table_name = f\"fct_{table_name}\"\n",
    "            else:\n",
    "                wh_table_name = f\"dim_{table_name}\"\n",
    "                \n",
    "            try:\n",
    "                # truncate table with sqlalchemy\n",
    "                conn = wh_engine_sqlalchemy()\n",
    "        \n",
    "                with conn.connect() as connection:\n",
    "                    # Execute the TRUNCATE TABLE command\n",
    "                    quoted_table_name = quoted_name(wh_table_name, quote=True)\n",
    "                    connection.execute(text(f\"TRUNCATE TABLE {quoted_table_name} RESTART IDENTITY CASCADE\"))\n",
    "                    connection.commit()\n",
    "                conn.dispose()\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_msg = spark.sparkContext\\\n",
    "                    .parallelize([(\"warehouse\", \"load\", \"fail\", \"validation passed tables\", table_name, current_timestamp, str(e))])\\\n",
    "                    .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "                load_log(spark, log_msg)\n",
    "            \n",
    "            try:\n",
    "                # load data               \n",
    "                df.write.jdbc(url = db_url,\n",
    "                            table = wh_table_name,\n",
    "                            mode = \"append\",\n",
    "                            properties = properties)\n",
    "                \n",
    "                #log message\n",
    "                log_msg = spark.sparkContext\\\n",
    "                    .parallelize([(\"warehouse\", \"load\", \"success\", \"validation passed tables\", table_name, current_timestamp)])\\\n",
    "                    .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                \n",
    "                # log message\n",
    "                log_msg = spark.sparkContext\\\n",
    "                    .parallelize([(\"warehouse\", \"load\", \"fail\", \"validation passed tables\", table_name, current_timestamp, str(e))])\\\n",
    "                    .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "            \n",
    "            finally:\n",
    "                load_log(spark, log_msg)\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e2b6f-7bfd-4a66-ad71-5e02c9ebcb5d",
   "metadata": {},
   "source": [
    "# Transform Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e886796-adda-4668-bb14-aa17e4ce8ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform():\n",
    "    @staticmethod\n",
    "    def _hashing(\n",
    "        df: DataFrame,\n",
    "        hash_cols: List[str],\n",
    "        hash_output_colname: str\n",
    "    ) -> DataFrame:\n",
    "        # Create UUID using sha256 hash\n",
    "        # Ensure all hash_cols are cast to string for consistent hashing\n",
    "        hash_expressions = [col(c).cast(\"string\") for c in hash_cols]\n",
    "        df = df.withColumn(hash_output_colname, sha2(concat_ws(\"||\", *hash_expressions), 256))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _common_transformations(\n",
    "        src_df: DataFrame,\n",
    "        nk_mapping: Optional[Dict[str, str]] = None,\n",
    "        type_mapping: Optional[Dict[str, Union[str, DataType]]] = None,\n",
    "        literals: Optional[Dict[str, str]] = None,\n",
    "        drop_cols: Optional[List[str]] = None,\n",
    "        hash_cols: Optional[List[str]] = None,\n",
    "        hash_output_colname: Optional[str] = None,\n",
    "        fk_df: Optional[DataFrame] = None,\n",
    "        fk_col: Optional[Dict[str, str]] = None,\n",
    "        select_colname: Optional[List[str]] = None\n",
    "    ) -> Optional[DataFrame]:\n",
    "        \"\"\"Applies renames, type changes, literals, drops, hash creation, foreign key joins, and column selection.\"\"\"\n",
    "        # Rename columns\n",
    "        if nk_mapping:\n",
    "            for original_col, new_cols in nk_mapping.items():\n",
    "                if original_col in src_df.columns:\n",
    "                    # Handle both single string and list of strings\n",
    "                    if isinstance(new_cols, str):\n",
    "                        new_cols = [new_cols]\n",
    "                    \n",
    "                    for new_col in new_cols:\n",
    "                        src_df = src_df.withColumn(new_col, col(original_col))\n",
    "    \n",
    "        # Modify column data type\n",
    "        if type_mapping:\n",
    "            for colname, datatype in type_mapping.items():\n",
    "                if colname in src_df.columns:\n",
    "                    src_df = src_df.withColumn(colname, col(colname).cast(datatype))\n",
    "                        \n",
    "        # Create literal column\n",
    "        if literals:\n",
    "            for colname, literal in literals.items():\n",
    "                src_df = src_df.withColumn(colname, lit(literal))\n",
    "    \n",
    "        # Drop columns\n",
    "        if drop_cols:\n",
    "            drop_cols = [c for c in drop_cols if c in src_df.columns]\n",
    "            src_df = src_df.drop(*drop_cols)\n",
    "    \n",
    "        # Create UUID using sha256 hash\n",
    "        if hash_cols and hash_output_colname:\n",
    "            hash_cols = [c for c in hash_cols if c in src_df.columns]\n",
    "            if hash_cols:\n",
    "                hash_expressions = [coalesce(col(c).cast(\"string\"), lit(\"null\")) for c in hash_cols]\n",
    "                src_df = src_df.withColumn(hash_output_colname, sha2(concat_ws(\"||\", *hash_expressions), 256))\n",
    "    \n",
    "        # Create foreign key\n",
    "        if fk_df and fk_col:\n",
    "            for lookup_key, lookup_val in fk_col.items():\n",
    "                if lookup_key in src_df.columns and lookup_val in fk_df.columns:\n",
    "                    fk_df_subset = fk_df.select(lookup_key, lookup_val)\n",
    "                    src_df = src_df.join(fk_df_subset, lookup_key, \"left\")\n",
    "    \n",
    "        # Select columns\n",
    "        if select_colname:\n",
    "            select_colname = [c for c in select_colname if c in src_df.columns]\n",
    "            src_df = src_df.select(*select_colname)\n",
    "    \n",
    "        return src_df\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_first_key(d, val):\n",
    "        return next((key for key, value in d.items() if value == val), None)\n",
    "        \n",
    "    @classmethod\n",
    "    def staging(cls, spark: SparkSession, df_dict: Dict[str, DataFrame]) -> Optional[Dict]:\n",
    "        # Create completely independent DataFrames copy:\n",
    "        dim = {\n",
    "            key: df.select(\"*\")\n",
    "            for key, df in df_dict.items()\n",
    "        }\n",
    "        \n",
    "        # Define current_timestamp for logging\n",
    "        current_timestamp = datetime.now()\n",
    "       \n",
    "        # ----------------------------------------------------------------- #\n",
    "        # Transform company into dim_company\n",
    "    \n",
    "        # office_id -> office_nk int\n",
    "        # object_id -> object_nk\n",
    "        # ensure latitude and longitude to be DecimalType(9,6)\n",
    "        table_name = \"company\"\n",
    "        hash_cols = [\"office_nk\", \"object_nk\", \"description\", \"region\",\n",
    "                    \"address1\", \"address2\", \"city\", \"zip_code\",\n",
    "                    \"state_code\", \"country_code\", \"latitude\", \"longitude\"]\n",
    "        try:\n",
    "            dim[\"company\"] = cls._common_transformations(\n",
    "                src_df=dim[\"company\"],\n",
    "                nk_mapping={\n",
    "                    \"office_id\": \"office_nk\", \n",
    "                    \"object_id\": \"object_nk\"\n",
    "                },\n",
    "                type_mapping={\n",
    "                    \"latitude\": DecimalType(9,6),\n",
    "                    \"longitude\": DecimalType(9,6)\n",
    "                },\n",
    "                hash_cols=hash_cols,\n",
    "                hash_output_colname=\"company_id\",\n",
    "                select_colname=[\"company_id\"] + hash_cols\n",
    "            )\n",
    "            \n",
    "            # Log success transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"success\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date'])\n",
    "            load_log(spark, log_msg)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Capture full traceback information\n",
    "            tb_str = traceback.format_exc()\n",
    "            error_msg = f\"\"\"\n",
    "            fail to perform transform operation for table '{table_name}'.\n",
    "            \n",
    "            Error Details:\n",
    "            - Error Type: {type(e).__name__}\n",
    "            - Error Message: {str(e)}\n",
    "            - Table: {table_name}\n",
    "            \n",
    "            Full Traceback:\n",
    "            {tb_str}\n",
    "            \"\"\"\n",
    "            # Log fail transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"fail\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp,\n",
    "                               error_msg)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date',\n",
    "                       'error_msg'])\n",
    "            load_log(spark, log_msg)\n",
    "            return None\n",
    "            \n",
    "        # ----------------------------------------------------------------- #\n",
    "        # Transform acquisition into dim_acquisition\n",
    "\n",
    "        # acquisition_id -> acquisition_nk\n",
    "        # acquiring_object_id -> acquiring_object_nk\n",
    "        # acquired_object_id -> acquired_object_nk\n",
    "        \n",
    "        table_name = \"acquisition\"\n",
    "        hash_cols =[\"acquisition_nk\", \"acquiring_object_nk\", \"acquired_object_nk\", \n",
    "                    \"term_code\", \"price_amount\", \"price_currency_code\"]\n",
    "        try:\n",
    "            dim[\"acquisition\"] = cls._common_transformations(\n",
    "                src_df=dim[\"acquisition\"],\n",
    "                nk_mapping={\n",
    "                    \"acquisition_id\": \"acquisition_nk\", \n",
    "                    \"acquiring_object_id\": \"acquiring_object_nk\",\n",
    "                    \"acquired_object_id\": \"acquired_object_nk\"\n",
    "                },\n",
    "                hash_cols=hash_cols,\n",
    "                hash_output_colname=\"acquisition_id\",\n",
    "                select_colname=[\"acquisition_id\"] + hash_cols\n",
    "            )\n",
    "            # Log success transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"success\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date'])\n",
    "            load_log(spark, log_msg)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Capture full traceback information\n",
    "            tb_str = traceback.format_exc()\n",
    "            error_msg = f\"\"\"\n",
    "            fail to perform transform operation for table '{table_name}'.\n",
    "            \n",
    "            Error Details:\n",
    "            - Error Type: {type(e).__name__}\n",
    "            - Error Message: {str(e)}\n",
    "            - Table: {table_name}\n",
    "            \n",
    "            Full Traceback:\n",
    "            {tb_str}\n",
    "            \"\"\"\n",
    "            # Log fail transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"fail\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp,\n",
    "                               error_msg)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date',\n",
    "                       'error_msg'])\n",
    "            load_log(spark, log_msg)\n",
    "            return None\n",
    "            \n",
    "        # ----------------------------------------------------------------- #\n",
    "        # Transform ipos into dim_ipos\n",
    "\n",
    "        # ipo_id -> ipo_nk\n",
    "        # object_id -> object_nk\n",
    "        table_name = \"ipos\"\n",
    "        hash_cols = [\"ipo_nk\", \"valuation_amount\", \n",
    "                    \"valuation_currency_code\", \"raised_amount\",\n",
    "                     \"raised_currency_code\", \"stock_symbol\"]\n",
    "        try:\n",
    "            dim[\"ipos\"] = cls._common_transformations(\n",
    "                src_df=dim[\"ipos\"],\n",
    "                nk_mapping={\n",
    "                    \"ipo_id\": \"ipo_nk\", \n",
    "                    \"object_id\": \"object_nk\"\n",
    "                },\n",
    "                hash_cols=hash_cols,\n",
    "                hash_output_colname=\"ipos_id\",\n",
    "                select_colname=[\"ipos_id\"] + hash_cols\n",
    "            )\n",
    "            # Log success transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"success\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date'])\n",
    "            load_log(spark, log_msg)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Capture full traceback information\n",
    "            tb_str = traceback.format_exc()\n",
    "            error_msg = f\"\"\"\n",
    "            fail to perform transform operation for table '{table_name}'.\n",
    "            \n",
    "            Error Details:\n",
    "            - Error Type: {type(e).__name__}\n",
    "            - Error Message: {str(e)}\n",
    "            - Table: {table_name}\n",
    "            \n",
    "            Full Traceback:\n",
    "            {tb_str}\n",
    "            \"\"\"\n",
    "            # Log fail transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"fail\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp,\n",
    "                               error_msg)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date',\n",
    "                       'error_msg'])\n",
    "            load_log(spark, log_msg)        \n",
    "            return None\n",
    "            \n",
    "        # ----------------------------------------------------------------- #\n",
    "        # Transform funds into dim_funds\n",
    "\n",
    "        # fund_id -> fund_nk\n",
    "        # object_id -> object_nk\n",
    "        table_name = \"funds\"\n",
    "        hash_cols = [\"fund_nk\", \"name\", \n",
    "                    \"raised_amount\", \"raised_currency_code\"]\n",
    "        try:\n",
    "            dim[\"funds\"] = cls._common_transformations(\n",
    "                src_df=dim[\"funds\"],\n",
    "                nk_mapping={\n",
    "                    \"fund_id\": \"fund_nk\", \n",
    "                    \"object_id\": \"object_nk\"\n",
    "                },\n",
    "                hash_cols=hash_cols,\n",
    "                hash_output_colname=\"funds_id\",\n",
    "                select_colname=[\"funds_id\"] + hash_cols\n",
    "            )\n",
    "            # Log success transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"success\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date'])\n",
    "            load_log(spark, log_msg)\n",
    "        except Exception as e:\n",
    "            # Capture full traceback information\n",
    "            tb_str = traceback.format_exc()\n",
    "            error_msg = f\"\"\"\n",
    "            fail to perform transform operation for table '{table_name}'.\n",
    "            \n",
    "            Error Details:\n",
    "            - Error Type: {type(e).__name__}\n",
    "            - Error Message: {str(e)}\n",
    "            - Table: {table_name}\n",
    "            \n",
    "            Full Traceback:\n",
    "            {tb_str}\n",
    "            \"\"\"\n",
    "            # Log fail transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"fail\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp,\n",
    "                               error_msg)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date',\n",
    "                       'error_msg'])\n",
    "            load_log(spark, log_msg)\n",
    "            return None\n",
    "            \n",
    "        # ----------------------------------------------------------------- #\n",
    "        # Transform funding_rounds into dim_funding_rounds\n",
    "\n",
    "        # funding_round_id -> funding_round_nk\n",
    "        # object_id -> object_nk\n",
    "        table_name = \"funding_rounds\"\n",
    "        hash_cols=[\"funding_round_nk\", \"funding_round_type\", \n",
    "                    \"funding_round_code\", \"raised_amount_usd\", \"raised_amount\",\n",
    "                    \"raised_currency_code\", \"pre_money_valuation_usd\", \"pre_money_valuation\",\n",
    "                    \"pre_money_currency_code\", \"post_money_valuation_usd\", \"post_money_valuation\", \n",
    "                    \"post_money_currency_code\", \"participants\", \"is_first_round\", \"is_last_round\",\n",
    "                    \"created_by\"]\n",
    "        try:\n",
    "            dim[\"funding_rounds\"] = cls._common_transformations(\n",
    "                src_df=dim[\"funding_rounds\"],\n",
    "                nk_mapping={\n",
    "                    \"funding_round_id\": \"funding_round_nk\", \n",
    "                    \"object_id\": \"funded_object_nk\"\n",
    "                },\n",
    "                hash_cols=hash_cols,\n",
    "                hash_output_colname=\"funding_rounds_id\",\n",
    "                select_colname=[\"funding_rounds_id\"] + hash_cols\n",
    "            )\n",
    "            # Log success transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"success\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date'])\n",
    "            load_log(spark, log_msg)\n",
    "        except Exception as e:\n",
    "            # Capture full traceback information\n",
    "            tb_str = traceback.format_exc()\n",
    "            error_msg = f\"\"\"\n",
    "            fail to perform transform operation for table '{table_name}'.\n",
    "            \n",
    "            Error Details:\n",
    "            - Error Type: {type(e).__name__}\n",
    "            - Error Message: {str(e)}\n",
    "            - Table: {table_name}\n",
    "            \n",
    "            Full Traceback:\n",
    "            {tb_str}\n",
    "            \"\"\"\n",
    "            # Log fail transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"fail\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp,\n",
    "                               error_msg)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date',\n",
    "                       'error_msg'])\n",
    "            load_log(spark, log_msg)\n",
    "            return None\n",
    "            \n",
    "        # ----------------------------------------------------------------- #\n",
    "        # Transform investments into dim_investments\n",
    "\n",
    "        # funding_round_id -> funding_round_nk\n",
    "        # object_id -> object_nk\n",
    "        # Create foreign key using funding_rounds_id from funding_rounds\n",
    "        table_name = \"investments\"\n",
    "        hash_cols=[\"funding_rounds_id\", \"investment_nk\", \"funding_round_nk\",\n",
    "                    \"funded_object_nk\", \"investor_object_nk\"]\n",
    "        try: \n",
    "            dim[\"investments\"] = cls._common_transformations(\n",
    "                src_df=dim[\"investments\"],\n",
    "                nk_mapping={\n",
    "                    \"investment_id\": \"investment_nk\", \n",
    "                    \"funding_round_id\": \"funding_round_nk\",\n",
    "                    \"funded_object_id\": \"funded_object_nk\",\n",
    "                    \"investor_object_id\": \"investor_object_nk\"\n",
    "                },\n",
    "                fk_df=dim[\"funding_rounds\"],\n",
    "                fk_col={\"funding_round_nk\":\"funding_rounds_id\"},\n",
    "                hash_cols=hash_cols,\n",
    "                hash_output_colname=\"investments_id\",\n",
    "                select_colname=[\"investments_id\"] + hash_cols\n",
    "            )\n",
    "            # Log success transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"success\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date'])\n",
    "            load_log(spark, log_msg)\n",
    "        except Exception as e:\n",
    "            # Capture full traceback information\n",
    "            tb_str = traceback.format_exc()\n",
    "            error_msg = f\"\"\"\n",
    "            fail to perform transform operation for table '{table_name}'.\n",
    "            \n",
    "            Error Details:\n",
    "            - Error Type: {type(e).__name__}\n",
    "            - Error Message: {str(e)}\n",
    "            - Table: {table_name}\n",
    "            \n",
    "            Full Traceback:\n",
    "            {tb_str}\n",
    "            \"\"\"\n",
    "            # Log fail transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"fail\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp,\n",
    "                               error_msg)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date',\n",
    "                       'error_msg'])\n",
    "            load_log(spark, log_msg)\n",
    "            return None\n",
    "            \n",
    "        # ----------------------------------------------------------------- #\n",
    "        # Transform people into dim_people\n",
    "\n",
    "        # people_id -> people_nk\n",
    "        # object_id -> object_nk\n",
    "        table_name = \"people\"\n",
    "        hash_cols=[\"people_nk\", \"first_name\", \n",
    "                   \"last_name\", \"birthplace\", \"affiliation_name\"]\n",
    "        try:\n",
    "            dim[\"people\"] = cls._common_transformations(\n",
    "                src_df=dim[\"people\"],\n",
    "                nk_mapping={\n",
    "                    \"people_id\": \"not_used\",\n",
    "                    \"object_id\": \"people_nk\", \n",
    "                },\n",
    "                hash_cols=hash_cols,\n",
    "                hash_output_colname=\"people_id\",\n",
    "                select_colname=[\"people_id\"] + hash_cols\n",
    "            )\n",
    "            # Log success transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"success\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date'])\n",
    "            load_log(spark, log_msg)\n",
    "        except Exception as e:\n",
    "            # Capture full traceback information\n",
    "            tb_str = traceback.format_exc()\n",
    "            error_msg = f\"\"\"\n",
    "            fail to perform transform operation for table '{table_name}'.\n",
    "            \n",
    "            Error Details:\n",
    "            - Error Type: {type(e).__name__}\n",
    "            - Error Message: {str(e)}\n",
    "            - Table: {table_name}\n",
    "            \n",
    "            Full Traceback:\n",
    "            {tb_str}\n",
    "            \"\"\"\n",
    "            # Log fail transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"fail\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp,\n",
    "                               error_msg)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date',\n",
    "                       'error_msg'])\n",
    "            load_log(spark, log_msg)\n",
    "            return None\n",
    "            \n",
    "        # ----------------------------------------------------------------- #\n",
    "        # Transform relationships into dim_relationships\n",
    "\n",
    "        # relationship_id -> relationship_nk\n",
    "        # person_object_id -> person_object_nk\n",
    "        # relationship_object_id -> relationship_object_nk\n",
    "        # ensure data type is correct for\n",
    "        ## start_at (timestamp), end_at (timestamp), is_past (bool) and sequence (integer)\n",
    "        # Create foreign key: people_id -> dim_people, company_id -> company\n",
    "        table_name = \"relationships\"\n",
    "        hash_cols=[\"people_id\", \"company_id\",\n",
    "                    \"relationship_nk\", \"person_object_nk\", \n",
    "                    \"relationship_object_nk\", \n",
    "                    \"start_at\", \"end_at\", \"is_past\",\n",
    "                    \"sequence\", \"title\"]\n",
    "        try:\n",
    "            dim[\"relationships\"] = cls._common_transformations(\n",
    "                src_df=dim[\"relationships\"],\n",
    "                nk_mapping={\n",
    "                    \"relationship_id\": \"relationship_nk\", \n",
    "                    \"person_object_id\": \"people_nk\",\n",
    "                    \"relationship_object_id\": \"object_nk\"\n",
    "                },\n",
    "                type_mapping={\n",
    "                    \"start_at\":\"timestamp\",\n",
    "                    \"end_at\":\"timestamp\",\n",
    "                    \"is_past\":\"boolean\",\n",
    "                    \"sequence\":\"int\"\n",
    "                },\n",
    "                fk_df=dim[\"people\"],\n",
    "                fk_col={\"people_nk\": \"object_nk\"}\n",
    "            )\n",
    "            dim[\"relationships\"] = cls._common_transformations(\n",
    "                src_df=dim[\"relationships\"],\n",
    "                fk_df=dim[\"company\"],\n",
    "                fk_col={\"object_nk\":\"company_id\"}\n",
    "            )\n",
    "            dim[\"relationships\"] = cls._common_transformations(\n",
    "                src_df=dim[\"relationships\"],\n",
    "                nk_mapping={\n",
    "                    \"people_nk\": \"person_object_nk\", \n",
    "                    \"object_nk\": \"relationship_object_nk\"\n",
    "                },                \n",
    "                hash_cols=hash_cols,\n",
    "                hash_output_colname=\"relationships_id\",\n",
    "                select_colname=[\"relationships_id\"] + hash_cols\n",
    "            )\n",
    "            # Log success transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"success\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date'])\n",
    "            load_log(spark, log_msg)\n",
    "        except Exception as e:\n",
    "            # Capture full traceback information\n",
    "            tb_str = traceback.format_exc()\n",
    "            error_msg = f\"\"\"\n",
    "            fail to perform transform operation for table '{table_name}'.\n",
    "            \n",
    "            Error Details:\n",
    "            - Error Type: {type(e).__name__}\n",
    "            - Error Message: {str(e)}\n",
    "            - Table: {table_name}\n",
    "            \n",
    "            Full Traceback:\n",
    "            {tb_str}\n",
    "            \"\"\"\n",
    "            # Log fail transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"fail\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp,\n",
    "                               error_msg)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date',\n",
    "                       'error_msg'])\n",
    "            load_log(spark, log_msg)\n",
    "            return None\n",
    "            \n",
    "        # ----------------------------------------------------------------- #\n",
    "        # Transform milestones into dim_milestones\n",
    "\n",
    "        # milestone_id -> milestone_nk\n",
    "        # object_id -> object_nk\n",
    "        table_name = \"milestones\"\n",
    "        hash_cols=[\"milestone_nk\", \"description\", \"milestone_code\"]\n",
    "        \n",
    "        try:\n",
    "            dim[\"milestones\"] = cls._common_transformations(\n",
    "                src_df=dim[\"milestones\"],\n",
    "                nk_mapping={\n",
    "                    \"milestone_id\": \"milestone_nk\", \n",
    "                },\n",
    "                hash_cols=hash_cols,\n",
    "                hash_output_colname=\"milestones_id\",\n",
    "                select_colname=[\"milestones_id\"] + hash_cols\n",
    "            )\n",
    "            # Log success transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"success\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date'])\n",
    "            load_log(spark, log_msg)\n",
    "        except Exception as e:\n",
    "            # Capture full traceback information\n",
    "            tb_str = traceback.format_exc()\n",
    "            error_msg = f\"\"\"\n",
    "            fail to perform transform operation for table '{table_name}'.\n",
    "            \n",
    "            Error Details:\n",
    "            - Error Type: {type(e).__name__}\n",
    "            - Error Message: {str(e)}\n",
    "            - Table: {table_name}\n",
    "            \n",
    "            Full Traceback:\n",
    "            {tb_str}\n",
    "            \"\"\"\n",
    "            # Log fail transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"fail\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp,\n",
    "                               error_msg)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date',\n",
    "                       'error_msg'])\n",
    "            load_log(spark, log_msg)\n",
    "            return None\n",
    "            \n",
    "        # ----------------------------------------------------------------- #\n",
    "        # Transform event_type into dim_event_type\n",
    "        ## unchanged\n",
    "        \n",
    "        # ----------------------------------------------------------------- #\n",
    "        # Transform date into dim_date\n",
    "        table_name = \"date\"\n",
    "        try:\n",
    "            dim[\"date\"] = cls._common_transformations(\n",
    "                src_df=dim[\"date\"],\n",
    "                type_mapping={\n",
    "                    \"date_id\": \"int\"\n",
    "                }\n",
    "            )\n",
    "            # Log success transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"success\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date'])\n",
    "            load_log(spark, log_msg)\n",
    "        except Exception as e:\n",
    "            # Capture full traceback information\n",
    "            tb_str = traceback.format_exc()\n",
    "            error_msg = f\"\"\"\n",
    "            fail to perform transform operation for table '{table_name}'.\n",
    "            \n",
    "            Error Details:\n",
    "            - Error Type: {type(e).__name__}\n",
    "            - Error Message: {str(e)}\n",
    "            - Table: {table_name}\n",
    "            \n",
    "            Full Traceback:\n",
    "            {tb_str}\n",
    "            \"\"\"\n",
    "            # Log fail transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"fail\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp,\n",
    "                               error_msg)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date',\n",
    "                       'error_msg'])\n",
    "            load_log(spark, log_msg)\n",
    "            return None\n",
    "        \n",
    "        # ----------------------------------------------------------------- #\n",
    "        # Define common variable that will be used\n",
    "        event_type_map = {row.event_type_id: row.event_type \n",
    "                          for row in dim[\"event_type\"].select(\"event_type_id\", \"event_type\").collect()}\n",
    "        common_colname = [\"company_id\", \"event_type_id\",\n",
    "                          \"event_datetime\", \"event_date\",\n",
    "                          \"source_url\", \"source_description\"]\n",
    "        \n",
    "        # Transform data into fct_startup_event\n",
    "        table_name = \"startup_event\"\n",
    "        try:\n",
    "            # ----------------------------------------------------------------- #\n",
    "            # Event: Acquiring\n",
    "            fct_acquiring = cls._common_transformations(\n",
    "                src_df=df_dict[\"acquisition\"].select(\"*\"),\n",
    "                nk_mapping={\n",
    "                    \"acquisition_id\":\"acquisition_nk\",\n",
    "                    \"acquiring_object_id\":\"object_nk\",\n",
    "                    \"acquired_at\":\"event_datetime\",\n",
    "                    \"acquired_at\":\"event_date\"\n",
    "                },\n",
    "                type_mapping={\n",
    "                    \"event_date\":\"timestamp\",\n",
    "                    \"event_date\":\"date\"\n",
    "                },\n",
    "                literals={\n",
    "                    \"event_type_id\":cls._get_first_key(\n",
    "                        event_type_map, \"Acquiring\"\n",
    "                    )\n",
    "                },\n",
    "                drop_cols=[\"acquisition_id\"],\n",
    "                fk_df=dim[\"acquisition\"],\n",
    "                fk_col={\"acquisition_nk\": \"acquisition_id\"}\n",
    "            )\n",
    "            fct_acquiring_final = cls._common_transformations(\n",
    "                src_df=fct_acquiring,\n",
    "                fk_df=dim[\"company\"],\n",
    "                fk_col={\"object_nk\": \"company_id\"},\n",
    "                select_colname=[\"acquisition_id\", \"acquisition_nk\"] + common_colname\n",
    "            )\n",
    "            \n",
    "            # ----------------------------------------------------------------- #\n",
    "            # Event: Acquired \n",
    "            fct_acquired = cls._common_transformations(\n",
    "                src_df=df_dict[\"acquisition\"].select(\"*\"),\n",
    "                nk_mapping={\n",
    "                    \"acquisition_id\":\"acquisition_nk\",\n",
    "                    \"acquired_object_id\":\"object_nk\",\n",
    "                    \"acquired_at\":\"event_datetime\",\n",
    "                    \"acquired_at\":\"event_date\"\n",
    "                },\n",
    "                type_mapping={\n",
    "                    \"event_datetime\":\"timestamp\",\n",
    "                    \"event_date\":\"date\"\n",
    "                },\n",
    "                literals={\n",
    "                    \"event_type_id\":cls._get_first_key(\n",
    "                        event_type_map, \"Acquired\"\n",
    "                    )\n",
    "                },\n",
    "                drop_cols=[\"acquisition_id\"],\n",
    "                fk_df=dim[\"acquisition\"],\n",
    "                fk_col={\"acquisition_nk\": \"acquisition_id\"}\n",
    "            )\n",
    "            fct_acquired_final = cls._common_transformations(\n",
    "                src_df=fct_acquired,\n",
    "                fk_df=dim[\"company\"],\n",
    "                fk_col={\"object_nk\": \"company_id\"},\n",
    "                select_colname=[\"acquisition_id\", \"acquisition_nk\"] + common_colname\n",
    "            )\n",
    "            \n",
    "            # ----------------------------------------------------------------- #\n",
    "            # Event: IPO \n",
    "            fct_ipos = cls._common_transformations(\n",
    "                src_df=df_dict[\"ipos\"].select(\"*\"),\n",
    "                nk_mapping={\n",
    "                    \"ipo_id\":\"ipo_nk\",\n",
    "                    \"object_id\":\"object_nk\",\n",
    "                    \"public_at\":\"event_datetime\",\n",
    "                    \"public_at\":\"event_date\"\n",
    "                },\n",
    "                type_mapping={\n",
    "                    \"event_date\":\"timestamp\",\n",
    "                    \"event_date\":\"date\"\n",
    "                },\n",
    "                literals={\n",
    "                    \"event_type_id\":cls._get_first_key(\n",
    "                        event_type_map, \"IPO\"\n",
    "                    )\n",
    "                },\n",
    "                fk_df=dim[\"ipos\"],\n",
    "                fk_col={\"ipo_nk\": \"ipos_id\"}\n",
    "            )\n",
    "            fct_ipos_final = cls._common_transformations(\n",
    "                src_df=fct_ipos,\n",
    "                fk_df=dim[\"company\"],\n",
    "                fk_col={\"object_nk\": \"company_id\"},\n",
    "                select_colname=[\"ipos_id\", \"ipo_nk\"] + common_colname\n",
    "            )  \n",
    "\n",
    "            # ----------------------------------------------------------------- #\n",
    "            # Event: Funds \n",
    "            fct_funds = cls._common_transformations(\n",
    "                src_df=df_dict[\"funds\"].select(\"*\"),\n",
    "                nk_mapping={\n",
    "                    \"fund_id\":\"fund_nk\",\n",
    "                    \"object_id\":\"object_nk\",\n",
    "                    \"funded_at\":\"event_datetime\",\n",
    "                    \"funded_at\":\"event_date\"\n",
    "                },\n",
    "                type_mapping={\n",
    "                    \"event_datetime\":\"timestamp\",\n",
    "                    \"event_date\":\"date\"\n",
    "                },\n",
    "                literals={\n",
    "                    \"event_type_id\":cls._get_first_key(\n",
    "                        event_type_map, \"Funds\"\n",
    "                    )\n",
    "                },\n",
    "                fk_df=dim[\"funds\"],\n",
    "                fk_col={\"fund_nk\": \"funds_id\"}\n",
    "            )\n",
    "            fct_funds_final = cls._common_transformations(\n",
    "                src_df=fct_funds,\n",
    "                fk_df=dim[\"company\"],\n",
    "                fk_col={\"object_nk\": \"company_id\"},\n",
    "                select_colname=[\"funds_id\", \"fund_nk\"] + common_colname\n",
    "            )\n",
    "            \n",
    "            # ----------------------------------------------------------------- #\n",
    "            # Event: Received Funding, Made Investment and Funding Rounds\n",
    "            fct_received_funding = cls._common_transformations(\n",
    "                src_df=df_dict[\"investments\"].select(\"*\"),\n",
    "                nk_mapping={\n",
    "                    \"investment_id\":\"investment_nk\",\n",
    "                    \"funding_round_id\":\"funding_round_nk\",\n",
    "                    \"funded_object_id\":\"object_nk\"\n",
    "                },\n",
    "                literals={\n",
    "                    \"event_type_id\":cls._get_first_key(\n",
    "                        event_type_map, \"Received Funding\"\n",
    "                    )\n",
    "                },\n",
    "                fk_df=dim[\"company\"],\n",
    "                fk_col={\"object_nk\": \"company_id\"},\n",
    "                select_colname=[\"investment_nk\", \"funding_round_nk\", \"company_id\", \"event_type_id\"]\n",
    "            )\n",
    "            fct_made_investment = cls._common_transformations(\n",
    "                src_df=df_dict[\"investments\"].select(\"*\"),\n",
    "                nk_mapping={\n",
    "                    \"investment_id\":\"investment_nk\",\n",
    "                    \"funding_round_id\":\"funding_round_nk\",\n",
    "                    \"investor_object_id\":\"object_nk\"\n",
    "                },\n",
    "                literals={\n",
    "                    \"event_type_id\":cls._get_first_key(\n",
    "                        event_type_map, \"Made Investment\"\n",
    "                    )\n",
    "                },\n",
    "                fk_df=dim[\"company\"],\n",
    "                fk_col={\"object_nk\": \"company_id\"},\n",
    "                select_colname=[\"investment_nk\",\"funding_round_nk\", \"company_id\", \"event_type_id\"]\n",
    "            )\n",
    "            fct_funding_rounds = cls._common_transformations(\n",
    "                src_df=df_dict[\"funding_rounds\"].select(\"*\"),\n",
    "                nk_mapping={\n",
    "                    \"funding_round_id\":\"funding_round_nk\",\n",
    "                    \"funded_at\":[\"event_datetime\", \"event_date\"]\n",
    "                },\n",
    "                type_mapping={\n",
    "                    \"event_datetime\":\"timestamp\",\n",
    "                    \"event_date\":\"date\"\n",
    "                },\n",
    "                fk_df=dim[\"funding_rounds\"],\n",
    "                fk_col={\"funding_round_nk\": \"funding_rounds_id\"},\n",
    "                select_colname=[\"funding_rounds_id\", \"funding_round_nk\", \"event_datetime\",\n",
    "                                \"event_date\", \"source_url\", \"source_description\"]\n",
    "            )\n",
    "            fct_received_funding_final = fct_received_funding.join(\n",
    "                fct_funding_rounds,\"funding_round_nk\",\"left\"\n",
    "            )\n",
    "            fct_made_investment_final = fct_made_investment.join(\n",
    "                fct_funding_rounds,\"funding_round_nk\",\"left\"\n",
    "            )\n",
    "            fct_funding_rounds_final = fct_received_funding_final\\\n",
    "                                           .unionByName(fct_made_investment_final, allowMissingColumns=True)\\\n",
    "                                           .select([\"funding_rounds_id\", \"funding_round_nk\", \"investment_nk\"] + common_colname)\n",
    "            \n",
    "            # ----------------------------------------------------------------- #\n",
    "            # Event: Milestone \n",
    "            fct_milestones = cls._common_transformations(\n",
    "                src_df=df_dict[\"milestones\"].select(\"*\"),\n",
    "                nk_mapping={\n",
    "                    \"milestone_id\":\"milestone_nk\",\n",
    "                    \"object_id\":\"object_nk\",\n",
    "                    \"milestone_at\":\"event_datetime\",\n",
    "                    \"milestone_at\":\"event_date\"\n",
    "                },\n",
    "                type_mapping={\n",
    "                    \"event_datetime\":\"timestamp\",\n",
    "                    \"event_date\":\"date\"\n",
    "                },\n",
    "                literals={\n",
    "                    \"event_type_id\":cls._get_first_key(\n",
    "                        event_type_map, \"Milestone\"\n",
    "                    )\n",
    "                },\n",
    "                fk_df=dim[\"milestones\"],\n",
    "                fk_col={\"milestone_nk\": \"milestones_id\"}\n",
    "            )\n",
    "            fct_milestones_final = cls._common_transformations(\n",
    "                src_df=fct_milestones,\n",
    "                fk_df=dim[\"company\"],\n",
    "                fk_col={\"object_nk\": \"company_id\"},\n",
    "                select_colname=[\"milestones_id\", \"milestone_nk\"] + common_colname\n",
    "            )   \n",
    "            \n",
    "            # ----------------------------------------------------------------- #\n",
    "            # Union All to become fct_startup_event\n",
    "            \n",
    "            # Union with unionByName\n",
    "            dim[\"startup_event\"] = fct_acquiring_final\n",
    "            for df in [fct_acquired_final, fct_ipos_final, fct_funds_final, fct_funding_rounds_final, fct_milestones_final]:\n",
    "                dim[\"startup_event\"] = dim[\"startup_event\"].unionByName(df, allowMissingColumns=True)\n",
    "            \n",
    "            # Add startup_event_id\n",
    "            dim[\"startup_event\"] = cls._common_transformations(\n",
    "                src_df=dim[\"startup_event\"],\n",
    "                hash_cols=[\"company_id\", \"acquisition_id\", \"ipos_id\",\n",
    "                           \"funds_id\", \"funding_rounds_id\", \"milestones_id\",\n",
    "                           \"event_type_id\", \"object_nk\", \"acquisition_nk\",\n",
    "                           \"ipo_nk\", \"fund_nk\", \"investment_nk\",\n",
    "                           \"funding_round_nk\", \"event_datetime\",\n",
    "                           \"event_date\", \"source_url\", \"source_description\"\n",
    "                          ],\n",
    "                hash_output_colname=\"startup_event_id\"\n",
    "            )\n",
    "\n",
    "            # Add stable_id to help upsert later by hashing all natural key\n",
    "            dim[\"startup_event\"] = cls._common_transformations(\n",
    "                src_df=dim[\"startup_event\"],\n",
    "                hash_cols=[\"object_nk\", \"acquisition_nk\", \"ipo_nk\", \n",
    "                           \"fund_nk\", \"funding_round_nk\", \"investment_nk\",\n",
    "                           \"milestone_nk\"],\n",
    "                hash_output_colname=\"stable_id\",\n",
    "                select_colname=[\n",
    "                    \"startup_event_id\", \"stable_id\", \"acquisition_id\", \"ipos_id\",  \n",
    "                    \"funds_id\", \"funding_rounds_id\", \"milestones_id\"\n",
    "                ] + common_colname\n",
    "            )\n",
    "            \n",
    "            # Add created_at and updated_at\n",
    "            for table_name, _ in dim.items():\n",
    "                dim[table_name] = dim[table_name]\\\n",
    "                                     .withColumn(\"created_at\", lit(datetime.now()))\\\n",
    "                                     .withColumn(\"updated_at\", lit(datetime.now()))\n",
    "                        \n",
    "            # Log success transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"success\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date'])\n",
    "            load_log(spark, log_msg)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Capture full traceback information\n",
    "            tb_str = traceback.format_exc()\n",
    "            error_msg = f\"\"\"\n",
    "            fail to perform transform operation for table '{table_name}'.\n",
    "            \n",
    "            Error Details:\n",
    "            - Error Type: {type(e).__name__}\n",
    "            - Error Message: {str(e)}\n",
    "            - Table: {table_name}\n",
    "            \n",
    "            Full Traceback:\n",
    "            {tb_str}\n",
    "            \"\"\"\n",
    "            # Log fail transformation\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"transform_staging\",\n",
    "                               \"fail\",\n",
    "                               \"staging extraction result\",\n",
    "                               table_name,\n",
    "                               current_timestamp,\n",
    "                               error_msg)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date',\n",
    "                       'error_msg'])\n",
    "            load_log(spark, log_msg)\n",
    "            return None\n",
    "        # ----------------------------------------------------------------- #\n",
    "        # Return back the dim dictionary\n",
    "        return dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a4f24-a4e2-4971-b2c8-655b703bb6b5",
   "metadata": {},
   "source": [
    "# Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "568de2ae-c170-4cb6-9f71-10de2cf87de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validate():    \n",
    "    @staticmethod\n",
    "    def transform_staging(\n",
    "        spark: SparkSession,\n",
    "        df_dict: Dict[str, DataFrame],\n",
    "    ) -> Optional[bool]:\n",
    "        try:\n",
    "            current_timestamp = datetime.now()\n",
    "            \n",
    "            table_name = \"startup_event\"\n",
    "            df = df_dict[table_name]\n",
    "            \n",
    "            col_to_check = \"company_id\"\n",
    "            \n",
    "            # Use first() to find any null - stops as soon as one is found\n",
    "            try:\n",
    "                first_null = df.filter(col(col_to_check).isNull()).first()\n",
    "                validation_passed = first_null is None\n",
    "            except:\n",
    "                null_count = df.filter(col(col_to_check).isNull()).limit(1).count()\n",
    "                validation_passed = null_count == 0\n",
    "                \n",
    "            if validation_passed:\n",
    "                # Log success validation\n",
    "                log_msg = spark.sparkContext\\\n",
    "                    .parallelize([(\"warehouse\",\n",
    "                                   \"validate_transformation\",\n",
    "                                   \"success\",\n",
    "                                   \"staging_transformation_result\",\n",
    "                                   table_name,\n",
    "                                   current_timestamp)])\\\n",
    "                    .toDF(['step',\n",
    "                           'process',\n",
    "                           'status',\n",
    "                           'source',\n",
    "                           'table_name',\n",
    "                           'etl_date'])\n",
    "                load_log(spark, log_msg)\n",
    "            else:\n",
    "                # Log fail validation\n",
    "                error_msg = f\"Found null values in company_id column of {table_name}\"\n",
    "                log_msg = spark.sparkContext\\\n",
    "                    .parallelize([(\"warehouse\",\n",
    "                                   \"validate_transformation\",\n",
    "                                   \"fail\",\n",
    "                                   \"staging_transformation_result\",\n",
    "                                   table_name,\n",
    "                                   current_timestamp,\n",
    "                                   error_msg)])\\\n",
    "                    .toDF(['step',\n",
    "                           'process',\n",
    "                           'status',\n",
    "                           'source',\n",
    "                           'table_name',\n",
    "                           'etl_date',\n",
    "                           'error_msg'])\n",
    "                load_log(spark, log_msg)\n",
    "                \n",
    "        except Exception as e:\n",
    "            tb_str = traceback.format_exc()\n",
    "            error_msg = f\"\"\"\n",
    "            fail to perform validation operation for table '{table_name}'.\n",
    "            \n",
    "            Error Details:\n",
    "            - Error Type: {type(e).__name__}  \n",
    "            - Error Message: {str(e)}\n",
    "            - Table: {table_name}\n",
    "            \n",
    "            Full Traceback:\n",
    "            {tb_str}\n",
    "            \"\"\"\n",
    "            log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\",\n",
    "                               \"validate_transformation\",\n",
    "                               \"fail\",\n",
    "                               \"staging_transformation_result\",\n",
    "                               table_name,\n",
    "                               current_timestamp,\n",
    "                               error_msg)])\\\n",
    "                .toDF(['step',\n",
    "                       'process',\n",
    "                       'status',\n",
    "                       'source',\n",
    "                       'table_name',\n",
    "                       'etl_date',\n",
    "                       'error_msg'])\n",
    "            load_log(spark, log_msg)\n",
    "            return False\n",
    "            \n",
    "        return validation_passed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74afe8d-75e8-45f7-91e1-d28324ea854f",
   "metadata": {},
   "source": [
    "# ETL Pipeline functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23abe774-6cfa-47f4-b624-7c8531ea688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ETLPipeline():\n",
    "#     @staticmethod\n",
    "#     def start(spark: SparkSession):\n",
    "\n",
    "# ----------------------------------------------------------------- #\n",
    "# EXTRACT FROM SOURCE\n",
    "# 1. Extract from source database\n",
    "db_table_to_extract = [\n",
    "    \"company\",\n",
    "    \"acquisition\",\n",
    "    \"ipos\",\n",
    "    \"funds\",\n",
    "    \"funding_rounds\",\n",
    "    \"investments\",\n",
    "]\n",
    "src_db_dict = Extract.from_database(\n",
    "    spark=spark,\n",
    "    table_to_extract=db_table_to_extract, \n",
    "    source_type=\"source\"\n",
    ")  \n",
    "# 2. Extract from source csv                 \n",
    "src_csv_dict = Extract.from_csv(\n",
    "    spark=spark\n",
    ")\n",
    "        \n",
    "# 3. Extract from source API\n",
    "src_api_dict = {}\n",
    "src_api_dict[\"milestones\"] = Extract.from_api(\n",
    "    spark=spark, \n",
    "    start_date=\"2005-01-01\", \n",
    "    end_date=\"2011-01-01\"\n",
    ")\n",
    "\n",
    "src_dict = {**src_db_dict, **src_csv_dict, **src_api_dict}\n",
    "\n",
    "# ----------------------------------------------------------------- #\n",
    "# LOAD TO STAGING\n",
    "# 5. Load to staging database\n",
    "Load.to_staging(\n",
    "    spark=spark, \n",
    "    df_dict=src_dict\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------- #\n",
    "# EXTRACT FROM STAGING\n",
    "# 6. Extract from staging database\n",
    "stg_db_tbl = [\n",
    "    \"company\",\n",
    "    \"acquisition\",\n",
    "    \"ipos\",\n",
    "    \"funds\",\n",
    "    \"funding_rounds\",\n",
    "    \"investments\",\n",
    "    \"people\",\n",
    "    \"relationships\",\n",
    "    \"date\",\n",
    "    \"milestones\",\n",
    "    \"event_type\"\n",
    "]\n",
    "src_dict = {}\n",
    "src_dict = Extract.from_database(\n",
    "    spark=spark,\n",
    "    table_to_extract=stg_db_tbl, \n",
    "    source_type=\"staging\"\n",
    ")\n",
    "# ----------------------------------------------------------------- #\n",
    "# TRANSFORM STAGING\n",
    "# 7. Transform staging database\n",
    "src_dict = Transform.staging(\n",
    "    spark=spark, \n",
    "    df_dict=src_dict\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# ----------------------------------------------------------------- #\n",
    "# VALIDATE TRANSFORMATION\n",
    "# 8. Validate data transformation from staging\n",
    "validation_passed = Validate.transform_staging(\n",
    "    spark=spark,\n",
    "    df_dict=src_dict\n",
    ")\n",
    "\n",
    "if not validation_passed:\n",
    "    print(\"Validation process fail. There is null in fact table\")\n",
    "else:\n",
    "    # ----------------------------------------------------------------- #\n",
    "    # LOAD TO WAREHOUSE\n",
    "    # 9. Load data to warehouse\n",
    "    Load.to_warehouse(\n",
    "        spark=spark,\n",
    "        df_dict=src_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "216113fe-4532-4418-a650-254c0e1ee6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipo_id</th>\n",
       "      <th>object_id</th>\n",
       "      <th>valuation_amount</th>\n",
       "      <th>valuation_currency_code</th>\n",
       "      <th>raised_amount</th>\n",
       "      <th>raised_currency_code</th>\n",
       "      <th>public_at</th>\n",
       "      <th>stock_symbol</th>\n",
       "      <th>source_url</th>\n",
       "      <th>source_description</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>c:1654</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>1980-12-19</td>\n",
       "      <td>NASDAQ:AAPL</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2008-02-09 05:17:45</td>\n",
       "      <td>2012-04-12 04:02:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>c:1242</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>1986-03-13</td>\n",
       "      <td>NASDAQ:MSFT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2008-02-09 05:25:18</td>\n",
       "      <td>2010-12-11 12:39:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>c:342</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>1969-06-09</td>\n",
       "      <td>NYSE:DIS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2008-02-09 05:40:32</td>\n",
       "      <td>2010-12-23 08:58:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>c:59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>2004-08-25</td>\n",
       "      <td>NASDAQ:GOOG</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2008-02-10 22:51:24</td>\n",
       "      <td>2011-08-01 20:47:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>c:317</td>\n",
       "      <td>100000000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>1997-05-01</td>\n",
       "      <td>NASDAQ:AMZN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2008-02-10 23:28:09</td>\n",
       "      <td>2011-08-01 21:11:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ipo_id object_id valuation_amount valuation_currency_code raised_amount  \\\n",
       "0      1    c:1654             0.00                     USD          0.00   \n",
       "1      2    c:1242             0.00                     USD          0.00   \n",
       "2      3     c:342             0.00                     USD          0.00   \n",
       "3      4      c:59             0.00                     USD          0.00   \n",
       "4      5     c:317  100000000000.00                     USD          0.00   \n",
       "\n",
       "  raised_currency_code  public_at stock_symbol source_url source_description  \\\n",
       "0                  USD 1980-12-19  NASDAQ:AAPL                                 \n",
       "1                      1986-03-13  NASDAQ:MSFT                                 \n",
       "2                      1969-06-09     NYSE:DIS                                 \n",
       "3                      2004-08-25  NASDAQ:GOOG                                 \n",
       "4                      1997-05-01  NASDAQ:AMZN                                 \n",
       "\n",
       "           created_at          updated_at  \n",
       "0 2008-02-09 05:17:45 2012-04-12 04:02:59  \n",
       "1 2008-02-09 05:25:18 2010-12-11 12:39:46  \n",
       "2 2008-02-09 05:40:32 2010-12-23 08:58:16  \n",
       "3 2008-02-10 22:51:24 2011-08-01 20:47:08  \n",
       "4 2008-02-10 23:28:09 2011-08-01 21:11:22  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dict[\"ipos\"].toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "95b96404-7f29-41f0-a6dd-44083c759121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|source_description|\n",
      "+------------------+\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tes_ipos = src_dict[\"ipos\"].select(\"*\")\n",
    "tes_ipos.filter(col(\"ipo_id\")==\"1377\").select(\"source_description\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2507cd46-b9f5-4d4e-a533-b8e5964b224d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>office_id</th>\n",
       "      <th>object_id</th>\n",
       "      <th>description</th>\n",
       "      <th>region</th>\n",
       "      <th>address1</th>\n",
       "      <th>address2</th>\n",
       "      <th>city</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>country_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>hash_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>c:29</td>\n",
       "      <td></td>\n",
       "      <td>SF Bay</td>\n",
       "      <td>701 First Avenue</td>\n",
       "      <td></td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>94089</td>\n",
       "      <td>CA</td>\n",
       "      <td>USA</td>\n",
       "      <td>37.418531</td>\n",
       "      <td>-122.025485</td>\n",
       "      <td>2025-08-06 02:59:38.519871</td>\n",
       "      <td>2025-08-06 02:59:38.531407</td>\n",
       "      <td>16e2c15c2fb4005a1215fd1e7cf393741fd319e9ebba60...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>c:32</td>\n",
       "      <td>Corporate Headquarters</td>\n",
       "      <td>Buckinghamshire</td>\n",
       "      <td>Wethered House, Pound Lane</td>\n",
       "      <td></td>\n",
       "      <td>Buckinghamshire</td>\n",
       "      <td>SL7 2AF</td>\n",
       "      <td></td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2025-08-06 02:59:38.519871</td>\n",
       "      <td>2025-08-06 02:59:38.531407</td>\n",
       "      <td>bcfbc4563d195715cd9b1d9fc8834007b5b197d08fe8cb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>c:35</td>\n",
       "      <td>Headquarters</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>888 East Walnut Street</td>\n",
       "      <td></td>\n",
       "      <td>Pasadena</td>\n",
       "      <td>91101</td>\n",
       "      <td>CA</td>\n",
       "      <td>USA</td>\n",
       "      <td>34.149471</td>\n",
       "      <td>-118.132747</td>\n",
       "      <td>2025-08-06 02:59:38.519871</td>\n",
       "      <td>2025-08-06 02:59:38.531407</td>\n",
       "      <td>9f4e4003d0314ee4de3d93a413844cf3b6f6c09d57117d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>c:54</td>\n",
       "      <td></td>\n",
       "      <td>SF Bay</td>\n",
       "      <td>400 Montgomery St, Suite 900</td>\n",
       "      <td></td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>94104</td>\n",
       "      <td>CA</td>\n",
       "      <td>USA</td>\n",
       "      <td>37.793148</td>\n",
       "      <td>-122.402567</td>\n",
       "      <td>2025-08-06 02:59:38.519871</td>\n",
       "      <td>2025-08-06 02:59:38.531407</td>\n",
       "      <td>85c1d42e27cfdb4858ee8147905f0a7dc48c1a3b69febb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65</td>\n",
       "      <td>c:66</td>\n",
       "      <td>Livestream HQ</td>\n",
       "      <td>New York</td>\n",
       "      <td>111 8th Avenue, #1509</td>\n",
       "      <td></td>\n",
       "      <td>New York</td>\n",
       "      <td>10011</td>\n",
       "      <td>NY</td>\n",
       "      <td>USA</td>\n",
       "      <td>40.726155</td>\n",
       "      <td>-73.995625</td>\n",
       "      <td>2025-08-06 02:59:38.519871</td>\n",
       "      <td>2025-08-06 02:59:38.531407</td>\n",
       "      <td>1aeb053acef70902bd19deeefcedd3078161f8e2ad6ae8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   office_id object_id             description           region  \\\n",
       "0         28      c:29                                   SF Bay   \n",
       "1         31      c:32  Corporate Headquarters  Buckinghamshire   \n",
       "2         34      c:35            Headquarters      Los Angeles   \n",
       "3         53      c:54                                   SF Bay   \n",
       "4         65      c:66           Livestream HQ         New York   \n",
       "\n",
       "                       address1 address2             city zip_code state_code  \\\n",
       "0              701 First Avenue                 Sunnyvale    94089         CA   \n",
       "1    Wethered House, Pound Lane           Buckinghamshire  SL7 2AF              \n",
       "2        888 East Walnut Street                  Pasadena    91101         CA   \n",
       "3  400 Montgomery St, Suite 900             San Francisco    94104         CA   \n",
       "4         111 8th Avenue, #1509                  New York    10011         NY   \n",
       "\n",
       "  country_code   latitude    longitude                 created_at  \\\n",
       "0          USA  37.418531  -122.025485 2025-08-06 02:59:38.519871   \n",
       "1          GBR   0.000000     0.000000 2025-08-06 02:59:38.519871   \n",
       "2          USA  34.149471  -118.132747 2025-08-06 02:59:38.519871   \n",
       "3          USA  37.793148  -122.402567 2025-08-06 02:59:38.519871   \n",
       "4          USA  40.726155   -73.995625 2025-08-06 02:59:38.519871   \n",
       "\n",
       "                  updated_at  \\\n",
       "0 2025-08-06 02:59:38.531407   \n",
       "1 2025-08-06 02:59:38.531407   \n",
       "2 2025-08-06 02:59:38.531407   \n",
       "3 2025-08-06 02:59:38.531407   \n",
       "4 2025-08-06 02:59:38.531407   \n",
       "\n",
       "                                             hash_id  \n",
       "0  16e2c15c2fb4005a1215fd1e7cf393741fd319e9ebba60...  \n",
       "1  bcfbc4563d195715cd9b1d9fc8834007b5b197d08fe8cb...  \n",
       "2  9f4e4003d0314ee4de3d93a413844cf3b6f6c09d57117d...  \n",
       "3  85c1d42e27cfdb4858ee8147905f0a7dc48c1a3b69febb...  \n",
       "4  1aeb053acef70902bd19deeefcedd3078161f8e2ad6ae8...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acquisition\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acquisition_id</th>\n",
       "      <th>acquiring_object_id</th>\n",
       "      <th>acquired_object_id</th>\n",
       "      <th>term_code</th>\n",
       "      <th>price_amount</th>\n",
       "      <th>price_currency_code</th>\n",
       "      <th>acquired_at</th>\n",
       "      <th>source_url</th>\n",
       "      <th>source_description</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>hash_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "      <td>c:2050</td>\n",
       "      <td>c:2108</td>\n",
       "      <td>cash</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>2007-08-01</td>\n",
       "      <td>http://www.dating-weblog.com/50226711/avid_lif...</td>\n",
       "      <td>Ashley Madison acquired by Avid Life Media</td>\n",
       "      <td>2025-08-06 02:59:38.539496</td>\n",
       "      <td>2025-08-06 02:59:38.547236</td>\n",
       "      <td>141879dcbb6b99982989840b8c3cfc57eea59ee7b8c660...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>833</td>\n",
       "      <td>c:16208</td>\n",
       "      <td>c:16257</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>2007-03-02</td>\n",
       "      <td>http://venturebeat.com/2007/03/02/leadis-panel...</td>\n",
       "      <td>Leadis, panel display chip company, buys music...</td>\n",
       "      <td>2025-08-06 02:59:38.539496</td>\n",
       "      <td>2025-08-06 02:59:38.547236</td>\n",
       "      <td>006e11b66252dcbf1116cac2673486d59bc7e1b6e7acf9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1238</td>\n",
       "      <td>c:7493</td>\n",
       "      <td>c:4369</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>2009-06-23</td>\n",
       "      <td>http://www.techcrunch.com/2009/06/23/posterous...</td>\n",
       "      <td>Posterous Acquires Fellow Y Combinator Alum Sl...</td>\n",
       "      <td>2025-08-06 02:59:38.539496</td>\n",
       "      <td>2025-08-06 02:59:38.547236</td>\n",
       "      <td>074d4339e2d6dfd0a961879331881cfccd09e4edee7c03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1342</td>\n",
       "      <td>c:26071</td>\n",
       "      <td>c:26070</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>2009-03-16</td>\n",
       "      <td>http://www.businesswire.com/portal/site/google...</td>\n",
       "      <td>ManTech Completes the Acquisition of DDK Techn...</td>\n",
       "      <td>2025-08-06 02:59:38.539496</td>\n",
       "      <td>2025-08-06 02:59:38.547236</td>\n",
       "      <td>7851628491e2affc771c2fc2b75b0c0acd90518f80ec85...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1580</td>\n",
       "      <td>c:26919</td>\n",
       "      <td>c:26918</td>\n",
       "      <td></td>\n",
       "      <td>7350000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>2008-06-09</td>\n",
       "      <td>http://findarticles.com/p/articles/mi_m0EIN/is...</td>\n",
       "      <td>INX Acquires AccessFlow, Inc., a Leading VMwar...</td>\n",
       "      <td>2025-08-06 02:59:38.539496</td>\n",
       "      <td>2025-08-06 02:59:38.547236</td>\n",
       "      <td>42aead5d1c9d9b2cee6cd354d473d9f6f924f7fd55dcd7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acquisition_id acquiring_object_id acquired_object_id term_code  \\\n",
       "0             148              c:2050             c:2108      cash   \n",
       "1             833             c:16208            c:16257             \n",
       "2            1238              c:7493             c:4369             \n",
       "3            1342             c:26071            c:26070             \n",
       "4            1580             c:26919            c:26918             \n",
       "\n",
       "  price_amount price_currency_code acquired_at  \\\n",
       "0         0.00                 USD  2007-08-01   \n",
       "1         0.00                 USD  2007-03-02   \n",
       "2         0.00                 USD  2009-06-23   \n",
       "3         0.00                 USD  2009-03-16   \n",
       "4   7350000.00                 USD  2008-06-09   \n",
       "\n",
       "                                          source_url  \\\n",
       "0  http://www.dating-weblog.com/50226711/avid_lif...   \n",
       "1  http://venturebeat.com/2007/03/02/leadis-panel...   \n",
       "2  http://www.techcrunch.com/2009/06/23/posterous...   \n",
       "3  http://www.businesswire.com/portal/site/google...   \n",
       "4  http://findarticles.com/p/articles/mi_m0EIN/is...   \n",
       "\n",
       "                                  source_description  \\\n",
       "0         Ashley Madison acquired by Avid Life Media   \n",
       "1  Leadis, panel display chip company, buys music...   \n",
       "2  Posterous Acquires Fellow Y Combinator Alum Sl...   \n",
       "3  ManTech Completes the Acquisition of DDK Techn...   \n",
       "4  INX Acquires AccessFlow, Inc., a Leading VMwar...   \n",
       "\n",
       "                  created_at                 updated_at  \\\n",
       "0 2025-08-06 02:59:38.539496 2025-08-06 02:59:38.547236   \n",
       "1 2025-08-06 02:59:38.539496 2025-08-06 02:59:38.547236   \n",
       "2 2025-08-06 02:59:38.539496 2025-08-06 02:59:38.547236   \n",
       "3 2025-08-06 02:59:38.539496 2025-08-06 02:59:38.547236   \n",
       "4 2025-08-06 02:59:38.539496 2025-08-06 02:59:38.547236   \n",
       "\n",
       "                                             hash_id  \n",
       "0  141879dcbb6b99982989840b8c3cfc57eea59ee7b8c660...  \n",
       "1  006e11b66252dcbf1116cac2673486d59bc7e1b6e7acf9...  \n",
       "2  074d4339e2d6dfd0a961879331881cfccd09e4edee7c03...  \n",
       "3  7851628491e2affc771c2fc2b75b0c0acd90518f80ec85...  \n",
       "4  42aead5d1c9d9b2cee6cd354d473d9f6f924f7fd55dcd7...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipo_id</th>\n",
       "      <th>object_id</th>\n",
       "      <th>valuation_amount</th>\n",
       "      <th>valuation_currency_code</th>\n",
       "      <th>raised_amount</th>\n",
       "      <th>raised_currency_code</th>\n",
       "      <th>public_at</th>\n",
       "      <th>stock_symbol</th>\n",
       "      <th>source_url</th>\n",
       "      <th>source_description</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>hash_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>296</td>\n",
       "      <td>c:39190</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>NaT</td>\n",
       "      <td>NASDAQ:EDGW</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-08-06 02:59:38.555792</td>\n",
       "      <td>2025-08-06 02:59:38.566955</td>\n",
       "      <td>d6c9609f60468b7b01cf4125c0f4e6d41c2ee99bb16efe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>467</td>\n",
       "      <td>c:29567</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>1986-10-03</td>\n",
       "      <td>NASDAQ:FISV</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-08-06 02:59:38.555792</td>\n",
       "      <td>2025-08-06 02:59:38.566955</td>\n",
       "      <td>b519525ecccfdb213b1a61038ce1eb0674a0e5c209743c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>675</td>\n",
       "      <td>c:9389</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>NaT</td>\n",
       "      <td>NASDAQ:ZAGG</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-08-06 02:59:38.555792</td>\n",
       "      <td>2025-08-06 02:59:38.566955</td>\n",
       "      <td>45da9425eb8f1668703bd3065561333dad216c78685d71...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>691</td>\n",
       "      <td>c:82539</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>NaT</td>\n",
       "      <td>NASDAQ:AVNW</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-08-06 02:59:38.555792</td>\n",
       "      <td>2025-08-06 02:59:38.566955</td>\n",
       "      <td>7aff283acac07901a4149648d4eb0f275654b7e1b2de17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>829</td>\n",
       "      <td>c:39921</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>2000-09-29</td>\n",
       "      <td>NYSE:UMC</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-08-06 02:59:38.555792</td>\n",
       "      <td>2025-08-06 02:59:38.566955</td>\n",
       "      <td>003972c78fd391f911d6533809c2668732f5285067cbde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ipo_id object_id valuation_amount valuation_currency_code raised_amount  \\\n",
       "0    296   c:39190             0.00                     USD          0.00   \n",
       "1    467   c:29567             0.00                     USD          0.00   \n",
       "2    675    c:9389             0.00                     USD          0.00   \n",
       "3    691   c:82539             0.00                     USD          0.00   \n",
       "4    829   c:39921             0.00                     USD          0.00   \n",
       "\n",
       "  raised_currency_code  public_at stock_symbol source_url source_description  \\\n",
       "0                             NaT  NASDAQ:EDGW                                 \n",
       "1                  USD 1986-10-03  NASDAQ:FISV                                 \n",
       "2                             NaT  NASDAQ:ZAGG                                 \n",
       "3                             NaT  NASDAQ:AVNW                                 \n",
       "4                  USD 2000-09-29     NYSE:UMC                                 \n",
       "\n",
       "                  created_at                 updated_at  \\\n",
       "0 2025-08-06 02:59:38.555792 2025-08-06 02:59:38.566955   \n",
       "1 2025-08-06 02:59:38.555792 2025-08-06 02:59:38.566955   \n",
       "2 2025-08-06 02:59:38.555792 2025-08-06 02:59:38.566955   \n",
       "3 2025-08-06 02:59:38.555792 2025-08-06 02:59:38.566955   \n",
       "4 2025-08-06 02:59:38.555792 2025-08-06 02:59:38.566955   \n",
       "\n",
       "                                             hash_id  \n",
       "0  d6c9609f60468b7b01cf4125c0f4e6d41c2ee99bb16efe...  \n",
       "1  b519525ecccfdb213b1a61038ce1eb0674a0e5c209743c...  \n",
       "2  45da9425eb8f1668703bd3065561333dad216c78685d71...  \n",
       "3  7aff283acac07901a4149648d4eb0f275654b7e1b2de17...  \n",
       "4  003972c78fd391f911d6533809c2668732f5285067cbde...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fund_id</th>\n",
       "      <th>object_id</th>\n",
       "      <th>name</th>\n",
       "      <th>funded_at</th>\n",
       "      <th>raised_amount</th>\n",
       "      <th>raised_currency_code</th>\n",
       "      <th>source_url</th>\n",
       "      <th>source_description</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>hash_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>296</td>\n",
       "      <td>f:1913</td>\n",
       "      <td>Health-Care Fund IV</td>\n",
       "      <td>2010-08-18</td>\n",
       "      <td>180000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>https://www.fis.dowjones.com/article.aspx?Prod...</td>\n",
       "      <td>Prospect Venture Partners Secures $180M Toward...</td>\n",
       "      <td>2025-08-06 02:59:38.576227</td>\n",
       "      <td>2025-08-06 02:59:38.588152</td>\n",
       "      <td>ce98df44e9a3cae5c40c2fe9faf0c7e37d94d323c9dfdd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>467</td>\n",
       "      <td>f:7448</td>\n",
       "      <td>GCP III</td>\n",
       "      <td>2011-05-20</td>\n",
       "      <td>160000000.00</td>\n",
       "      <td>GBP</td>\n",
       "      <td>http://www.finsmes.com/2011/05/growth-capital-...</td>\n",
       "      <td>Growth Capital Partners Closes Â£160M Third Lo...</td>\n",
       "      <td>2025-08-06 02:59:38.576227</td>\n",
       "      <td>2025-08-06 02:59:38.588152</td>\n",
       "      <td>aeb68c8929eb6fc76e4601bc0334957695a5b8d8b7e191...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>691</td>\n",
       "      <td>f:8768</td>\n",
       "      <td>EC3H</td>\n",
       "      <td>2011-06-02</td>\n",
       "      <td>1500000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>http://www.nebraskaglobal.com/News</td>\n",
       "      <td>Nebraska Global News</td>\n",
       "      <td>2025-08-06 02:59:38.576227</td>\n",
       "      <td>2025-08-06 02:59:38.588152</td>\n",
       "      <td>2ceb080c1bccdcb94d0d525c76e2296b060e2339a5d3de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>675</td>\n",
       "      <td>f:4850</td>\n",
       "      <td>Fund III</td>\n",
       "      <td>2010-08-02</td>\n",
       "      <td>73500000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>http://www.sec.gov/Archives/edgar/data/1497490...</td>\n",
       "      <td>SEC</td>\n",
       "      <td>2025-08-06 02:59:38.576227</td>\n",
       "      <td>2025-08-06 02:59:38.588152</td>\n",
       "      <td>6cce5e03e4f0902c48c981860a6d6a8976ea0789e72b48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>829</td>\n",
       "      <td>f:1140</td>\n",
       "      <td>Tandem Fund II</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>32000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>http://techcrunch.com/2012/06/14/tandem-raises...</td>\n",
       "      <td>Tandem Raises $32M Fund From Verifone CEO, Pla...</td>\n",
       "      <td>2025-08-06 02:59:38.576227</td>\n",
       "      <td>2025-08-06 02:59:38.588152</td>\n",
       "      <td>e929a589c54bfcc6858aaa4580154d662cb9b1bd43c785...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  fund_id object_id                 name   funded_at raised_amount  \\\n",
       "0     296    f:1913  Health-Care Fund IV  2010-08-18  180000000.00   \n",
       "1     467    f:7448              GCP III  2011-05-20  160000000.00   \n",
       "2     691    f:8768                 EC3H  2011-06-02    1500000.00   \n",
       "3     675    f:4850             Fund III  2010-08-02   73500000.00   \n",
       "4     829    f:1140       Tandem Fund II  2012-06-14   32000000.00   \n",
       "\n",
       "  raised_currency_code                                         source_url  \\\n",
       "0                  USD  https://www.fis.dowjones.com/article.aspx?Prod...   \n",
       "1                  GBP  http://www.finsmes.com/2011/05/growth-capital-...   \n",
       "2                  USD                 http://www.nebraskaglobal.com/News   \n",
       "3                  USD  http://www.sec.gov/Archives/edgar/data/1497490...   \n",
       "4                  USD  http://techcrunch.com/2012/06/14/tandem-raises...   \n",
       "\n",
       "                                  source_description  \\\n",
       "0  Prospect Venture Partners Secures $180M Toward...   \n",
       "1  Growth Capital Partners Closes Â£160M Third Lo...   \n",
       "2                               Nebraska Global News   \n",
       "3                                                SEC   \n",
       "4  Tandem Raises $32M Fund From Verifone CEO, Pla...   \n",
       "\n",
       "                  created_at                 updated_at  \\\n",
       "0 2025-08-06 02:59:38.576227 2025-08-06 02:59:38.588152   \n",
       "1 2025-08-06 02:59:38.576227 2025-08-06 02:59:38.588152   \n",
       "2 2025-08-06 02:59:38.576227 2025-08-06 02:59:38.588152   \n",
       "3 2025-08-06 02:59:38.576227 2025-08-06 02:59:38.588152   \n",
       "4 2025-08-06 02:59:38.576227 2025-08-06 02:59:38.588152   \n",
       "\n",
       "                                             hash_id  \n",
       "0  ce98df44e9a3cae5c40c2fe9faf0c7e37d94d323c9dfdd...  \n",
       "1  aeb68c8929eb6fc76e4601bc0334957695a5b8d8b7e191...  \n",
       "2  2ceb080c1bccdcb94d0d525c76e2296b060e2339a5d3de...  \n",
       "3  6cce5e03e4f0902c48c981860a6d6a8976ea0789e72b48...  \n",
       "4  e929a589c54bfcc6858aaa4580154d662cb9b1bd43c785...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funding_rounds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>funding_round_id</th>\n",
       "      <th>object_id</th>\n",
       "      <th>funded_at</th>\n",
       "      <th>funding_round_type</th>\n",
       "      <th>funding_round_code</th>\n",
       "      <th>raised_amount_usd</th>\n",
       "      <th>raised_amount</th>\n",
       "      <th>raised_currency_code</th>\n",
       "      <th>pre_money_valuation_usd</th>\n",
       "      <th>pre_money_valuation</th>\n",
       "      <th>...</th>\n",
       "      <th>post_money_currency_code</th>\n",
       "      <th>participants</th>\n",
       "      <th>is_first_round</th>\n",
       "      <th>is_last_round</th>\n",
       "      <th>source_url</th>\n",
       "      <th>source_description</th>\n",
       "      <th>created_by</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>hash_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>c:34</td>\n",
       "      <td>2007-02-01</td>\n",
       "      <td>series-b</td>\n",
       "      <td>b</td>\n",
       "      <td>5500000.00</td>\n",
       "      <td>5500000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>USD</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>http://venturebeat.com/2007/03/06/widget-compa...</td>\n",
       "      <td>Widget company, Clearspring, says it leads mar...</td>\n",
       "      <td>initial-importer</td>\n",
       "      <td>2025-08-06 02:59:38.599559</td>\n",
       "      <td>2025-08-06 02:59:38.611979</td>\n",
       "      <td>63e8afc119470f7b2ae44d262b2d13828439af25ad5aec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>c:35</td>\n",
       "      <td>2007-06-01</td>\n",
       "      <td>series-a</td>\n",
       "      <td>a</td>\n",
       "      <td>5000000.00</td>\n",
       "      <td>5000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>USD</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>http://www.techcrunch.com/2007/06/13/openads-o...</td>\n",
       "      <td></td>\n",
       "      <td>initial-importer</td>\n",
       "      <td>2025-08-06 02:59:38.599559</td>\n",
       "      <td>2025-08-06 02:59:38.611979</td>\n",
       "      <td>1bb839d19ea5eca80632e5189e8106ca3884a42227fccb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>c:40</td>\n",
       "      <td>2007-08-01</td>\n",
       "      <td>series-c+</td>\n",
       "      <td>c</td>\n",
       "      <td>25000000.00</td>\n",
       "      <td>25000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>http://newteevee.com/2007/06/14/veoh-goes-for-...</td>\n",
       "      <td>Veoh Goes for the Big Money</td>\n",
       "      <td>initial-importer</td>\n",
       "      <td>2025-08-06 02:59:38.599559</td>\n",
       "      <td>2025-08-06 02:59:38.611979</td>\n",
       "      <td>dc303232407768cd02bfffb903590b5916e4235d03e337...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>c:75</td>\n",
       "      <td>2005-04-01</td>\n",
       "      <td>angel</td>\n",
       "      <td>seed</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>http://www.techcrunch.com/?p=23</td>\n",
       "      <td>Profile: Del.icio.us</td>\n",
       "      <td>initial-importer</td>\n",
       "      <td>2025-08-06 02:59:38.599559</td>\n",
       "      <td>2025-08-06 02:59:38.611979</td>\n",
       "      <td>1bb9667c029116d88cc99e119ceb00b603697bb15c2082...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65</td>\n",
       "      <td>c:84</td>\n",
       "      <td>2006-06-01</td>\n",
       "      <td>series-a</td>\n",
       "      <td>a</td>\n",
       "      <td>5000000.00</td>\n",
       "      <td>5000000.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>USD</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>http://www.techcrunch.com/2006/08/02/kleiner-p...</td>\n",
       "      <td></td>\n",
       "      <td>initial-importer</td>\n",
       "      <td>2025-08-06 02:59:38.599559</td>\n",
       "      <td>2025-08-06 02:59:38.611979</td>\n",
       "      <td>be19521611e2f64289c25e5410975f7dbd8d429e795320...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   funding_round_id object_id   funded_at funding_round_type  \\\n",
       "0                28      c:34  2007-02-01           series-b   \n",
       "1                31      c:35  2007-06-01           series-a   \n",
       "2                34      c:40  2007-08-01          series-c+   \n",
       "3                53      c:75  2005-04-01              angel   \n",
       "4                65      c:84  2006-06-01           series-a   \n",
       "\n",
       "  funding_round_code raised_amount_usd raised_amount raised_currency_code  \\\n",
       "0                  b        5500000.00    5500000.00                  USD   \n",
       "1                  a        5000000.00    5000000.00                  USD   \n",
       "2                  c       25000000.00   25000000.00                  USD   \n",
       "3               seed              0.00          0.00                        \n",
       "4                  a        5000000.00    5000000.00                  USD   \n",
       "\n",
       "  pre_money_valuation_usd pre_money_valuation  ... post_money_currency_code  \\\n",
       "0                    0.00                0.00  ...                      USD   \n",
       "1                    0.00                0.00  ...                      USD   \n",
       "2                    0.00                0.00  ...                            \n",
       "3                    0.00                0.00  ...                            \n",
       "4                    0.00                0.00  ...                      USD   \n",
       "\n",
       "  participants is_first_round is_last_round  \\\n",
       "0            6          False         False   \n",
       "1            4          False         False   \n",
       "2            5          False         False   \n",
       "3            4          False         False   \n",
       "4            2          False         False   \n",
       "\n",
       "                                          source_url  \\\n",
       "0  http://venturebeat.com/2007/03/06/widget-compa...   \n",
       "1  http://www.techcrunch.com/2007/06/13/openads-o...   \n",
       "2  http://newteevee.com/2007/06/14/veoh-goes-for-...   \n",
       "3                    http://www.techcrunch.com/?p=23   \n",
       "4  http://www.techcrunch.com/2006/08/02/kleiner-p...   \n",
       "\n",
       "                                  source_description        created_by  \\\n",
       "0  Widget company, Clearspring, says it leads mar...  initial-importer   \n",
       "1                                                     initial-importer   \n",
       "2                        Veoh Goes for the Big Money  initial-importer   \n",
       "3                               Profile: Del.icio.us  initial-importer   \n",
       "4                                                     initial-importer   \n",
       "\n",
       "                  created_at                 updated_at  \\\n",
       "0 2025-08-06 02:59:38.599559 2025-08-06 02:59:38.611979   \n",
       "1 2025-08-06 02:59:38.599559 2025-08-06 02:59:38.611979   \n",
       "2 2025-08-06 02:59:38.599559 2025-08-06 02:59:38.611979   \n",
       "3 2025-08-06 02:59:38.599559 2025-08-06 02:59:38.611979   \n",
       "4 2025-08-06 02:59:38.599559 2025-08-06 02:59:38.611979   \n",
       "\n",
       "                                             hash_id  \n",
       "0  63e8afc119470f7b2ae44d262b2d13828439af25ad5aec...  \n",
       "1  1bb839d19ea5eca80632e5189e8106ca3884a42227fccb...  \n",
       "2  dc303232407768cd02bfffb903590b5916e4235d03e337...  \n",
       "3  1bb9667c029116d88cc99e119ceb00b603697bb15c2082...  \n",
       "4  be19521611e2f64289c25e5410975f7dbd8d429e795320...  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "investments\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>investment_id</th>\n",
       "      <th>funding_round_id</th>\n",
       "      <th>funded_object_id</th>\n",
       "      <th>investor_object_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>hash_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "      <td>86</td>\n",
       "      <td>c:25732</td>\n",
       "      <td>f:72</td>\n",
       "      <td>2025-08-06 02:59:38.621590</td>\n",
       "      <td>2025-08-06 02:59:38.629096</td>\n",
       "      <td>9264c23c055c90c4e9900af1649e876cb4e7906d8529e9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>833</td>\n",
       "      <td>512</td>\n",
       "      <td>c:630</td>\n",
       "      <td>f:51</td>\n",
       "      <td>2025-08-06 02:59:38.621590</td>\n",
       "      <td>2025-08-06 02:59:38.629096</td>\n",
       "      <td>98ad0c5b6931a3256418160c9a7fbc88f43b2f8b06fa61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>463</td>\n",
       "      <td>276</td>\n",
       "      <td>c:326</td>\n",
       "      <td>f:11</td>\n",
       "      <td>2025-08-06 02:59:38.621590</td>\n",
       "      <td>2025-08-06 02:59:38.629096</td>\n",
       "      <td>800585bc517f56d2442c102cc7358c49ba0c83dec75b68...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>471</td>\n",
       "      <td>278</td>\n",
       "      <td>c:302</td>\n",
       "      <td>f:89</td>\n",
       "      <td>2025-08-06 02:59:38.621590</td>\n",
       "      <td>2025-08-06 02:59:38.629096</td>\n",
       "      <td>6551ac561a259a78d17d7a7d13dc5ddf6d66640ec2b0d8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>496</td>\n",
       "      <td>291</td>\n",
       "      <td>c:325</td>\n",
       "      <td>f:1287</td>\n",
       "      <td>2025-08-06 02:59:38.621590</td>\n",
       "      <td>2025-08-06 02:59:38.629096</td>\n",
       "      <td>ad9e2b2aaa14bfb4224f04f3fb8de374aae0299aa82dab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   investment_id  funding_round_id funded_object_id investor_object_id  \\\n",
       "0            148                86          c:25732               f:72   \n",
       "1            833               512            c:630               f:51   \n",
       "2            463               276            c:326               f:11   \n",
       "3            471               278            c:302               f:89   \n",
       "4            496               291            c:325             f:1287   \n",
       "\n",
       "                  created_at                 updated_at  \\\n",
       "0 2025-08-06 02:59:38.621590 2025-08-06 02:59:38.629096   \n",
       "1 2025-08-06 02:59:38.621590 2025-08-06 02:59:38.629096   \n",
       "2 2025-08-06 02:59:38.621590 2025-08-06 02:59:38.629096   \n",
       "3 2025-08-06 02:59:38.621590 2025-08-06 02:59:38.629096   \n",
       "4 2025-08-06 02:59:38.621590 2025-08-06 02:59:38.629096   \n",
       "\n",
       "                                             hash_id  \n",
       "0  9264c23c055c90c4e9900af1649e876cb4e7906d8529e9...  \n",
       "1  98ad0c5b6931a3256418160c9a7fbc88f43b2f8b06fa61...  \n",
       "2  800585bc517f56d2442c102cc7358c49ba0c83dec75b68...  \n",
       "3  6551ac561a259a78d17d7a7d13dc5ddf6d66640ec2b0d8...  \n",
       "4  ad9e2b2aaa14bfb4224f04f3fb8de374aae0299aa82dab...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>date_actual</th>\n",
       "      <th>day_suffix</th>\n",
       "      <th>day_name</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>week_of_year_iso</th>\n",
       "      <th>month_actual</th>\n",
       "      <th>month_name</th>\n",
       "      <th>...</th>\n",
       "      <th>last_day_of_month</th>\n",
       "      <th>first_day_of_quarter</th>\n",
       "      <th>last_day_of_quarter</th>\n",
       "      <th>first_day_of_year</th>\n",
       "      <th>last_day_of_year</th>\n",
       "      <th>mmyyyy</th>\n",
       "      <th>mmddyyyy</th>\n",
       "      <th>weekend_indr</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19500101</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1st</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>1950-W52</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>...</td>\n",
       "      <td>1950-01-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-03-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-12-31</td>\n",
       "      <td>11950</td>\n",
       "      <td>1011950</td>\n",
       "      <td>weekend</td>\n",
       "      <td>2025-08-06 03:49:03.952004</td>\n",
       "      <td>2025-08-06 03:49:03.964572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19500102</td>\n",
       "      <td>1950-01-02</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Monday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1950-W01</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>...</td>\n",
       "      <td>1950-01-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-03-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-12-31</td>\n",
       "      <td>11950</td>\n",
       "      <td>1021950</td>\n",
       "      <td>weekday</td>\n",
       "      <td>2025-08-06 03:49:03.952004</td>\n",
       "      <td>2025-08-06 03:49:03.964572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19500103</td>\n",
       "      <td>1950-01-03</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1950-W01</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>...</td>\n",
       "      <td>1950-01-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-03-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-12-31</td>\n",
       "      <td>11950</td>\n",
       "      <td>1031950</td>\n",
       "      <td>weekday</td>\n",
       "      <td>2025-08-06 03:49:03.952004</td>\n",
       "      <td>2025-08-06 03:49:03.964572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19500104</td>\n",
       "      <td>1950-01-04</td>\n",
       "      <td>4th</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1950-W01</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>...</td>\n",
       "      <td>1950-01-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-03-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-12-31</td>\n",
       "      <td>11950</td>\n",
       "      <td>1041950</td>\n",
       "      <td>weekday</td>\n",
       "      <td>2025-08-06 03:49:03.952004</td>\n",
       "      <td>2025-08-06 03:49:03.964572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19500105</td>\n",
       "      <td>1950-01-05</td>\n",
       "      <td>5th</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1950-W01</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>...</td>\n",
       "      <td>1950-01-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-03-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-12-31</td>\n",
       "      <td>11950</td>\n",
       "      <td>1051950</td>\n",
       "      <td>weekday</td>\n",
       "      <td>2025-08-06 03:49:03.952004</td>\n",
       "      <td>2025-08-06 03:49:03.964572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    date_id date_actual day_suffix   day_name  day_of_year  week_of_month  \\\n",
       "0  19500101  1950-01-01        1st     Sunday            1              1   \n",
       "1  19500102  1950-01-02        2nd     Monday            2              1   \n",
       "2  19500103  1950-01-03        3rd    Tuesday            3              1   \n",
       "3  19500104  1950-01-04        4th  Wednesday            4              1   \n",
       "4  19500105  1950-01-05        5th   Thursday            5              1   \n",
       "\n",
       "   week_of_year week_of_year_iso  month_actual month_name  ...  \\\n",
       "0            52         1950-W52             1    January  ...   \n",
       "1             1         1950-W01             1    January  ...   \n",
       "2             1         1950-W01             1    January  ...   \n",
       "3             1         1950-W01             1    January  ...   \n",
       "4             1         1950-W01             1    January  ...   \n",
       "\n",
       "  last_day_of_month  first_day_of_quarter last_day_of_quarter  \\\n",
       "0        1950-01-31            1950-01-01          1950-03-31   \n",
       "1        1950-01-31            1950-01-01          1950-03-31   \n",
       "2        1950-01-31            1950-01-01          1950-03-31   \n",
       "3        1950-01-31            1950-01-01          1950-03-31   \n",
       "4        1950-01-31            1950-01-01          1950-03-31   \n",
       "\n",
       "   first_day_of_year last_day_of_year mmyyyy mmddyyyy weekend_indr  \\\n",
       "0         1950-01-01       1950-12-31  11950  1011950      weekend   \n",
       "1         1950-01-01       1950-12-31  11950  1021950      weekday   \n",
       "2         1950-01-01       1950-12-31  11950  1031950      weekday   \n",
       "3         1950-01-01       1950-12-31  11950  1041950      weekday   \n",
       "4         1950-01-01       1950-12-31  11950  1051950      weekday   \n",
       "\n",
       "                  created_at                 updated_at  \n",
       "0 2025-08-06 03:49:03.952004 2025-08-06 03:49:03.964572  \n",
       "1 2025-08-06 03:49:03.952004 2025-08-06 03:49:03.964572  \n",
       "2 2025-08-06 03:49:03.952004 2025-08-06 03:49:03.964572  \n",
       "3 2025-08-06 03:49:03.952004 2025-08-06 03:49:03.964572  \n",
       "4 2025-08-06 03:49:03.952004 2025-08-06 03:49:03.964572  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people_id</th>\n",
       "      <th>object_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>birthplace</th>\n",
       "      <th>affiliation_name</th>\n",
       "      <th>hash_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>p:32</td>\n",
       "      <td>Ken</td>\n",
       "      <td>Howery</td>\n",
       "      <td>None</td>\n",
       "      <td>The Founder's Fund</td>\n",
       "      <td>7bc45caa0807c532b07a56b463b4f496976ca0b80c9f54...</td>\n",
       "      <td>2025-08-06 02:59:38.658130</td>\n",
       "      <td>2025-08-06 02:59:38.665566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>p:35</td>\n",
       "      <td>George</td>\n",
       "      <td>Zachary</td>\n",
       "      <td>None</td>\n",
       "      <td>Millennial Media</td>\n",
       "      <td>049b5f2e183e1522852f203b2330de5b4c5e04c052126f...</td>\n",
       "      <td>2025-08-06 02:59:38.658130</td>\n",
       "      <td>2025-08-06 02:59:38.665566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>p:40</td>\n",
       "      <td>Andy</td>\n",
       "      <td>Gavin</td>\n",
       "      <td>None</td>\n",
       "      <td>Flektor</td>\n",
       "      <td>6489369ef88475e67b508ec93721e187739103ecb604b2...</td>\n",
       "      <td>2025-08-06 02:59:38.658130</td>\n",
       "      <td>2025-08-06 02:59:38.665566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>p:63</td>\n",
       "      <td>Jared</td>\n",
       "      <td>Friedman</td>\n",
       "      <td>None</td>\n",
       "      <td>Scribd</td>\n",
       "      <td>8e338e0dbc395cb75087f680f964fcbb36b40dea9e0880...</td>\n",
       "      <td>2025-08-06 02:59:38.658130</td>\n",
       "      <td>2025-08-06 02:59:38.665566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65</td>\n",
       "      <td>p:75</td>\n",
       "      <td>Pierre</td>\n",
       "      <td>Omidyar</td>\n",
       "      <td>None</td>\n",
       "      <td>Omidyar Network</td>\n",
       "      <td>7fc4d066bca8f2cbcc827a0f1838bc5a2d222b5510f189...</td>\n",
       "      <td>2025-08-06 02:59:38.658130</td>\n",
       "      <td>2025-08-06 02:59:38.665566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   people_id object_id first_name last_name birthplace    affiliation_name  \\\n",
       "0         28      p:32        Ken    Howery       None  The Founder's Fund   \n",
       "1         31      p:35     George   Zachary       None    Millennial Media   \n",
       "2         34      p:40       Andy     Gavin       None             Flektor   \n",
       "3         53      p:63      Jared  Friedman       None              Scribd   \n",
       "4         65      p:75     Pierre   Omidyar       None     Omidyar Network   \n",
       "\n",
       "                                             hash_id  \\\n",
       "0  7bc45caa0807c532b07a56b463b4f496976ca0b80c9f54...   \n",
       "1  049b5f2e183e1522852f203b2330de5b4c5e04c052126f...   \n",
       "2  6489369ef88475e67b508ec93721e187739103ecb604b2...   \n",
       "3  8e338e0dbc395cb75087f680f964fcbb36b40dea9e0880...   \n",
       "4  7fc4d066bca8f2cbcc827a0f1838bc5a2d222b5510f189...   \n",
       "\n",
       "                  created_at                 updated_at  \n",
       "0 2025-08-06 02:59:38.658130 2025-08-06 02:59:38.665566  \n",
       "1 2025-08-06 02:59:38.658130 2025-08-06 02:59:38.665566  \n",
       "2 2025-08-06 02:59:38.658130 2025-08-06 02:59:38.665566  \n",
       "3 2025-08-06 02:59:38.658130 2025-08-06 02:59:38.665566  \n",
       "4 2025-08-06 02:59:38.658130 2025-08-06 02:59:38.665566  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relationships\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relationship_id</th>\n",
       "      <th>person_object_id</th>\n",
       "      <th>relationship_object_id</th>\n",
       "      <th>start_at</th>\n",
       "      <th>end_at</th>\n",
       "      <th>is_past</th>\n",
       "      <th>sequence</th>\n",
       "      <th>title</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>hash_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>p:31</td>\n",
       "      <td>c:9</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>Board Member</td>\n",
       "      <td>2025-08-06 02:59:38.676302</td>\n",
       "      <td>2025-08-06 02:59:38.684417</td>\n",
       "      <td>2b431ff0f3403880e4863c8a516f21222d40f5ae909bb9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>p:33</td>\n",
       "      <td>f:6</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>Partner</td>\n",
       "      <td>2025-08-06 02:59:38.676302</td>\n",
       "      <td>2025-08-06 02:59:38.684417</td>\n",
       "      <td>91674b2266a873073be51c643585e3ec47659f1e1cf888...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>p:35</td>\n",
       "      <td>c:9</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>Board Member</td>\n",
       "      <td>2025-08-06 02:59:38.676302</td>\n",
       "      <td>2025-08-06 02:59:38.684417</td>\n",
       "      <td>3c62d7409bbfe836d25a18481a2b0cc5ec43e60937e3ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>p:52</td>\n",
       "      <td>c:14</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Chief Technology Officer</td>\n",
       "      <td>2025-08-06 02:59:38.676302</td>\n",
       "      <td>2025-08-06 02:59:38.684417</td>\n",
       "      <td>78fefff8c1396f83e24735f52427e08d1aa904f0e6b99d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65</td>\n",
       "      <td>p:64</td>\n",
       "      <td>c:15</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>COO/Co-Founder</td>\n",
       "      <td>2025-08-06 02:59:38.676302</td>\n",
       "      <td>2025-08-06 02:59:38.684417</td>\n",
       "      <td>24b9f7102a5e5e0725316389d4b67dfc75a3f5fc22a996...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   relationship_id person_object_id relationship_object_id start_at end_at  \\\n",
       "0               28             p:31                    c:9      NaT    NaT   \n",
       "1               31             p:33                    f:6      NaT    NaT   \n",
       "2               34             p:35                    c:9      NaT    NaT   \n",
       "3               53             p:52                   c:14      NaT    NaT   \n",
       "4               65             p:64                   c:15      NaT    NaT   \n",
       "\n",
       "   is_past  sequence                     title                 created_at  \\\n",
       "0    False         3              Board Member 2025-08-06 02:59:38.676302   \n",
       "1    False         5                   Partner 2025-08-06 02:59:38.676302   \n",
       "2    False         2              Board Member 2025-08-06 02:59:38.676302   \n",
       "3    False         1  Chief Technology Officer 2025-08-06 02:59:38.676302   \n",
       "4    False         1            COO/Co-Founder 2025-08-06 02:59:38.676302   \n",
       "\n",
       "                  updated_at  \\\n",
       "0 2025-08-06 02:59:38.684417   \n",
       "1 2025-08-06 02:59:38.684417   \n",
       "2 2025-08-06 02:59:38.684417   \n",
       "3 2025-08-06 02:59:38.684417   \n",
       "4 2025-08-06 02:59:38.684417   \n",
       "\n",
       "                                             hash_id  \n",
       "0  2b431ff0f3403880e4863c8a516f21222d40f5ae909bb9...  \n",
       "1  91674b2266a873073be51c643585e3ec47659f1e1cf888...  \n",
       "2  3c62d7409bbfe836d25a18481a2b0cc5ec43e60937e3ce...  \n",
       "3  78fefff8c1396f83e24735f52427e08d1aa904f0e6b99d...  \n",
       "4  24b9f7102a5e5e0725316389d4b67dfc75a3f5fc22a996...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "milestones\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>description</th>\n",
       "      <th>milestone_at</th>\n",
       "      <th>milestone_code</th>\n",
       "      <th>milestone_id</th>\n",
       "      <th>object_id</th>\n",
       "      <th>source_description</th>\n",
       "      <th>source_url</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>hash_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-06 02:59:38.693422</td>\n",
       "      <td>The African Virtual School Maths Book is avail...</td>\n",
       "      <td>2009-12-13</td>\n",
       "      <td>other</td>\n",
       "      <td>2453</td>\n",
       "      <td>c:36287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.amazon.com/African-Virtual-School-M...</td>\n",
       "      <td>2025-08-06 02:59:38.701004</td>\n",
       "      <td>25972407eb7c70e2d9ec566b59a25a86cb831256ae5e33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-06 02:59:38.693422</td>\n",
       "      <td>SaaS business intelligence with TBlox</td>\n",
       "      <td>2008-01-01</td>\n",
       "      <td>other</td>\n",
       "      <td>3506</td>\n",
       "      <td>c:43798</td>\n",
       "      <td>Control Your TBlox Managed Business Activities</td>\n",
       "      <td>http://www.insfocus.com/TBlox/</td>\n",
       "      <td>2025-08-06 02:59:38.701004</td>\n",
       "      <td>9304413f9dc8b2a3cb88bef6b8a9c0dc0859e359b2e5e0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-06 02:59:38.693422</td>\n",
       "      <td>Awarded Microsoft Australia Online Services Pa...</td>\n",
       "      <td>2010-09-03</td>\n",
       "      <td>other</td>\n",
       "      <td>6721</td>\n",
       "      <td>c:27524</td>\n",
       "      <td>DiData takes out Microsoft enterprise partner ...</td>\n",
       "      <td>http://www.crn.com.au/News/230958,didata-takes...</td>\n",
       "      <td>2025-08-06 02:59:38.701004</td>\n",
       "      <td>1b95c2e31e3b7038a3d8f3f5994849847bf186bfe57258...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-06 02:59:38.693422</td>\n",
       "      <td>SourceForge, Inc. Changes its Name to Geeknet,...</td>\n",
       "      <td>2009-11-04</td>\n",
       "      <td>other</td>\n",
       "      <td>2509</td>\n",
       "      <td>c:4303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://geek.net/press/sourceforge-inc-changes-...</td>\n",
       "      <td>2025-08-06 02:59:38.701004</td>\n",
       "      <td>c9cb5202411f13eee3fc39c53e2b30521ebca91c0b28a5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-06 02:59:38.693422</td>\n",
       "      <td>HONG KONG, Dec. 18 /PRNewswire-Asia/ -- Mulabo...</td>\n",
       "      <td>2009-12-18</td>\n",
       "      <td>other</td>\n",
       "      <td>2529</td>\n",
       "      <td>c:6336</td>\n",
       "      <td>Muecs Ltd. Launches an Online Collaboration To...</td>\n",
       "      <td>http://www.prnewswire.com/news-releases/muecs-...</td>\n",
       "      <td>2025-08-06 02:59:38.701004</td>\n",
       "      <td>64671c594f0aa50647a8050ba50c6a021c7b17dfa3863f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at  \\\n",
       "0 2025-08-06 02:59:38.693422   \n",
       "1 2025-08-06 02:59:38.693422   \n",
       "2 2025-08-06 02:59:38.693422   \n",
       "3 2025-08-06 02:59:38.693422   \n",
       "4 2025-08-06 02:59:38.693422   \n",
       "\n",
       "                                         description milestone_at  \\\n",
       "0  The African Virtual School Maths Book is avail...   2009-12-13   \n",
       "1              SaaS business intelligence with TBlox   2008-01-01   \n",
       "2  Awarded Microsoft Australia Online Services Pa...   2010-09-03   \n",
       "3  SourceForge, Inc. Changes its Name to Geeknet,...   2009-11-04   \n",
       "4  HONG KONG, Dec. 18 /PRNewswire-Asia/ -- Mulabo...   2009-12-18   \n",
       "\n",
       "  milestone_code  milestone_id object_id  \\\n",
       "0          other          2453   c:36287   \n",
       "1          other          3506   c:43798   \n",
       "2          other          6721   c:27524   \n",
       "3          other          2509    c:4303   \n",
       "4          other          2529    c:6336   \n",
       "\n",
       "                                  source_description  \\\n",
       "0                                                NaN   \n",
       "1     Control Your TBlox Managed Business Activities   \n",
       "2  DiData takes out Microsoft enterprise partner ...   \n",
       "3                                                NaN   \n",
       "4  Muecs Ltd. Launches an Online Collaboration To...   \n",
       "\n",
       "                                          source_url  \\\n",
       "0  http://www.amazon.com/African-Virtual-School-M...   \n",
       "1                     http://www.insfocus.com/TBlox/   \n",
       "2  http://www.crn.com.au/News/230958,didata-takes...   \n",
       "3  http://geek.net/press/sourceforge-inc-changes-...   \n",
       "4  http://www.prnewswire.com/news-releases/muecs-...   \n",
       "\n",
       "                  updated_at  \\\n",
       "0 2025-08-06 02:59:38.701004   \n",
       "1 2025-08-06 02:59:38.701004   \n",
       "2 2025-08-06 02:59:38.701004   \n",
       "3 2025-08-06 02:59:38.701004   \n",
       "4 2025-08-06 02:59:38.701004   \n",
       "\n",
       "                                             hash_id  \n",
       "0  25972407eb7c70e2d9ec566b59a25a86cb831256ae5e33...  \n",
       "1  9304413f9dc8b2a3cb88bef6b8a9c0dc0859e359b2e5e0...  \n",
       "2  1b95c2e31e3b7038a3d8f3f5994849847bf186bfe57258...  \n",
       "3  c9cb5202411f13eee3fc39c53e2b30521ebca91c0b28a5...  \n",
       "4  64671c594f0aa50647a8050ba50c6a021c7b17dfa3863f...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k, v in src_dict.items():\n",
    "    print(k)\n",
    "    display(v.limit(5).toPandas().head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79395aff-ca50-46bb-9405-87ea40bd9620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubahh csv date\n",
    "pd.options.display.max_columns = 25\n",
    "date_df = pd.read_csv(\"data/date.csv\")\n",
    "date_df = date_df.set_index(\"date_id\")\n",
    "# date_df.to_csv(\"data/date.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f08f1e7-41eb-4c8d-b12d-e7f8a50ec8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_actual</th>\n",
       "      <th>day_suffix</th>\n",
       "      <th>day_name</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>week_of_year_iso</th>\n",
       "      <th>month_actual</th>\n",
       "      <th>month_name</th>\n",
       "      <th>month_name_abbreviated</th>\n",
       "      <th>quarter_actual</th>\n",
       "      <th>quarter_name</th>\n",
       "      <th>year_actual</th>\n",
       "      <th>first_day_of_week</th>\n",
       "      <th>last_day_of_week</th>\n",
       "      <th>first_day_of_month</th>\n",
       "      <th>last_day_of_month</th>\n",
       "      <th>first_day_of_quarter</th>\n",
       "      <th>last_day_of_quarter</th>\n",
       "      <th>first_day_of_year</th>\n",
       "      <th>last_day_of_year</th>\n",
       "      <th>mmyyyy</th>\n",
       "      <th>mmddyyyy</th>\n",
       "      <th>weekend_indr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19500101</th>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1st</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>1950-W52</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>Jan</td>\n",
       "      <td>1</td>\n",
       "      <td>First</td>\n",
       "      <td>1950</td>\n",
       "      <td>1949-12-26</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-01-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-03-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-12-31</td>\n",
       "      <td>11950</td>\n",
       "      <td>1011950</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19500102</th>\n",
       "      <td>1950-01-02</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Monday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1950-W01</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>Jan</td>\n",
       "      <td>1</td>\n",
       "      <td>First</td>\n",
       "      <td>1950</td>\n",
       "      <td>1950-01-02</td>\n",
       "      <td>1950-01-08</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-01-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-03-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-12-31</td>\n",
       "      <td>11950</td>\n",
       "      <td>1021950</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19500103</th>\n",
       "      <td>1950-01-03</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1950-W01</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>Jan</td>\n",
       "      <td>1</td>\n",
       "      <td>First</td>\n",
       "      <td>1950</td>\n",
       "      <td>1950-01-02</td>\n",
       "      <td>1950-01-08</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-01-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-03-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-12-31</td>\n",
       "      <td>11950</td>\n",
       "      <td>1031950</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19500104</th>\n",
       "      <td>1950-01-04</td>\n",
       "      <td>4th</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1950-W01</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>Jan</td>\n",
       "      <td>1</td>\n",
       "      <td>First</td>\n",
       "      <td>1950</td>\n",
       "      <td>1950-01-02</td>\n",
       "      <td>1950-01-08</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-01-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-03-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-12-31</td>\n",
       "      <td>11950</td>\n",
       "      <td>1041950</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19500105</th>\n",
       "      <td>1950-01-05</td>\n",
       "      <td>5th</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1950-W01</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>Jan</td>\n",
       "      <td>1</td>\n",
       "      <td>First</td>\n",
       "      <td>1950</td>\n",
       "      <td>1950-01-02</td>\n",
       "      <td>1950-01-08</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-01-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-03-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-12-31</td>\n",
       "      <td>11950</td>\n",
       "      <td>1051950</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20771227</th>\n",
       "      <td>2077-12-27</td>\n",
       "      <td>27th</td>\n",
       "      <td>Monday</td>\n",
       "      <td>361</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>2077-W52</td>\n",
       "      <td>12</td>\n",
       "      <td>December</td>\n",
       "      <td>Dec</td>\n",
       "      <td>4</td>\n",
       "      <td>Fourth</td>\n",
       "      <td>2077</td>\n",
       "      <td>2077-12-27</td>\n",
       "      <td>2078-01-02</td>\n",
       "      <td>2077-12-01</td>\n",
       "      <td>2077-12-31</td>\n",
       "      <td>2077-10-01</td>\n",
       "      <td>2077-12-31</td>\n",
       "      <td>2077-01-01</td>\n",
       "      <td>2077-12-31</td>\n",
       "      <td>122077</td>\n",
       "      <td>12272077</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20771228</th>\n",
       "      <td>2077-12-28</td>\n",
       "      <td>28th</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>362</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>2077-W52</td>\n",
       "      <td>12</td>\n",
       "      <td>December</td>\n",
       "      <td>Dec</td>\n",
       "      <td>4</td>\n",
       "      <td>Fourth</td>\n",
       "      <td>2077</td>\n",
       "      <td>2077-12-27</td>\n",
       "      <td>2078-01-02</td>\n",
       "      <td>2077-12-01</td>\n",
       "      <td>2077-12-31</td>\n",
       "      <td>2077-10-01</td>\n",
       "      <td>2077-12-31</td>\n",
       "      <td>2077-01-01</td>\n",
       "      <td>2077-12-31</td>\n",
       "      <td>122077</td>\n",
       "      <td>12282077</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20771229</th>\n",
       "      <td>2077-12-29</td>\n",
       "      <td>29th</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>363</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>2077-W52</td>\n",
       "      <td>12</td>\n",
       "      <td>December</td>\n",
       "      <td>Dec</td>\n",
       "      <td>4</td>\n",
       "      <td>Fourth</td>\n",
       "      <td>2077</td>\n",
       "      <td>2077-12-27</td>\n",
       "      <td>2078-01-02</td>\n",
       "      <td>2077-12-01</td>\n",
       "      <td>2077-12-31</td>\n",
       "      <td>2077-10-01</td>\n",
       "      <td>2077-12-31</td>\n",
       "      <td>2077-01-01</td>\n",
       "      <td>2077-12-31</td>\n",
       "      <td>122077</td>\n",
       "      <td>12292077</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20771230</th>\n",
       "      <td>2077-12-30</td>\n",
       "      <td>30th</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>364</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>2077-W52</td>\n",
       "      <td>12</td>\n",
       "      <td>December</td>\n",
       "      <td>Dec</td>\n",
       "      <td>4</td>\n",
       "      <td>Fourth</td>\n",
       "      <td>2077</td>\n",
       "      <td>2077-12-27</td>\n",
       "      <td>2078-01-02</td>\n",
       "      <td>2077-12-01</td>\n",
       "      <td>2077-12-31</td>\n",
       "      <td>2077-10-01</td>\n",
       "      <td>2077-12-31</td>\n",
       "      <td>2077-01-01</td>\n",
       "      <td>2077-12-31</td>\n",
       "      <td>122077</td>\n",
       "      <td>12302077</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20771231</th>\n",
       "      <td>2077-12-31</td>\n",
       "      <td>31st</td>\n",
       "      <td>Friday</td>\n",
       "      <td>365</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>2077-W52</td>\n",
       "      <td>12</td>\n",
       "      <td>December</td>\n",
       "      <td>Dec</td>\n",
       "      <td>4</td>\n",
       "      <td>Fourth</td>\n",
       "      <td>2077</td>\n",
       "      <td>2077-12-27</td>\n",
       "      <td>2078-01-02</td>\n",
       "      <td>2077-12-01</td>\n",
       "      <td>2077-12-31</td>\n",
       "      <td>2077-10-01</td>\n",
       "      <td>2077-12-31</td>\n",
       "      <td>2077-01-01</td>\n",
       "      <td>2077-12-31</td>\n",
       "      <td>122077</td>\n",
       "      <td>12312077</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46752 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date_actual day_suffix   day_name  day_of_year  week_of_month  \\\n",
       "date_id                                                                  \n",
       "19500101  1950-01-01        1st     Sunday            1              1   \n",
       "19500102  1950-01-02        2nd     Monday            2              1   \n",
       "19500103  1950-01-03        3rd    Tuesday            3              1   \n",
       "19500104  1950-01-04        4th  Wednesday            4              1   \n",
       "19500105  1950-01-05        5th   Thursday            5              1   \n",
       "...              ...        ...        ...          ...            ...   \n",
       "20771227  2077-12-27       27th     Monday          361              4   \n",
       "20771228  2077-12-28       28th    Tuesday          362              4   \n",
       "20771229  2077-12-29       29th  Wednesday          363              5   \n",
       "20771230  2077-12-30       30th   Thursday          364              5   \n",
       "20771231  2077-12-31       31st     Friday          365              5   \n",
       "\n",
       "          week_of_year week_of_year_iso  month_actual month_name  \\\n",
       "date_id                                                            \n",
       "19500101            52         1950-W52             1    January   \n",
       "19500102             1         1950-W01             1    January   \n",
       "19500103             1         1950-W01             1    January   \n",
       "19500104             1         1950-W01             1    January   \n",
       "19500105             1         1950-W01             1    January   \n",
       "...                ...              ...           ...        ...   \n",
       "20771227            52       2077-W52              12   December   \n",
       "20771228            52       2077-W52              12   December   \n",
       "20771229            52       2077-W52              12   December   \n",
       "20771230            52       2077-W52              12   December   \n",
       "20771231            52       2077-W52              12   December   \n",
       "\n",
       "         month_name_abbreviated  quarter_actual quarter_name  year_actual  \\\n",
       "date_id                                                                     \n",
       "19500101                    Jan               1        First         1950   \n",
       "19500102                    Jan               1        First         1950   \n",
       "19500103                    Jan               1        First         1950   \n",
       "19500104                    Jan               1        First         1950   \n",
       "19500105                    Jan               1        First         1950   \n",
       "...                         ...             ...          ...          ...   \n",
       "20771227                    Dec               4       Fourth         2077   \n",
       "20771228                    Dec               4       Fourth         2077   \n",
       "20771229                    Dec               4       Fourth         2077   \n",
       "20771230                    Dec               4       Fourth         2077   \n",
       "20771231                    Dec               4       Fourth         2077   \n",
       "\n",
       "         first_day_of_week last_day_of_week first_day_of_month  \\\n",
       "date_id                                                          \n",
       "19500101        1949-12-26       1950-01-01         1950-01-01   \n",
       "19500102        1950-01-02       1950-01-08         1950-01-01   \n",
       "19500103        1950-01-02       1950-01-08         1950-01-01   \n",
       "19500104        1950-01-02       1950-01-08         1950-01-01   \n",
       "19500105        1950-01-02       1950-01-08         1950-01-01   \n",
       "...                    ...              ...                ...   \n",
       "20771227        2077-12-27       2078-01-02         2077-12-01   \n",
       "20771228        2077-12-27       2078-01-02         2077-12-01   \n",
       "20771229        2077-12-27       2078-01-02         2077-12-01   \n",
       "20771230        2077-12-27       2078-01-02         2077-12-01   \n",
       "20771231        2077-12-27       2078-01-02         2077-12-01   \n",
       "\n",
       "         last_day_of_month first_day_of_quarter last_day_of_quarter  \\\n",
       "date_id                                                               \n",
       "19500101        1950-01-31           1950-01-01          1950-03-31   \n",
       "19500102        1950-01-31           1950-01-01          1950-03-31   \n",
       "19500103        1950-01-31           1950-01-01          1950-03-31   \n",
       "19500104        1950-01-31           1950-01-01          1950-03-31   \n",
       "19500105        1950-01-31           1950-01-01          1950-03-31   \n",
       "...                    ...                  ...                 ...   \n",
       "20771227        2077-12-31           2077-10-01          2077-12-31   \n",
       "20771228        2077-12-31           2077-10-01          2077-12-31   \n",
       "20771229        2077-12-31           2077-10-01          2077-12-31   \n",
       "20771230        2077-12-31           2077-10-01          2077-12-31   \n",
       "20771231        2077-12-31           2077-10-01          2077-12-31   \n",
       "\n",
       "         first_day_of_year last_day_of_year  mmyyyy  mmddyyyy weekend_indr  \n",
       "date_id                                                                     \n",
       "19500101        1950-01-01       1950-12-31   11950   1011950      weekend  \n",
       "19500102        1950-01-01       1950-12-31   11950   1021950      weekday  \n",
       "19500103        1950-01-01       1950-12-31   11950   1031950      weekday  \n",
       "19500104        1950-01-01       1950-12-31   11950   1041950      weekday  \n",
       "19500105        1950-01-01       1950-12-31   11950   1051950      weekday  \n",
       "...                    ...              ...     ...       ...          ...  \n",
       "20771227        2077-01-01       2077-12-31  122077  12272077      weekday  \n",
       "20771228        2077-01-01       2077-12-31  122077  12282077      weekday  \n",
       "20771229        2077-01-01       2077-12-31  122077  12292077      weekday  \n",
       "20771230        2077-01-01       2077-12-31  122077  12302077      weekday  \n",
       "20771231        2077-01-01       2077-12-31  122077  12312077      weekday  \n",
       "\n",
       "[46752 rows x 24 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11b96f72-b8ad-4c9f-854c-c61eb241c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = date_df.reset_index().drop_duplicates(\"date_id\", keep=\"first\").set_index(\"date_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87bfcf2c-aabf-40d5-aeff-dd738adf2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df.to_csv(\"data/date.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee0a4fc1-41d0-4516-916f-e25dfa8ff6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1997\n",
       "1        1997\n",
       "2        1997\n",
       "3        1997\n",
       "4        1997\n",
       "         ... \n",
       "29580    2077\n",
       "29581    2077\n",
       "29582    2077\n",
       "29583    2077\n",
       "29584    2077\n",
       "Name: year_actual, Length: 29585, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_df[\"year_actual\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dadc8611-dd6e-4f81-8ab1-6313d3c2c3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>date_actual</th>\n",
       "      <th>day_suffix</th>\n",
       "      <th>day_name</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>week_of_year_iso</th>\n",
       "      <th>month_actual</th>\n",
       "      <th>month_name</th>\n",
       "      <th>month_name_abbreviated</th>\n",
       "      <th>quarter_actual</th>\n",
       "      <th>quarter_name</th>\n",
       "      <th>year_actual</th>\n",
       "      <th>first_day_of_week</th>\n",
       "      <th>last_day_of_week</th>\n",
       "      <th>first_day_of_month</th>\n",
       "      <th>last_day_of_month</th>\n",
       "      <th>first_day_of_quarter</th>\n",
       "      <th>last_day_of_quarter</th>\n",
       "      <th>first_day_of_year</th>\n",
       "      <th>last_day_of_year</th>\n",
       "      <th>mmyyyy</th>\n",
       "      <th>mmddyyyy</th>\n",
       "      <th>weekend_indr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5560</th>\n",
       "      <td>20120323</td>\n",
       "      <td>2012-03-23</td>\n",
       "      <td>23rd</td>\n",
       "      <td>Friday</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>2012-W12</td>\n",
       "      <td>3</td>\n",
       "      <td>March</td>\n",
       "      <td>Mar</td>\n",
       "      <td>1</td>\n",
       "      <td>First</td>\n",
       "      <td>2012</td>\n",
       "      <td>2012-03-19</td>\n",
       "      <td>2012-03-25</td>\n",
       "      <td>2012-03-01</td>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>32012</td>\n",
       "      <td>3232012</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date_id date_actual day_suffix day_name  day_of_year  week_of_month  \\\n",
       "5560  20120323  2012-03-23       23rd   Friday           83              4   \n",
       "\n",
       "      week_of_year week_of_year_iso  month_actual month_name  \\\n",
       "5560            12       2012-W12               3      March   \n",
       "\n",
       "     month_name_abbreviated  quarter_actual quarter_name  year_actual  \\\n",
       "5560                    Mar               1        First         2012   \n",
       "\n",
       "     first_day_of_week last_day_of_week first_day_of_month last_day_of_month  \\\n",
       "5560        2012-03-19       2012-03-25         2012-03-01        2012-03-31   \n",
       "\n",
       "     first_day_of_quarter last_day_of_quarter first_day_of_year  \\\n",
       "5560           2012-01-01          2012-03-31        2012-01-01   \n",
       "\n",
       "     last_day_of_year  mmyyyy  mmddyyyy weekend_indr  \n",
       "5560       2012-12-31   32012   3232012      weekday  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_df = pd.read_csv(\"data/date.csv\")\n",
    "date_df[date_df[\"date_id\"]==20120323]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ff73797-ad79-48ac-ad4e-aeec11b60975",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_date_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnew_date_df\u001b[49m\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_date_df' is not defined"
     ]
    }
   ],
   "source": [
    "new_date_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24415135-e010-4f68-ae7a-083efcb785d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>date_actual</th>\n",
       "      <th>day_suffix</th>\n",
       "      <th>day_name</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>week_of_year_iso</th>\n",
       "      <th>month_actual</th>\n",
       "      <th>month_name</th>\n",
       "      <th>...</th>\n",
       "      <th>last_day_of_week</th>\n",
       "      <th>first_day_of_month</th>\n",
       "      <th>last_day_of_month</th>\n",
       "      <th>first_day_of_quarter</th>\n",
       "      <th>last_day_of_quarter</th>\n",
       "      <th>first_day_of_year</th>\n",
       "      <th>last_day_of_year</th>\n",
       "      <th>mmyyyy</th>\n",
       "      <th>mmddyyyy</th>\n",
       "      <th>weekend_indr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19500101</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1st</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>1950-W52</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>...</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-01-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-03-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-12-31</td>\n",
       "      <td>011950</td>\n",
       "      <td>01011950</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19500102</td>\n",
       "      <td>1950-01-02</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Monday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1950-W01</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>...</td>\n",
       "      <td>1950-01-08</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-01-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-03-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-12-31</td>\n",
       "      <td>011950</td>\n",
       "      <td>01021950</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19500103</td>\n",
       "      <td>1950-01-03</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1950-W01</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>...</td>\n",
       "      <td>1950-01-08</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-01-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-03-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-12-31</td>\n",
       "      <td>011950</td>\n",
       "      <td>01031950</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19500104</td>\n",
       "      <td>1950-01-04</td>\n",
       "      <td>4th</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1950-W01</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>...</td>\n",
       "      <td>1950-01-08</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-01-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-03-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-12-31</td>\n",
       "      <td>011950</td>\n",
       "      <td>01041950</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19500105</td>\n",
       "      <td>1950-01-05</td>\n",
       "      <td>5th</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1950-W01</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>...</td>\n",
       "      <td>1950-01-08</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-01-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-03-31</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>1950-12-31</td>\n",
       "      <td>011950</td>\n",
       "      <td>01051950</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952</th>\n",
       "      <td>19791227</td>\n",
       "      <td>1979-12-27</td>\n",
       "      <td>27th</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>361</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>1979-W52</td>\n",
       "      <td>12</td>\n",
       "      <td>December</td>\n",
       "      <td>...</td>\n",
       "      <td>1979-12-30</td>\n",
       "      <td>1979-12-01</td>\n",
       "      <td>1979-12-31</td>\n",
       "      <td>1979-10-01</td>\n",
       "      <td>1979-12-31</td>\n",
       "      <td>1979-01-01</td>\n",
       "      <td>1979-12-31</td>\n",
       "      <td>121979</td>\n",
       "      <td>12271979</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10953</th>\n",
       "      <td>19791228</td>\n",
       "      <td>1979-12-28</td>\n",
       "      <td>28th</td>\n",
       "      <td>Friday</td>\n",
       "      <td>362</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>1979-W52</td>\n",
       "      <td>12</td>\n",
       "      <td>December</td>\n",
       "      <td>...</td>\n",
       "      <td>1979-12-30</td>\n",
       "      <td>1979-12-01</td>\n",
       "      <td>1979-12-31</td>\n",
       "      <td>1979-10-01</td>\n",
       "      <td>1979-12-31</td>\n",
       "      <td>1979-01-01</td>\n",
       "      <td>1979-12-31</td>\n",
       "      <td>121979</td>\n",
       "      <td>12281979</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10954</th>\n",
       "      <td>19791229</td>\n",
       "      <td>1979-12-29</td>\n",
       "      <td>29th</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>363</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>1979-W52</td>\n",
       "      <td>12</td>\n",
       "      <td>December</td>\n",
       "      <td>...</td>\n",
       "      <td>1979-12-30</td>\n",
       "      <td>1979-12-01</td>\n",
       "      <td>1979-12-31</td>\n",
       "      <td>1979-10-01</td>\n",
       "      <td>1979-12-31</td>\n",
       "      <td>1979-01-01</td>\n",
       "      <td>1979-12-31</td>\n",
       "      <td>121979</td>\n",
       "      <td>12291979</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10955</th>\n",
       "      <td>19791230</td>\n",
       "      <td>1979-12-30</td>\n",
       "      <td>30th</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>364</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>1979-W52</td>\n",
       "      <td>12</td>\n",
       "      <td>December</td>\n",
       "      <td>...</td>\n",
       "      <td>1979-12-30</td>\n",
       "      <td>1979-12-01</td>\n",
       "      <td>1979-12-31</td>\n",
       "      <td>1979-10-01</td>\n",
       "      <td>1979-12-31</td>\n",
       "      <td>1979-01-01</td>\n",
       "      <td>1979-12-31</td>\n",
       "      <td>121979</td>\n",
       "      <td>12301979</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10956</th>\n",
       "      <td>19791231</td>\n",
       "      <td>1979-12-31</td>\n",
       "      <td>31st</td>\n",
       "      <td>Monday</td>\n",
       "      <td>365</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1979-W01</td>\n",
       "      <td>12</td>\n",
       "      <td>December</td>\n",
       "      <td>...</td>\n",
       "      <td>1980-01-06</td>\n",
       "      <td>1979-12-01</td>\n",
       "      <td>1979-12-31</td>\n",
       "      <td>1979-10-01</td>\n",
       "      <td>1979-12-31</td>\n",
       "      <td>1979-01-01</td>\n",
       "      <td>1979-12-31</td>\n",
       "      <td>121979</td>\n",
       "      <td>12311979</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10957 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date_id date_actual day_suffix   day_name day_of_year week_of_month  \\\n",
       "0      19500101  1950-01-01        1st     Sunday           1             1   \n",
       "1      19500102  1950-01-02        2nd     Monday           2             1   \n",
       "2      19500103  1950-01-03        3rd    Tuesday           3             1   \n",
       "3      19500104  1950-01-04        4th  Wednesday           4             1   \n",
       "4      19500105  1950-01-05        5th   Thursday           5             1   \n",
       "...         ...         ...        ...        ...         ...           ...   \n",
       "10952  19791227  1979-12-27       27th   Thursday         361             4   \n",
       "10953  19791228  1979-12-28       28th     Friday         362             4   \n",
       "10954  19791229  1979-12-29       29th   Saturday         363             5   \n",
       "10955  19791230  1979-12-30       30th     Sunday         364             5   \n",
       "10956  19791231  1979-12-31       31st     Monday         365             5   \n",
       "\n",
       "      week_of_year week_of_year_iso month_actual month_name  ...  \\\n",
       "0               52         1950-W52            1    January  ...   \n",
       "1                1         1950-W01            1    January  ...   \n",
       "2                1         1950-W01            1    January  ...   \n",
       "3                1         1950-W01            1    January  ...   \n",
       "4                1         1950-W01            1    January  ...   \n",
       "...            ...              ...          ...        ...  ...   \n",
       "10952           52         1979-W52           12   December  ...   \n",
       "10953           52         1979-W52           12   December  ...   \n",
       "10954           52         1979-W52           12   December  ...   \n",
       "10955           52         1979-W52           12   December  ...   \n",
       "10956            1         1979-W01           12   December  ...   \n",
       "\n",
       "      last_day_of_week first_day_of_month last_day_of_month  \\\n",
       "0           1950-01-01         1950-01-01        1950-01-31   \n",
       "1           1950-01-08         1950-01-01        1950-01-31   \n",
       "2           1950-01-08         1950-01-01        1950-01-31   \n",
       "3           1950-01-08         1950-01-01        1950-01-31   \n",
       "4           1950-01-08         1950-01-01        1950-01-31   \n",
       "...                ...                ...               ...   \n",
       "10952       1979-12-30         1979-12-01        1979-12-31   \n",
       "10953       1979-12-30         1979-12-01        1979-12-31   \n",
       "10954       1979-12-30         1979-12-01        1979-12-31   \n",
       "10955       1979-12-30         1979-12-01        1979-12-31   \n",
       "10956       1980-01-06         1979-12-01        1979-12-31   \n",
       "\n",
       "      first_day_of_quarter last_day_of_quarter first_day_of_year  \\\n",
       "0               1950-01-01          1950-03-31        1950-01-01   \n",
       "1               1950-01-01          1950-03-31        1950-01-01   \n",
       "2               1950-01-01          1950-03-31        1950-01-01   \n",
       "3               1950-01-01          1950-03-31        1950-01-01   \n",
       "4               1950-01-01          1950-03-31        1950-01-01   \n",
       "...                    ...                 ...               ...   \n",
       "10952           1979-10-01          1979-12-31        1979-01-01   \n",
       "10953           1979-10-01          1979-12-31        1979-01-01   \n",
       "10954           1979-10-01          1979-12-31        1979-01-01   \n",
       "10955           1979-10-01          1979-12-31        1979-01-01   \n",
       "10956           1979-10-01          1979-12-31        1979-01-01   \n",
       "\n",
       "      last_day_of_year  mmyyyy  mmddyyyy weekend_indr  \n",
       "0           1950-12-31  011950  01011950      weekend  \n",
       "1           1950-12-31  011950  01021950      weekday  \n",
       "2           1950-12-31  011950  01031950      weekday  \n",
       "3           1950-12-31  011950  01041950      weekday  \n",
       "4           1950-12-31  011950  01051950      weekday  \n",
       "...                ...     ...       ...          ...  \n",
       "10952       1979-12-31  121979  12271979      weekday  \n",
       "10953       1979-12-31  121979  12281979      weekday  \n",
       "10954       1979-12-31  121979  12291979      weekend  \n",
       "10955       1979-12-31  121979  12301979      weekend  \n",
       "10956       1979-12-31  121979  12311979      weekday  \n",
       "\n",
       "[10957 rows x 25 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "\n",
    "def generate_date_dimension_1980():\n",
    "    # Create date range\n",
    "    start_date = datetime(1950, 1, 1)\n",
    "    end_date = datetime(1979, 12, 31)\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    # Initialize list to store all rows\n",
    "    rows = []\n",
    "    \n",
    "    for date in date_range:\n",
    "        # Basic date info\n",
    "        date_id = int(date.strftime('%Y%m%d'))\n",
    "        date_actual = date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Day suffix (1st, 2nd, 3rd, etc.)\n",
    "        day = date.day\n",
    "        if 10 <= day % 100 <= 20:\n",
    "            suffix = 'th'\n",
    "        else:\n",
    "            suffix = {1: 'st', 2: 'nd', 3: 'rd'}.get(day % 10, 'th')\n",
    "        day_suffix = f\"{day}{suffix}\"\n",
    "        \n",
    "        # Day name\n",
    "        day_name = date.strftime('%A')\n",
    "        \n",
    "        # Day of year\n",
    "        day_of_year = date.timetuple().tm_yday\n",
    "        \n",
    "        # Week calculations\n",
    "        # Week of month (1-based)\n",
    "        first_day_of_month = date.replace(day=1)\n",
    "        week_of_month = ((date.day - 1) // 7) + 1\n",
    "        \n",
    "        # Week of year (ISO and regular)\n",
    "        week_of_year = date.isocalendar()[1]\n",
    "        week_of_year_iso = f\"{date.year}-W{week_of_year:02d}\"\n",
    "        \n",
    "        # Month info\n",
    "        month_actual = date.month\n",
    "        month_name = date.strftime('%B')\n",
    "        month_name_abbreviated = date.strftime('%b')\n",
    "        \n",
    "        # Quarter info\n",
    "        quarter_actual = (date.month - 1) // 3 + 1\n",
    "        quarter_names = {1: 'First', 2: 'Second', 3: 'Third', 4: 'Fourth'}\n",
    "        quarter_name = quarter_names[quarter_actual]\n",
    "        \n",
    "        # Year\n",
    "        year_actual = date.year\n",
    "        \n",
    "        # Week boundaries (Monday to Sunday)\n",
    "        days_since_monday = date.weekday()\n",
    "        first_day_of_week = (date - timedelta(days=days_since_monday)).strftime('%Y-%m-%d')\n",
    "        last_day_of_week = (date + timedelta(days=6-days_since_monday)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Month boundaries\n",
    "        first_day_of_month = date.replace(day=1).strftime('%Y-%m-%d')\n",
    "        last_day_of_month = date.replace(day=calendar.monthrange(date.year, date.month)[1]).strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Quarter boundaries\n",
    "        quarter_start_month = (quarter_actual - 1) * 3 + 1\n",
    "        first_day_of_quarter = datetime(date.year, quarter_start_month, 1).strftime('%Y-%m-%d')\n",
    "        \n",
    "        if quarter_actual == 4:\n",
    "            last_day_of_quarter = datetime(date.year, 12, 31).strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            next_quarter_start = datetime(date.year, quarter_start_month + 3, 1)\n",
    "            last_day_of_quarter = (next_quarter_start - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Year boundaries\n",
    "        first_day_of_year = f\"{date.year}-01-01\"\n",
    "        last_day_of_year = f\"{date.year}-12-31\"\n",
    "        \n",
    "        # Special formats\n",
    "        mmyyyy = f\"{date.month:02d}{date.year}\"  # Fixed: added zero-padding for month\n",
    "        mmddyyyy = f\"{date.month:02d}{date.day:02d}{date.year}\"\n",
    "        \n",
    "        # Weekend indicator\n",
    "        weekend_indr = 'weekend' if date.weekday() >= 5 else 'weekday'\n",
    "        \n",
    "        # Create row\n",
    "        row = {\n",
    "            'date_id': date_id,\n",
    "            'date_actual': date_actual,\n",
    "            'day_suffix': day_suffix,\n",
    "            'day_name': day_name,\n",
    "            'day_of_year': str(day_of_year),\n",
    "            'week_of_month': str(week_of_month),\n",
    "            'week_of_year': str(week_of_year),\n",
    "            'week_of_year_iso': week_of_year_iso,\n",
    "            'month_actual': str(month_actual),\n",
    "            'month_name': month_name,\n",
    "            'month_name_abbreviated': month_name_abbreviated,\n",
    "            'quarter_actual': str(quarter_actual),\n",
    "            'quarter_name': quarter_name,\n",
    "            'year_actual': str(year_actual),\n",
    "            'first_day_of_week': first_day_of_week,\n",
    "            'last_day_of_week': last_day_of_week,\n",
    "            'first_day_of_month': first_day_of_month,\n",
    "            'last_day_of_month': last_day_of_month,\n",
    "            'first_day_of_quarter': first_day_of_quarter,\n",
    "            'last_day_of_quarter': last_day_of_quarter,\n",
    "            'first_day_of_year': first_day_of_year,\n",
    "            'last_day_of_year': last_day_of_year,\n",
    "            'mmyyyy': mmyyyy,\n",
    "            'mmddyyyy': mmddyyyy,\n",
    "            'weekend_indr': weekend_indr\n",
    "        }\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "# Generate the date dimension for 1980-1996 (fixed function call)\n",
    "date_dim_1980 = generate_date_dimension_1980()\n",
    "\n",
    "# Display first few rows to verify\n",
    "date_dim_1980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df2596f3-032c-41e4-8104-338399cd0c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = pd.read_csv(\"data/date.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4abe5790-d527-43e2-b53d-6555e02a1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df_new = pd.concat([date_df, date_dim_1980], ignore_index=True).sort_values(\"date_id\").reset_index(drop=True).set_index(\"date_id\")\n",
    "date_df_new.to_csv(\"data/date.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0794e6b4-bc23-4ec7-b6ee-d2325e1fdfab",
   "metadata": {},
   "source": [
    "# Tes transform function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86d34da-84f1-44e1-9b5f-532ee0ac5998",
   "metadata": {},
   "source": [
    "# Profiling data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5cdc13-5907-4a46-b294-35b78adbefcb",
   "metadata": {},
   "source": [
    "## Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef9ea73-eb88-4062-855b-b042ad14b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_data_spark = SparkSession \\\n",
    "                    .builder \\\n",
    "                    .appName(\"profile_data\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92b07d3f-5d97-4657-9048-0ae42848e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiling_result_path = \"profiling_result/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f20399-e4ea-4526-98a5-df4a4b62204a",
   "metadata": {},
   "source": [
    "## Source Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b29d35-8a97-4b9a-a3cf-58c3a80513d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_db_df = ProfileData.from_database(profile_data_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d10b06c9-657a-4727-aeea-dba88ec6fd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['company', 'acquisition', 'funding_rounds', 'funds', 'investments',\n",
       "       'ipos'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_db_df.table_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3765605-47db-4677-b64d-bdcca95b0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_db_df.to_csv(f\"{profiling_result_path}profile_db.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a4fe6-695e-436d-bfe2-b5a6a09600d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "company = profile_db_df.query(\"table_name == 'company'\")\n",
    "company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c06a9d-8923-4ae1-8ee5-f7876f40a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_db_df.query(\"table_name == 'acquisition'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b0464a-618b-42bb-922a-4dde2a960dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_db_df.query(\"table_name == 'funding_rounds'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c236a0-60af-4a61-ab6f-f68e982e5be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_db_df.query(\"table_name == 'funds'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c49e8-5a5f-47e2-8401-99a06dc848e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_db_df.query(\"table_name == 'investments'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff626d-2bb0-49e5-afef-2de83f67bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_db_df.query(\"table_name == 'ipos'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7683b7-e5ea-47c6-8299-f0509be41b9c",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fce25-22ed-401b-8c38-84737fe1fdd6",
   "metadata": {},
   "source": [
    "- date dan datetime masih dalam bentuk string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b0bf427-e791-4763-9406-d351797a06a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_csv_df = ProfileData.from_csv(profile_data_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75c7527e-8f9c-4c4f-8fa1-359a12805efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_csv_df.to_csv(f\"{profiling_result_path}profile_csv.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e8c561-ce75-490a-a542-f21f9cc2e05c",
   "metadata": {},
   "source": [
    "profile_csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2745ad-7d75-4fc3-b2fb-f84349608b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships = profile_csv_df.query(\"table_name == 'relationships.csv'\")\n",
    "relationships[[\"column_name\", \"data_type\", \"mode\", \"distinct_value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc04474b-304a-4d5b-85fd-468b32c4ff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship = profile_csv_df.query(\"table_name == 'relationships.csv' and column_name == 'person_object_id'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02240ff-8c7c-4bfe-9ad7-abf46f5c51ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c79a8f-b697-4030-9dd1-0c4d21c95672",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = relationship[\"distinct_value\"].values[0]\n",
    "rel_prefix = {r[0] for r in rel}\n",
    "rel_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04080d79-fb10-4d66-95a3-7dd15d99f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "people = profile_csv_df.query(\"table_name == 'people.csv'\")\n",
    "people"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de93380b-4d4d-4648-9be2-c5a562a3172f",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f109d-a008-4482-9d61-3b12a288ebc8",
   "metadata": {},
   "source": [
    "- data date dan datetime masih berbentuk string\n",
    "- start_date = 2005-01-01\n",
    "- end_date = 2011-01-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9137f8-33d0-4982-a00f-27aef0d5a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"start_date\":\"2005-01-01\",\n",
    "    \"end_date\":\"2011-01-01\"\n",
    "}\n",
    "\n",
    "profile_api_df = ProfileData.from_api(profile_data_spark, \n",
    "                                      api_url=API_PATH, \n",
    "                                      params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30cf59-6678-4509-a1b4-8f39a7a74b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_api_df.to_csv(f\"{profiling_result_path}profile_api.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ceed30-fcb1-4dd2-bd67-245086a03e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_api_df[[\"column_name\", \"data_type\", \"distinct_value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d12cc-f4c5-48f4-9e2f-3d74d125c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_api_df.query(\"column_name == 'description'\")[\"distinct_value\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d164aaa-727a-4bff-ba8c-6a8bc9c43aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_data_spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014317e7-1f3a-471e-8d54-79a7fb37de1e",
   "metadata": {},
   "source": [
    "# Run ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a720a66-8095-49d4-90c7-a035a16f5da8",
   "metadata": {},
   "source": [
    "# Try Extraction process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa19f01b-488c-4ee5-819a-d213e9c99d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ETL pipeline\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a93179-0251-46f1-9321-1c7ce380f901",
   "metadata": {},
   "source": [
    "## Source database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fe19d1f-7198-4572-a41f-7bdd5eca6eda",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Extract.from_database() got an unexpected keyword argument 'initial_load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Start loop to extract each tables and store in df_dict in the form of spark dataframe\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tab \u001b[38;5;129;01min\u001b[39;00m tables:\n\u001b[0;32m---> 10\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mExtract\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                              \u001b[49m\u001b[43msource_engine\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msource_engine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mwrite_log\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                              \u001b[49m\u001b[43minitial_load\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     df_dict[tab] \u001b[38;5;241m=\u001b[39m df\n",
      "\u001b[0;31mTypeError\u001b[0m: Extract.from_database() got an unexpected keyword argument 'initial_load'"
     ]
    }
   ],
   "source": [
    "# List tables to extract from database\n",
    "tables = ['company', 'acquisition', 'funding_rounds', 'funds', 'investments',\n",
    "          'ipos']\n",
    "\n",
    "# Initialize dictionary to store multiple tables\n",
    "df_dict = {}\n",
    "\n",
    "# Start loop to extract each tables and store in df_dict in the form of spark dataframe\n",
    "for tab in tables:\n",
    "    df = Extract.from_database(spark, \n",
    "                              table_name = tab, \n",
    "                              source_engine = source_engine, \n",
    "                              write_log = False)\n",
    "    df_dict[tab] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc99b88e-e3e4-428b-b002-00ccacb2418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_db_company = df_dict[\"company\"].toPandas()\n",
    "df_db_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0171e-7e74-4ad2-9a6c-67d15bcbc0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Extract._extract_postgres(spark, \"company\", write_log=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92142a03-423c-400e-992c-e4080c45550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(max(\"created_at\").cast(\"string\").alias(\"max_created_at\")).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d59fb-b3e1-45b6-8de9-b2941a7b1913",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1951d58-34b9-4b76-bbb4-c227d9da8ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = '2008-06-09'\n",
    "try:\n",
    "    response = requests.get(\n",
    "        url=API_PATH,\n",
    "        params={\"start_date\": ds, \"end_date\": ds},\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"fail to fetch data from API. Status code: {response.status_code}\")\n",
    "\n",
    "    json_data = response.json()\n",
    "    if not json_data:\n",
    "        print(\"No new data in Dellstore API. Skipped...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a749a-c7b2-474a-bedd-e198799dfdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = pd.DataFrame(json_data)\n",
    "json_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa9bb92-261a-4b24-9baf-45e157c86737",
   "metadata": {},
   "source": [
    "# Unused code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da8f08d0-a5bb-4350-9f60-ae69a412a474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>event_type_id</th>\n",
       "      <th>event_type</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22d603c6a09a14c27e2881d3e9dc77dac499318b07b620...</td>\n",
       "      <td>Acquired</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>70d236289ca8544f3b0853a9ba7c5af7dbf7d28b11afe2...</td>\n",
       "      <td>Acquiring</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>54a22fae449c3a25678fdf6fc62c628b0fc67f61d6cfe9...</td>\n",
       "      <td>IPO</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>03f90270ba8c153f9b5888366ef04fdcf783f47876be54...</td>\n",
       "      <td>Received Funding</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>723ab14024a6e4f082338c3fc749a04dd72616bf7371f3...</td>\n",
       "      <td>Made Investment</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3004682ecde79e43db18fe9274598e43529a9e3fc24b72...</td>\n",
       "      <td>Funds</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>9e86248cf2351e388065b80307b7ac00a2fe5ed922d93d...</td>\n",
       "      <td>Milestone</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      event_type_id  \\\n",
       "0           0  22d603c6a09a14c27e2881d3e9dc77dac499318b07b620...   \n",
       "1           1  70d236289ca8544f3b0853a9ba7c5af7dbf7d28b11afe2...   \n",
       "2           2  54a22fae449c3a25678fdf6fc62c628b0fc67f61d6cfe9...   \n",
       "3           3  03f90270ba8c153f9b5888366ef04fdcf783f47876be54...   \n",
       "4           4  723ab14024a6e4f082338c3fc749a04dd72616bf7371f3...   \n",
       "5           5  3004682ecde79e43db18fe9274598e43529a9e3fc24b72...   \n",
       "6           6  9e86248cf2351e388065b80307b7ac00a2fe5ed922d93d...   \n",
       "\n",
       "         event_type                  created_at                  updated_at  \n",
       "0          Acquired  2025-08-07 08:35:32.123446  2025-08-07 08:35:32.123446  \n",
       "1         Acquiring  2025-08-07 08:35:32.123446  2025-08-07 08:35:32.123446  \n",
       "2               IPO  2025-08-07 08:35:32.123446  2025-08-07 08:35:32.123446  \n",
       "3  Received Funding  2025-08-07 08:35:32.123446  2025-08-07 08:35:32.123446  \n",
       "4   Made Investment  2025-08-07 08:35:32.123446  2025-08-07 08:35:32.123446  \n",
       "5             Funds  2025-08-07 08:35:32.123446  2025-08-07 08:35:32.123446  \n",
       "6         Milestone  2025-08-07 08:35:32.123446  2025-08-07 08:35:32.123446  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tes_csv = pd.read_csv(\"data/event_type.csv\")\n",
    "tes_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "992c56ca-bb5c-492e-a0ea-1c876f069129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_type</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_type_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22d603c6a09a14c27e2881d3e9dc77dac499318b07b620a135b6ebd97b4ae817</th>\n",
       "      <td>Acquired</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70d236289ca8544f3b0853a9ba7c5af7dbf7d28b11afe29691d0027c53b95fdc</th>\n",
       "      <td>Acquiring</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54a22fae449c3a25678fdf6fc62c628b0fc67f61d6cfe9d8e738c60116d04d3f</th>\n",
       "      <td>IPO</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03f90270ba8c153f9b5888366ef04fdcf783f47876be546dedcd021a5f3e02a2</th>\n",
       "      <td>Received Funding</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723ab14024a6e4f082338c3fc749a04dd72616bf7371f31b8912b9dea12c1b42</th>\n",
       "      <td>Made Investment</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004682ecde79e43db18fe9274598e43529a9e3fc24b726bb27d235e9c81e577</th>\n",
       "      <td>Funds</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9e86248cf2351e388065b80307b7ac00a2fe5ed922d93d1316c241cdd9c61146</th>\n",
       "      <td>Milestone</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "      <td>2025-08-07 08:35:32.123446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          event_type  \\\n",
       "event_type_id                                                          \n",
       "22d603c6a09a14c27e2881d3e9dc77dac499318b07b620a...          Acquired   \n",
       "70d236289ca8544f3b0853a9ba7c5af7dbf7d28b11afe29...         Acquiring   \n",
       "54a22fae449c3a25678fdf6fc62c628b0fc67f61d6cfe9d...               IPO   \n",
       "03f90270ba8c153f9b5888366ef04fdcf783f47876be546...  Received Funding   \n",
       "723ab14024a6e4f082338c3fc749a04dd72616bf7371f31...   Made Investment   \n",
       "3004682ecde79e43db18fe9274598e43529a9e3fc24b726...             Funds   \n",
       "9e86248cf2351e388065b80307b7ac00a2fe5ed922d93d1...         Milestone   \n",
       "\n",
       "                                                                    created_at  \\\n",
       "event_type_id                                                                    \n",
       "22d603c6a09a14c27e2881d3e9dc77dac499318b07b620a...  2025-08-07 08:35:32.123446   \n",
       "70d236289ca8544f3b0853a9ba7c5af7dbf7d28b11afe29...  2025-08-07 08:35:32.123446   \n",
       "54a22fae449c3a25678fdf6fc62c628b0fc67f61d6cfe9d...  2025-08-07 08:35:32.123446   \n",
       "03f90270ba8c153f9b5888366ef04fdcf783f47876be546...  2025-08-07 08:35:32.123446   \n",
       "723ab14024a6e4f082338c3fc749a04dd72616bf7371f31...  2025-08-07 08:35:32.123446   \n",
       "3004682ecde79e43db18fe9274598e43529a9e3fc24b726...  2025-08-07 08:35:32.123446   \n",
       "9e86248cf2351e388065b80307b7ac00a2fe5ed922d93d1...  2025-08-07 08:35:32.123446   \n",
       "\n",
       "                                                                    updated_at  \n",
       "event_type_id                                                                   \n",
       "22d603c6a09a14c27e2881d3e9dc77dac499318b07b620a...  2025-08-07 08:35:32.123446  \n",
       "70d236289ca8544f3b0853a9ba7c5af7dbf7d28b11afe29...  2025-08-07 08:35:32.123446  \n",
       "54a22fae449c3a25678fdf6fc62c628b0fc67f61d6cfe9d...  2025-08-07 08:35:32.123446  \n",
       "03f90270ba8c153f9b5888366ef04fdcf783f47876be546...  2025-08-07 08:35:32.123446  \n",
       "723ab14024a6e4f082338c3fc749a04dd72616bf7371f31...  2025-08-07 08:35:32.123446  \n",
       "3004682ecde79e43db18fe9274598e43529a9e3fc24b726...  2025-08-07 08:35:32.123446  \n",
       "9e86248cf2351e388065b80307b7ac00a2fe5ed922d93d1...  2025-08-07 08:35:32.123446  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tes_csv = tes_csv.drop(columns=\"Unnamed: 0\").set_index(\"event_type_id\")\n",
    "tes_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "190b01a1-dfc5-467c-85f5-b61ea52afc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tes_csv.to_csv(\"data/event_type.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7983c64d-9c50-48e7-a24a-7add78cf078b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created successfully:\n",
      "                                       event_type_id        event_type  \\\n",
      "0  22d603c6a09a14c27e2881d3e9dc77dac499318b07b620...          Acquired   \n",
      "1  70d236289ca8544f3b0853a9ba7c5af7dbf7d28b11afe2...         Acquiring   \n",
      "2  54a22fae449c3a25678fdf6fc62c628b0fc67f61d6cfe9...               IPO   \n",
      "3  03f90270ba8c153f9b5888366ef04fdcf783f47876be54...  Received Funding   \n",
      "4  723ab14024a6e4f082338c3fc749a04dd72616bf7371f3...   Made Investment   \n",
      "5  3004682ecde79e43db18fe9274598e43529a9e3fc24b72...             Funds   \n",
      "6  9e86248cf2351e388065b80307b7ac00a2fe5ed922d93d...         Milestone   \n",
      "\n",
      "                  created_at                 updated_at  \n",
      "0 2025-08-07 08:35:32.123446 2025-08-07 08:35:32.123446  \n",
      "1 2025-08-07 08:35:32.123446 2025-08-07 08:35:32.123446  \n",
      "2 2025-08-07 08:35:32.123446 2025-08-07 08:35:32.123446  \n",
      "3 2025-08-07 08:35:32.123446 2025-08-07 08:35:32.123446  \n",
      "4 2025-08-07 08:35:32.123446 2025-08-07 08:35:32.123446  \n",
      "5 2025-08-07 08:35:32.123446 2025-08-07 08:35:32.123446  \n",
      "6 2025-08-07 08:35:32.123446 2025-08-07 08:35:32.123446  \n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import hashlib\n",
    "# from datetime import datetime\n",
    "\n",
    "# def sha256_hash(*cols):\n",
    "#     concatenated = ''.join(str(col) if col is not None else '' for col in cols) + '||'\n",
    "#     concatenated = concatenated.rstrip('||')\n",
    "#     return hashlib.sha256(concatenated.encode()).hexdigest()\n",
    "\n",
    "# event_types = [\n",
    "#     'Acquired',\n",
    "#     'Acquiring',\n",
    "#     'IPO',\n",
    "#     'Received Funding',\n",
    "#     'Made Investment',\n",
    "#     'Funds',\n",
    "#     'Milestone'\n",
    "# ]\n",
    "\n",
    "# current_time = datetime.now()\n",
    "\n",
    "# data = {\n",
    "#     'event_type_id': [sha256_hash(event) for event in event_types],\n",
    "#     'event_type': event_types,\n",
    "#     'created_at': [current_time] * len(event_types),\n",
    "#     'updated_at': [current_time] * len(event_types)\n",
    "# }\n",
    "\n",
    "# event_type_df = pd.DataFrame(data)\n",
    "\n",
    "# try:\n",
    "#     if event_type_df.shape[0] == 0:\n",
    "#         raise ValueError(\"DataFrame is empty\")\n",
    "#     print(\"DataFrame created successfully:\")\n",
    "#     print(event_type_df)\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Exception caught: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db1f6f-d6b8-415a-9af0-b90d9cead29b",
   "metadata": {},
   "source": [
    "initial_load belum didelete!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7553c57-9ddb-40f0-9da8-9d0e154d7ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @classmethod\n",
    "#     def source(cls, spark: SparkSession, df_dict: Dict[str, DataFrame]) -> Optional[Dict]:\n",
    "#         # Create completely independent DataFrames copy:\n",
    "#         src = {\n",
    "#             key: df.select(\"*\")\n",
    "#             for key, df in df_dict.items()\n",
    "#         }\n",
    "\n",
    "#         # Define current_timestamp for logging\n",
    "#         current_timestamp = datetime.now()  \n",
    "        \n",
    "#         # Hash configurations for each table\n",
    "#         hash_configs = {\n",
    "#             \"company\": [\n",
    "#                 \"office_id\",\n",
    "#                 \"object_id\",\n",
    "#                 \"description\",\n",
    "#                 \"region\",\n",
    "#                 \"address1\",\n",
    "#                 \"address2\",\n",
    "#                 \"city\",\n",
    "#                 \"zip_code\",\n",
    "#                 \"state_code\",\n",
    "#                 \"country_code\",\n",
    "#                 \"latitude\",\n",
    "#                 \"longitude\",\n",
    "#                 \"created_at\",\n",
    "#                 \"updated_at\"\n",
    "#             ],\n",
    "            \n",
    "#             \"acquisition\": [\n",
    "#                 \"acquisition_id\",\n",
    "#                 \"acquiring_object_id\",\n",
    "#                 \"acquired_object_id\",\n",
    "#                 \"term_code\",\n",
    "#                 \"price_amount\",\n",
    "#                 \"price_currency_code\",\n",
    "#                 \"acquired_at\",\n",
    "#                 \"source_url\",\n",
    "#                 \"source_description\",\n",
    "#                 \"created_at\",\n",
    "#                 \"updated_at\",\n",
    "#             ],\n",
    "            \n",
    "#             \"ipos\": [\n",
    "#                 \"ipo_id\",\n",
    "#                 \"object_id\",\n",
    "#                 \"valuation_amount\",\n",
    "#                 \"valuation_currency_code\",\n",
    "#                 \"raised_amount\",\n",
    "#                 \"raised_currency_code\",\n",
    "#                 \"public_at\",\n",
    "#                 \"stock_symbol\",\n",
    "#                 \"source_url\",\n",
    "#                 \"source_description\",\n",
    "#                 \"created_at\",\n",
    "#                 \"updated_at\",\n",
    "#             ],\n",
    "            \n",
    "#             \"funds\": [\n",
    "#                 \"fund_id\",\n",
    "#                 \"object_id\",\n",
    "#                 \"name\",\n",
    "#                 \"funded_at\",\n",
    "#                 \"raised_amount\",\n",
    "#                 \"raised_currency_code\",\n",
    "#                 \"source_url\",\n",
    "#                 \"source_description\",\n",
    "#                 \"created_at\",\n",
    "#                 \"updated_at\",\n",
    "#             ],\n",
    "            \n",
    "#             \"funding_rounds\": [\n",
    "#                 \"funding_round_id\",\n",
    "#                 \"object_id\",\n",
    "#                 \"funded_at\",\n",
    "#                 \"funding_round_type\",\n",
    "#                 \"funding_round_code\",\n",
    "#                 \"raised_amount_usd\",\n",
    "#                 \"raised_amount\",\n",
    "#                 \"raised_currency_code\",\n",
    "#                 \"pre_money_valuation_usd\",\n",
    "#                 \"pre_money_valuation\",\n",
    "#                 \"pre_money_currency_code\",\n",
    "#                 \"post_money_valuation_usd\",\n",
    "#                 \"post_money_valuation\",\n",
    "#                 \"post_money_currency_code\",\n",
    "#                 \"participants\",\n",
    "#                 \"is_first_round\",\n",
    "#                 \"is_last_round\",\n",
    "#                 \"source_url\",\n",
    "#                 \"source_description\",\n",
    "#                 \"created_by\",\n",
    "#                 \"created_at\",\n",
    "#                 \"updated_at\",\n",
    "#             ],\n",
    "            \n",
    "#             \"investments\": [\n",
    "#                 \"investment_id\",\n",
    "#                 \"funding_round_id\",\n",
    "#                 \"funded_object_id\",\n",
    "#                 \"investor_object_id\",\n",
    "#                 \"created_at\",\n",
    "#                 \"updated_at\",\n",
    "#             ],\n",
    "            \n",
    "#             \"people\": [\n",
    "#                 \"people_id\",\n",
    "#                 \"object_id\",\n",
    "#                 \"first_name\",\n",
    "#                 \"last_name\",\n",
    "#                 \"birthplace\",\n",
    "#                 \"affiliation_name\",\n",
    "#             ],\n",
    "            \n",
    "#             \"relationships\": [\n",
    "#                 \"relationship_id\",\n",
    "#                 \"person_object_id\",\n",
    "#                 \"relationship_object_id\",\n",
    "#                 \"start_at\",\n",
    "#                 \"end_at\",\n",
    "#                 \"is_past\",\n",
    "#                 \"sequence\",\n",
    "#                 \"title\",\n",
    "#                 \"created_at\",\n",
    "#                 \"updated_at\",\n",
    "#             ],\n",
    "            \n",
    "#             \"milestones\": [\n",
    "#                 \"milestone_id\",\n",
    "#                 \"description\",\n",
    "#                 \"milestone_at\",\n",
    "#                 \"milestone_code\",\n",
    "#                 \"object_id\",\n",
    "#                 \"source_description\",\n",
    "#                 \"source_url\",\n",
    "#                 \"created_at\",\n",
    "#                 \"updated_at\",\n",
    "#             ],\n",
    "#         }\n",
    "#         # Add hashing to all tables\n",
    "#         # No hashing for date table and event_type table\n",
    "#         for table_name, hash_cols in hash_configs.items():\n",
    "#             try:\n",
    "#                 if table_name in src:\n",
    "#                     src[table_name] = cls._hashing(\n",
    "#                         df=src[table_name],\n",
    "#                         hash_cols=hash_cols,\n",
    "#                         hash_output_colname=\"hash_id\"\n",
    "#                     )\n",
    "#             except Exception as e:\n",
    "#                 # Capture full traceback information\n",
    "#                 tb_str = traceback.format_exc()\n",
    "#                 error_msg = f\"\"\"\n",
    "#                 fail to perform hashing operation for table '{table_name}'.\n",
    "                \n",
    "#                 Error Details:\n",
    "#                 - Error Type: {type(e).__name__}\n",
    "#                 - Error Message: {str(e)}\n",
    "#                 - Table: {table_name}\n",
    "#                 - Initial Load: {initial_load}\n",
    "                \n",
    "#                 Full Traceback:\n",
    "#                 {tb_str}\n",
    "#                 \"\"\"\n",
    "            \n",
    "#                  # Log fail transformation\n",
    "#                 log_msg = spark.sparkContext\\\n",
    "#                     .parallelize([(\"staging\",\n",
    "#                                    \"transform_source\",\n",
    "#                                    \"fail\",\n",
    "#                                    \"source extraction result\",\n",
    "#                                    table_name,\n",
    "#                                    current_timestamp,\n",
    "#                                    error_msg)])\\\n",
    "#                     .toDF(['step',\n",
    "#                            'process',\n",
    "#                            'status',\n",
    "#                            'source',\n",
    "#                            'table_name',\n",
    "#                            'etl_date',\n",
    "#                            'error_msg'])\n",
    "#                 load_log(spark, log_msg)\n",
    "#                 return None\n",
    "\n",
    "#         # ----------------------------------------------------------------- #\n",
    "#         # Add created_at and updated_at\n",
    "#         key_columns = {\n",
    "#             \"company\": \"office_id\",\n",
    "#             \"acquisition\": \"acquisition_id\",\n",
    "#             \"ipos\": \"ipo_id\",\n",
    "#             \"funds\": \"fund_id\",\n",
    "#             \"funding_rounds\": \"funding_round_id\",\n",
    "#             \"investments\": \"investment_id\",\n",
    "#             \"people\": \"people_id\",\n",
    "#             \"relationships\": \"relationship_id\",\n",
    "#             \"milestones\": \"milestone_id\",\n",
    "#         } \n",
    "        \n",
    "#         for table_name, _ in src.items():\n",
    "#             src[table_name] = src[table_name]\\\n",
    "#                                  .withColumn(\"created_at\", lit(datetime.now()))\\\n",
    "#                                  .withColumn(\"updated_at\", lit(datetime.now()))\n",
    "            \n",
    "#             # Create logic for upsert\n",
    "#             if not initial_load:\n",
    "#                 if table_name == \"date\":\n",
    "#                     continue\n",
    "#                 try:\n",
    "#                     # Extract staging data\n",
    "#                     stg_df = Extract.from_database(\n",
    "#                         spark=spark,\n",
    "#                         table_name=table_name,\n",
    "#                         source_engine=stg_engine,\n",
    "#                         write_log=False,\n",
    "#                         initial_load=initial_load\n",
    "#                     )\n",
    "                    \n",
    "#                     if stg_df is None or stg_df.count() == 0:\n",
    "#                         # Capture full traceback information\n",
    "#                         tb_str = traceback.format_exc()\n",
    "#                         error_msg = f\"\"\"\n",
    "#                         fail to perform upsert operation for table '{table_name}'.\n",
    "                        \n",
    "#                         Error Details:\n",
    "#                         - Error Type: {type(e).__name__}\n",
    "#                         - Error Message: {str(e)}\n",
    "#                         - Table: {table_name}\n",
    "#                         - Initial Load: {initial_load}\n",
    "                        \n",
    "#                         Full Traceback:\n",
    "#                         {tb_str}\n",
    "#                         \"\"\"\n",
    "#                         # Log fail transformation\n",
    "#                         current_timestamp = datetime.now() if not initial_load else datetime(1111, 11, 11)\n",
    "#                         log_msg = spark.sparkContext\\\n",
    "#                             .parallelize([(\"staging\",\n",
    "#                                            \"transform_source\",\n",
    "#                                            \"fail\",\n",
    "#                                            \"source extraction result\",\n",
    "#                                            table_name,\n",
    "#                                            current_timestamp,\n",
    "#                                            error_msg)])\\\n",
    "#                             .toDF(['step',\n",
    "#                                    'process',\n",
    "#                                    'status',\n",
    "#                                    'source',\n",
    "#                                    'table_name',\n",
    "#                                    'etl_date',\n",
    "#                                    'error_msg'])\n",
    "#                         load_log(spark, log_msg)\n",
    "#                         return None\n",
    "                        \n",
    "#                     # Get join key and hash column\n",
    "#                     join_key = key_columns[table_name]\n",
    "#                     pk_col = \"hash_id\"\n",
    "                    \n",
    "#                     # Validate required columns exist\n",
    "#                     if join_key not in src[table_name].columns:\n",
    "#                         raise ValueError(f\"Join key '{join_key}' not found in source DataFrame for table {table_name}\")\n",
    "#                     if pk_col not in src[table_name].columns:\n",
    "#                         raise ValueError(f\"Hash column '{pk_col}' not found in source DataFrame for table {table_name}\")\n",
    "                    \n",
    "#                     # Create aliases for DataFrames\n",
    "#                     src_df_alias = src[table_name].alias(\"src\")\n",
    "#                     stg_df_alias = stg_df.alias(\"stg\")\n",
    "                    \n",
    "#                     # Perform inner join to find matching records\n",
    "#                     joined_df = src_df_alias.join(\n",
    "#                         stg_df_alias,\n",
    "#                         col(f\"src.{join_key}\") == col(f\"stg.{join_key}\"),\n",
    "#                         \"inner\"\n",
    "#                     )\n",
    "                    \n",
    "#                     # Define update condition based on hash comparison\n",
    "#                     update_condition = col(f\"src.{pk_col}\") != col(f\"stg.{pk_col}\")\n",
    "                    \n",
    "#                     # Build select expressions for update logic - use staging schema as reference\n",
    "#                     select_expressions = []\n",
    "#                     current_timestamp_lit = lit(datetime.now())\n",
    "                    \n",
    "#                     for column in stg_df.columns:\n",
    "#                         if column == \"updated_at\":\n",
    "#                             # Update timestamp only if record has changed\n",
    "#                             expr = when(update_condition, current_timestamp_lit).otherwise(col(f\"stg.{column}\"))\n",
    "#                         elif column == \"created_at\":\n",
    "#                             # Keep original created_at from staging\n",
    "#                             expr = col(f\"stg.{column}\")\n",
    "#                         else:\n",
    "#                             # Use source data if updated, otherwise keep staging data\n",
    "#                             expr = when(update_condition, col(f\"src.{column}\")).otherwise(col(f\"stg.{column}\"))\n",
    "                        \n",
    "#                         select_expressions.append(expr.alias(column))\n",
    "                    \n",
    "#                     # Create updated DataFrame\n",
    "#                     updated_df = joined_df.select(*select_expressions)\n",
    "                    \n",
    "#                     # Handle inserts (records in source but not in staging)\n",
    "#                     insert_df = src[table_name].join(\n",
    "#                         stg_df.select(join_key),\n",
    "#                         join_key,\n",
    "#                         \"leftanti\"\n",
    "#                     )\n",
    "                    \n",
    "                    \n",
    "#                     # Select columns in same order as staging\n",
    "#                     insert_df = insert_df.select(*stg_df.columns)\n",
    "                    \n",
    "#                     # Combine updated and inserted records\n",
    "#                     if insert_df.count() == 0:\n",
    "#                         upserted_df = updated_df\n",
    "#                     elif updated_df.count() == 0:\n",
    "#                         upserted_df = insert_df\n",
    "#                     else:\n",
    "#                         upserted_df = updated_df.unionByName(insert_df)\n",
    "                    \n",
    "#                     src[table_name] = upserted_df\n",
    "                    \n",
    "#                     # Log success transforming\n",
    "#                     log_msg = spark.sparkContext\\\n",
    "#                         .parallelize([(\"staging\",\n",
    "#                                        \"transform_source\",\n",
    "#                                        \"success\",\n",
    "#                                        \"source\",\n",
    "#                                        table_name,\n",
    "#                                        current_timestamp)])\\\n",
    "#                         .toDF(['step',\n",
    "#                                'process',\n",
    "#                                'status',\n",
    "#                                'source',\n",
    "#                                'table_name',\n",
    "#                                'etl_date'])\n",
    "#                     load_log(spark, log_msg)\n",
    "                    \n",
    "#                 except Exception as e:\n",
    "#                     # Capture full traceback information\n",
    "#                     tb_str = traceback.format_exc()\n",
    "#                     error_msg = f\"\"\"\n",
    "#                     fail to perform upsert operation for table '{table_name}'.\n",
    "                    \n",
    "#                     Error Details:\n",
    "#                     - Error Type: {type(e).__name__}\n",
    "#                     - Error Message: {str(e)}\n",
    "#                     - Table: {table_name}\n",
    "#                     - Initial Load: {initial_load}\n",
    "                    \n",
    "#                     Full Traceback:\n",
    "#                     {tb_str}\n",
    "#                     \"\"\"\n",
    "#                     # Log fail transformation\n",
    "#                     current_timestamp = datetime.now() if not initial_load else datetime(1111, 11, 11)\n",
    "#                     log_msg = spark.sparkContext\\\n",
    "#                         .parallelize([(\"staging\",\n",
    "#                                        \"transform_source\",\n",
    "#                                        \"fail\",\n",
    "#                                        \"source extraction result\",\n",
    "#                                        table_name,\n",
    "#                                        current_timestamp,\n",
    "#                                        error_msg)])\\\n",
    "#                         .toDF(['step',\n",
    "#                                'process',\n",
    "#                                'status',\n",
    "#                                'source',\n",
    "#                                'table_name',\n",
    "#                                'etl_date',\n",
    "#                                'error_msg'])\n",
    "#                     load_log(spark, log_msg)\n",
    "#                     return None\n",
    "\n",
    "#         # ----------------------------------------------------------------- #\n",
    "#         # Return back the src dictionary\n",
    "#         return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5bbcc2-cd2b-4f6a-baf3-6d50dfc99ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # @staticmethod\n",
    "   #  def to_temp(spark: SparkSession, df, table_name, path=\"/temp\"):\n",
    "   #      current_timestamp = datetime.now()\n",
    "       \n",
    "   #      try:\n",
    "   #          # load data to provided csv path\n",
    "   #          full_path = os.path.join(path, table_name)\n",
    "   #          os.chmod(os.path.join(os.getcwd(), path), stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)\n",
    "   #          os.makedirs(full_path, exist_ok=True)\n",
    "            \n",
    "   #          # Write to single CSV file\n",
    "   #          df.coalesce(1).write.mode(\"overwrite\").csv(full_path, header=True)\n",
    "            \n",
    "   #          # Find the actual CSV file (Spark creates part-*.csv files)\n",
    "   #          csv_files = glob.glob(f\"{full_path}/part-*.csv\")\n",
    "            \n",
    "   #          # Rename to desired filename\n",
    "   #          final_csv_path = f\"{full_path}.csv\"\n",
    "   #          os.rename(csv_files[0], final_csv_path)\n",
    "            \n",
    "   #          # Clean up temporary directory\n",
    "   #          shutil.rmtree(full_path)\n",
    "            \n",
    "   #          print(f\"CSV file written to: {final_csv_path}\")\n",
    "\n",
    "            \n",
    "   #          #log message\n",
    "   #          log_msg = spark.sparkContext\\\n",
    "   #              .parallelize([(\"warehouse\", \"load\", \"success\", \"staging transformation result\", table_name, current_timestamp)])\\\n",
    "   #              .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "            \n",
    "   #      except Exception as e:\n",
    "   #          # log message\n",
    "   #          log_msg = spark.sparkContext\\\n",
    "   #              .parallelize([(\"warehouse\", \"load\", \"fail\", \"staging transformation result\", table_name, current_timestamp, str(e))])\\\n",
    "   #              .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "            \n",
    "   #      finally:\n",
    "   #          load_log(spark, log_msg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e7227-07ae-493f-badc-2a01bf616b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af499e7a-11bd-481b-8d43-8449a38f317c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c22fa-37a5-4f36-91cf-0ec6e1f0ad0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf37d3-4eec-420c-980c-25f1bac48921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad2700d-f42e-4d9e-963e-c741ba3a4c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b20720-4ed1-4f71-bb66-f49633a285f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b77504-4822-4481-900d-d7df13ea9314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9f6b18-553c-406d-b53b-dd81e9857851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9aa99-9eda-4d43-92fe-b6825dd94b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504bc847-e613-4df1-8077-9d7549c6b452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265851cb-2f85-4cf0-8e57-1060fbd75fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b5ed8-d0f4-4cc3-83dd-3541b8b270f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51fb877-ca9a-4a6a-bdb9-ead3c3406944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
